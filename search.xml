<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PyTorch学习</title>
      <link href="/2023/01/14/pyTorch/"/>
      <url>/2023/01/14/pyTorch/</url>
      
        <content type="html"><![CDATA[<h1 id="一datasets-dataloaders">（一）Datasets &amp; DataLoaders</h1><p>处理数据样本的代码可能会变得凌乱，难以维护；理想情况下，我们希望数据集代码与模型训练代码分离，以获得更好的可读性和模块性。PyTorch提供了两种数据原语：<code>torch.utils.data.DataLoader</code>和<code>torch.utils.data.Dataset</code>，允许你使用预加载的数据集以及自己的数据。<code>Dataset</code> 存储样本及其相应的标签，<code>DataLoader</code>将<code>Dataset</code>封装成一个迭代器以便轻松访问样本。PyTorch域库提供了许多预加载的数据集（比如FashionMNIST），属于<code>torch.utils.data.Dataset</code>的子类，并实现指定于特定数据的功能。它们可以用于原型和基准测试你的模型。</p><h2 id="加载数据集">加载数据集</h2><p>这是一个从TorchVision中加载Fashion-MNIST数据集的例子，Fashion MNIST是Zalando文章图片的数据集，包含60000个训练示例和10000个测试示例。每个示例包括一个28×28的灰度图像和一个来自10个类别之一的相关标签。加载FashionMNIST需要以下参数</p><ul><li><code>root</code>：训练/测试数据存储路径</li><li><code>train</code>：指定训练或测试数据集</li><li><code>download=True</code>：如果在“根目录”中不可用，则从internet下载数据</li><li><code>transform</code> 和<code>target_transform</code>指定特征和标签变换</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.utils.data import Dataset</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision.transforms import ToTensor</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_data = datasets.FashionMNIST(root=&#x27;data&#x27;,train=True,download=True,transform=ToTensor())</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(root=&#x27;data&#x27;,train=False,download=True,transform=ToTensor())</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="迭代和可视化数据集">迭代和可视化数据集</h2><p>我们可以像列表一样手动索引<code>Datasets</code>：<code>train_data[index]</code>。使用<code>matplotlib</code>可视化一些训练数据样本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">labels_map = &#123;</span><br><span class="line">    0: &quot;T-Shirt&quot;,</span><br><span class="line">    1: &quot;Trouser&quot;,</span><br><span class="line">    2: &quot;Pullover&quot;,</span><br><span class="line">    3: &quot;Dress&quot;,</span><br><span class="line">    4: &quot;Coat&quot;,</span><br><span class="line">    5: &quot;Sandal&quot;,</span><br><span class="line">    6: &quot;Shirt&quot;,</span><br><span class="line">    7: &quot;Sneaker&quot;,</span><br><span class="line">    8: &quot;Bag&quot;,</span><br><span class="line">    9: &quot;Ankle Boot&quot;,&#125;</span><br><span class="line">figure = plt.figure(figsize=(8,8))</span><br><span class="line">cols,rows = 3,3</span><br><span class="line">for i in range(1,cols * rows + 1):</span><br><span class="line">    sample_index = torch.randint(len(train_data),size=(1,)).item() # 获取随机索引</span><br><span class="line">    img,label = train_data[sample_index] # 找到随机索引下的图像和标签</span><br><span class="line">    figure.add_subplot(rows,cols,i) # 增加子图，add_subplot面向对象，subplot面向函数</span><br><span class="line">    plt.title(labels_map[label])</span><br><span class="line">    plt.axis(&quot;off&quot;) # 关闭坐标轴</span><br><span class="line">    plt.imshow(img.squeeze(),cmap=&#x27;gray&#x27;) # 对图像进行处理,cmap颜色图谱</span><br><span class="line">plt.show() # 显示图像</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/cf1df216ba834e599353f36da6e045c7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAYnVqYnVqYml1,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述" /><figcaption>在这里插入图片描述</figcaption></figure><h2 id="创建自定义数据集文件">创建自定义数据集文件</h2><p>自定义数据集类必须包含三个函数：<code>__init__</code>, <code>__len__</code>,和 <code>__getitem__</code>。比如图像存储在<code>img_dir</code>目录里，标签分开存储在一个CSV 文件<code>annotations_file</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import pandas as pd</span><br><span class="line">from torchvision.io import read_image</span><br><span class="line"></span><br><span class="line">class CustomImageDataset(Dataset):</span><br><span class="line">    def __init__(self,annotations_file,img_dir,transform = None,target_transform = None):</span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        self.img_dir = img_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.traget_transform = target_transform</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.img_labels)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        # iloc[:,:]切片，左闭右开，iloc[idx,0]取idx行0列元素</span><br><span class="line">        # os.path.join路径连接</span><br><span class="line">        img_path = os.path.join(self.img_dir,self.img_labels.iloc[idx,0])</span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx,1]</span><br><span class="line">        if self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        if self.traget_transform:</span><br><span class="line">            label = self.traget_transform(label)</span><br><span class="line">        return image,label</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>****init****</p><p><code>__init__</code>函数在实例化Dataset对象时运行一次。我们初始化包含图像、注释文件和两种转换的目录。labels.csv文件内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tshirt1.jpg, 0</span><br><span class="line">tshirt2.jpg, 0</span><br><span class="line">......</span><br><span class="line">ankleboot999.jpg, 9</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):</span><br><span class="line">    self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">    self.img_dir = img_dir</span><br><span class="line">    self.transform = transform</span><br><span class="line">    self.target_transform = target_transform</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>****len****</p><p><code>__len__</code>函数返回数据集中的样本数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def __len__(self):</span><br><span class="line">    return len(self.img_labels)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>****getitem****</p><p><code>__getitem__</code>函数加载和返回数据集中给定索引<code>idx</code>位置的一个样本。基于索引，它识别图像在磁盘上的位置，使用<code>read_image</code>将其转换为张量，从<code>self.img_labels</code>csv数据中检索相应的标签。调用其上的变换函数（如果适用），并以元组形式返回张量图像和相应标签。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def __getitem__(self, idx):</span><br><span class="line">    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])</span><br><span class="line">    image = read_image(img_path)</span><br><span class="line">    label = self.img_labels.iloc[idx, 1]</span><br><span class="line">    if self.transform:</span><br><span class="line">        image = self.transform(image)</span><br><span class="line">    if self.target_transform:</span><br><span class="line">        label = self.target_transform(label)</span><br><span class="line">    return image, label</span><br><span class="line"></span><br></pre></td></tr></table></figure><hr /><h2 id="使用dataloaders预备训练数据">使用DataLoaders预备训练数据</h2><p><code>Dataset</code>一次检索一个样本的数据集特征和标签，在训练模型时，我们通常希望以“小minibatches”的方式传递样本，在每个epoch重新排列数据以减少模型过度拟合，并使用Python的<code>multiprocessing</code>加速数据检索。<code>DataLoader</code> 是一个迭代器能实现上面功能</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"># shuffle如果设置为True，则会在每个epoch重新排列数据</span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="通过dataloader进行迭代"><a href="https://blog.csdn.net/weixin_45526117/article/details/123997738?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368727216782429796331%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368727216782429796331&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-123997738-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=pytorch%E6%95%99%E7%A8%8B&amp;spm=1018.2226.3001.4187"></a>通过DataLoader进行迭代</h2><p>已经将数据加载到<code>DataLoader</code>，能够迭代遍历数据集，每次迭代都会返回批量（<code>batch_size=64</code>）的<code>train_features</code>和<code>train_labels</code>，设置了<code>shuffle=True</code>，在我们迭代所有batches之后，数据被洗牌（以便对数据加载顺序进行更细粒度的控制）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_features,train_labels = next(iter(train_dataloader))</span><br><span class="line">print(f&#x27;feature batch shape:&#123;train_features.size()&#125;&#x27;)</span><br><span class="line">print(f&#x27;label batch shape:&#123;train_labels.size()&#125;&#x27;)</span><br><span class="line">img = train_features[0].squeeze()</span><br><span class="line">label = train_labels[0]</span><br><span class="line">plt.imshow(img,cmap=&#x27;gray&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line">print(f&#x27;label:&#123;label&#125;&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">feature batch shape:torch.Size([64, 1, 28, 28])</span><br><span class="line">label batch shape:torch.Size([64])</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/54ab5732d25045c5993b1a1f4eaee2d1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAYnVqYnVqYml1,size_15,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述" /><figcaption>在这里插入图片描述</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">label:4</span><br><span class="line"></span><br></pre></td></tr></table></figure><hr /><h1 id="二transforms">（二）Transforms</h1><p>数据并不总是以训练机器学习算法所需的最终处理形式出现。我们使用<strong>transforms</strong>对数据进行一些操作，使其适合训练。所有的TorchVision数据集都有两个参数<code>transform</code>（修正特征），<code>target_transform</code>（修正标签），torchvision.transforms模块提供了几种常用的转换。<br />FashionMNIST特征是PIL图像形式, 标签是整数。为了训练，需要把特征作为归一化张量，标签作为一个one-hot编码张量。使用<code>ToTensor</code> 和<code>Lambda</code>实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision.transforms import ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">ds = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=True,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor(),</span><br><span class="line">    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="totensor"><a href="https://blog.csdn.net/weixin_45526117/article/details/123997738?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368727216782429796331%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368727216782429796331&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-123997738-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=pytorch%E6%95%99%E7%A8%8B&amp;spm=1018.2226.3001.4187"></a>ToTensor( )</h2><p><code>ToTensor</code>将一个<code>PIL image</code>或者NumPy 数组<code>ndarray</code>变成浮点型张量<code>FloatTensor</code>，在[0,1]范围内缩放图像的像素强度值</p><h2 id="lambda-transforms">Lambda Transforms</h2><p>Lambda transforms应用任何用户定义的Lambda函数，此处定义了一个函数将整数变成one-hot编码张量，首先创建一个大小为10（标签数）的全0张量，然后调用<code>scatter_</code> 在标签y的索引位置上将值修改为1</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">target_transform = Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>Tensor.scatter_(dim, index, src, reduce=None)</code>在dim维度上，找到index对应的元素，将值换成src</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(3), value=1))</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])</span><br><span class="line"></span><br></pre></td></tr></table></figure><hr /><h1 id="三构建神经网络">（三）构建神经网络</h1><p>使用pytorch构建神经网络进行FashionMNIST数据集中的图像分类</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torchvision import datasets, transforms</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;</span><br><span class="line">print(&#x27;using &#123;&#125; device&#x27;.format(device))</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">using cpu device</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="定义神经网络类">定义神经网络类</h2><p>继承<code>nn.Module</code>构建神经网络，包括两个部分</p><ul><li><code>__init__</code>：定义网络层</li><li><code>forward</code>：执行前向传播</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class network(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(network, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten() # 将连续范围的维度拉平成张量</span><br><span class="line">        self.layers = nn.Sequential(</span><br><span class="line">            nn.Linear(28*28,512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512,512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512,10))</span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x = self.flatten(x) # 输入到网络中的是（batch_size,input)</span><br><span class="line">        values = self.layers(x)</span><br><span class="line">        return values</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>torch.nn.Flatten(start_dim=1, end_dim=- 1)</code>默认只保留第一维度</p><ul><li><p>start_dim：first dim to flatten (default = 1).</p></li><li><p>end_dim：last dim to flatten (default = -1).</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># torch.nn.Flatten示例</span><br><span class="line">input = torch.randn(32,1,5,5)</span><br><span class="line">m = nn.Flatten()</span><br><span class="line">output = m(input)</span><br><span class="line">print(output.size())</span><br><span class="line">m1 = nn.Flatten(0,2)</span><br><span class="line">print(m1(input).size())</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([32, 25])</span><br><span class="line">torch.Size([160, 5])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>创建一个<code>network</code>实例并移动到 <code>device</code>，输出结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = network().to(device)</span><br><span class="line">print(model)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">network(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (layers): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>遍历输入数据，执行模型前向传播，不用直接调用<code>forward</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(2,28,28,device=device)</span><br><span class="line">value = model(x)</span><br><span class="line">print(value)</span><br><span class="line">print(value.size())</span><br><span class="line">pred_probab = nn.Softmax(dim=1)(value)</span><br><span class="line">print(pred_probab)</span><br><span class="line">y_pred = pred_probab.argmax(1)</span><br><span class="line">print(f&#x27;predicted class:&#123;y_pred&#125;&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.0355,  0.0948, -0.1048,  0.0802,  0.0177,  0.0038, -0.0281, -0.0767,</span><br><span class="line">          0.0303, -0.1290],</span><br><span class="line">        [-0.0238,  0.1298, -0.0700,  0.0861,  0.0168, -0.0418, -0.0421, -0.0772,</span><br><span class="line">          0.0369, -0.1391]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line">torch.Size([2, 10])</span><br><span class="line">tensor([[0.0977, 0.1113, 0.0912, 0.1097, 0.1030, 0.1016, 0.0984, 0.0938, 0.1043,</span><br><span class="line">         0.0890],</span><br><span class="line">        [0.0986, 0.1149, 0.0941, 0.1100, 0.1027, 0.0968, 0.0968, 0.0935, 0.1048,</span><br><span class="line">         0.0878]], grad_fn=&lt;SoftmaxBackward0&gt;)</span><br><span class="line">predicted class:tensor([1, 1])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>torch.nn.Softmax(dim=None)</code>softmax归一化</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># torch.nn.Softmax示例</span><br><span class="line">m = nn.Softmax(dim=1)</span><br><span class="line">input = torch.randn(2,3)</span><br><span class="line">print(input)</span><br><span class="line">output = m(input)</span><br><span class="line">print(output)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.5471,  1.3495,  1.5911],</span><br><span class="line">        [-0.0185, -0.1420, -0.0556]])</span><br><span class="line">tensor([[0.0619, 0.4126, 0.5254],</span><br><span class="line">        [0.3512, 0.3104, 0.3384]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><hr /><h2 id="模型结构层">模型结构层</h2><p>拆解模型中的层次，观察输入和输出</p><p>原始输入</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input_image = torch.rand(3,28,28)</span><br><span class="line">print(input_image.size())</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 28, 28])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="nn.flatten">nn.Flatten</h3><p>将2维的28✖️28图像变成784像素值，batch维度（dim=0）保留</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image)</span><br><span class="line">print(flat_image.size())</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 784])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="nn.linear">nn.Linear</h3><p>线性转换</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">layer1 = nn.Linear(in_features=28*28,out_features=20)</span><br><span class="line">hidden1 = layer1(flat_image)</span><br><span class="line">print(hidden1.size( ))</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 20])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="nn.relu">nn.ReLU</h3><p>非线性修正单元（激活函数）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(f&quot;Before ReLU: &#123;hidden1&#125;\n\n&quot;)</span><br><span class="line">hidden1 = nn.ReLU()(hidden1)</span><br><span class="line">print(f&quot;After ReLU: &#123;hidden1&#125;&quot;)</span><br><span class="line">print(hidden1.size())</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Before ReLU: tensor([[ 0.4574, -0.5313, -0.4628, -0.9403, -0.7630,  0.1807, -0.2847, -0.2741,</span><br><span class="line">          0.0954,  0.2327,  0.4603,  0.0227, -0.1299, -0.2346, -0.1800,  0.9115,</span><br><span class="line">         -0.0870, -0.0171, -0.0064,  0.0540],</span><br><span class="line">        [ 0.0888, -0.6782, -0.2557, -0.6717, -0.4488,  0.1024, -0.3013, -0.3186,</span><br><span class="line">         -0.1338,  0.3944,  0.0704,  0.1429,  0.0521, -0.3326, -0.3113,  0.6518,</span><br><span class="line">         -0.0978, -0.0721, -0.3396,  0.4712],</span><br><span class="line">        [ 0.1781,  0.0885, -0.4775, -0.5661, -0.0099,  0.2617, -0.2678, -0.1444,</span><br><span class="line">          0.1345,  0.3259,  0.3984,  0.2392,  0.0529, -0.0349, -0.3266,  0.7488,</span><br><span class="line">         -0.3498,  0.1157,  0.0126,  0.3502]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">After ReLU: tensor([[0.4574, 0.0000, 0.0000, 0.0000, 0.0000, 0.1807, 0.0000, 0.0000, 0.0954,</span><br><span class="line">         0.2327, 0.4603, 0.0227, 0.0000, 0.0000, 0.0000, 0.9115, 0.0000, 0.0000,</span><br><span class="line">         0.0000, 0.0540],</span><br><span class="line">        [0.0888, 0.0000, 0.0000, 0.0000, 0.0000, 0.1024, 0.0000, 0.0000, 0.0000,</span><br><span class="line">         0.3944, 0.0704, 0.1429, 0.0521, 0.0000, 0.0000, 0.6518, 0.0000, 0.0000,</span><br><span class="line">         0.0000, 0.4712],</span><br><span class="line">        [0.1781, 0.0885, 0.0000, 0.0000, 0.0000, 0.2617, 0.0000, 0.0000, 0.1345,</span><br><span class="line">         0.3259, 0.3984, 0.2392, 0.0529, 0.0000, 0.0000, 0.7488, 0.0000, 0.1157,</span><br><span class="line">         0.0126, 0.3502]], grad_fn=&lt;ReluBackward0&gt;)</span><br><span class="line">torch.Size([3, 20])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="nn.sequential">nn.Sequential</h3><p><code>nn.Sequential</code> 是一个模块的有序容纳器，数据按照定义的顺序传递给所有模块</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seq_modules = nn.Sequential(flatten,layer1,nn.ReLU(),nn.Linear(20,10))</span><br><span class="line">input_image = torch.randn(3,28,28)</span><br><span class="line">values1 = seq_modules(input_image)</span><br><span class="line">print(values1)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.2472,  0.2597, -0.0157,  0.3206, -0.0073,  0.1631,  0.2956,  0.0561,</span><br><span class="line">          0.2993,  0.1807],</span><br><span class="line">        [-0.0782,  0.1838, -0.0215,  0.2395, -0.0804, -0.0021,  0.0883, -0.0698,</span><br><span class="line">          0.1463, -0.0151],</span><br><span class="line">        [-0.1162,  0.0673, -0.2301,  0.1612, -0.1472, -0.0447,  0.0671, -0.2915,</span><br><span class="line">          0.3176,  0.2391]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="nn.softmax">nn.Softmax</h3><p>神经网络的最后一个线性层返回原始值在[-\infty, \infty]，经过<code>nn.Softmax</code>模块，输出值在[0, 1]，代表了每个类别的预测概率，<code>dim</code>参数表示改维度的值总和为1</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">softmax = nn.Softmax(dim=1)</span><br><span class="line">pred_probab1 = softmax(values1)</span><br><span class="line">print(pred_probab1)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.1062, 0.1075, 0.0816, 0.1143, 0.0823, 0.0976, 0.1115, 0.0877, 0.1119,</span><br><span class="line">         0.0994],</span><br><span class="line">        [0.0884, 0.1148, 0.0935, 0.1214, 0.0882, 0.0954, 0.1044, 0.0891, 0.1106,</span><br><span class="line">         0.0941],</span><br><span class="line">        [0.0872, 0.1048, 0.0778, 0.1151, 0.0845, 0.0937, 0.1048, 0.0732, 0.1346,</span><br><span class="line">         0.1244]], grad_fn=&lt;SoftmaxBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="模型参数">模型参数</h2><p>使用<code>parameters()</code>和<code>named_parameters()</code>能获取每层的参数,包括weight和bias</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(f&#x27;model structure:&#123;model&#125;\n&#x27;)</span><br><span class="line"></span><br><span class="line">for name,param in model.named_parameters():</span><br><span class="line">    print(f&#x27;layer:&#123;name&#125;|size&quot;&#123;param.size()&#125;|param:&#123;param[:2]&#125;\n&#x27;)</span><br><span class="line"></span><br><span class="line">#print(model.parameters())</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">model structure:network(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (layers): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">layer:layers.0.weight|size&quot;torch.Size([512, 784])|param:tensor([[ 0.0122, -0.0204, -0.0185,  ..., -0.0196,  0.0257, -0.0084],</span><br><span class="line">        [-0.0066, -0.0195, -0.0199,  ..., -0.0175, -0.0007,  0.0003]],</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">layer:layers.0.bias|size&quot;torch.Size([512])|param:tensor([0.0086, 0.0104], grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">layer:layers.2.weight|size&quot;torch.Size([512, 512])|param:tensor([[-0.0306, -0.0408,  0.0062,  ...,  0.0289, -0.0164,  0.0099],</span><br><span class="line">        [ 0.0015,  0.0052,  0.0182,  ...,  0.0431, -0.0174,  0.0049]],</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">layer:layers.2.bias|size&quot;torch.Size([512])|param:tensor([-0.0337,  0.0294], grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">layer:layers.4.weight|size&quot;torch.Size([10, 512])|param:tensor([[ 0.0413,  0.0015,  0.0388,  ...,  0.0347,  0.0160,  0.0221],</span><br><span class="line">        [-0.0010,  0.0031,  0.0421,  ..., -0.0226,  0.0340, -0.0220]],</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">layer:layers.4.bias|size&quot;torch.Size([10])|param:tensor([0.0210, 0.0243], grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><hr /><h1 id="四自动差分-torch.autograd">（四）自动差分 <code>torch.autograd</code></h1><p>训练神经网络使用最频繁的算法是反向传播<strong>back propagation</strong>，参数(model weights)根据损失函数的梯度<strong>gradient</strong>进行调整。为了计算梯度，pytorch内置了<br />差分引擎<code>torch.autograd</code>，支持任何一个计算图的梯度计算，以最简单的单层神经网络为例，输入<code>x</code>，参数<code>w</code>和<code>b</code>和一些损失函数，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(5)  # 输入张量</span><br><span class="line">y = torch.zeros(3)  # 预期输出</span><br><span class="line">w = torch.randn(5, 3, requires_grad=True)</span><br><span class="line">b = torch.randn(3, requires_grad=True)</span><br><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)</span><br><span class="line">print(loss)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(2.2890, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="张量函数计算图">张量，函数，计算图</h2><p>代码定义了如下的计算图 <strong>computational graph</strong>:<br /><img src="https://img-blog.csdnimg.cn/5f846b5d81e844c7b9e6be0bc2dbcf2a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAYnVqYnVqYml1,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" /></p><p><code>w</code> 和 <code>b</code> 是需要优化的参数，因此需要计算这些变量各自对损失函数的梯度，设置张量的<code>requires_grad</code>属性</p><p>可以在创建一个张量的时候设置<code>requires_grad</code>的值，或者之后使用<code>x.requires_grad_(True)</code>方法，用在张量上实现前向传播和反向传播的函数是类<code>Function</code>的实例，反向传播函数存储在张量的<code>grad_fn</code>属性上</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(f&#x27;gradient function for z=&#123;z.grad_fn&#125;\n&#x27;)</span><br><span class="line">print(f&#x27;gradient function for loss=&#123;loss.grad_fn&#125;\n&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gradient function for z=&lt;AddBackward0 object at 0x7fb47069aa30&gt;</span><br><span class="line"></span><br><span class="line">gradient function for loss=&lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x7fb47069a250&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="计算梯度">计算梯度</h2><p>为了优化网络参数的权重，需要计算<code>x</code> 和 <code>y</code>固定值下损失函数对各参数的导数 ∂ l o s s ∂ w \frac{\partial loss}{\partial w} ∂w∂loss​和 ∂ l o s s ∂ b \frac{\partial loss}{\partial b} ∂b∂loss​ ，为了计算这些导数，需要调用<code>loss.backward()</code>，通过<code>w.grad</code>和<code>b.grad</code>获取梯度值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.3263, 0.0754, 0.3122],</span><br><span class="line">        [0.3263, 0.0754, 0.3122],</span><br><span class="line">        [0.3263, 0.0754, 0.3122],</span><br><span class="line">        [0.3263, 0.0754, 0.3122],</span><br><span class="line">        [0.3263, 0.0754, 0.3122]])</span><br><span class="line">tensor([0.3263, 0.0754, 0.3122])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>只能获取计算图叶子节点的<code>grad</code>属性，其<code>requires_grad</code>设置为true，对于其它节点，梯度不可获取；出于性能原因，只能在给定的图形上使用“backward”进行一次梯度计算。如果要在同一个图上执行好几次“backward”调用，将“retain_graph=True”传递给“backward”调用</p><h2 id="禁用梯度跟踪">禁用梯度跟踪</h2><p>设置<code>requires_grad=True</code>的张量会追踪计算历史并且支持梯度计算，但是某些情况下，不需要这么做，比如模型已经完成训练后，将其应用在输入数据上，只需执行前向传播_forward_，可以通过<code>torch.no_grad()</code>阻止跟踪计算</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x,w) + b</span><br><span class="line">print(z.requires_grad)</span><br><span class="line"></span><br><span class="line">with torch.no_grad():</span><br><span class="line">    z = torch.matmul(x,w) + b</span><br><span class="line">print(z.requires_grad)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">True</span><br><span class="line">False</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>另一种有同样效果的方法是对张量使用<code>detach()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">z_det = z.detach()</span><br><span class="line">print(z_det.requires_grad)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">False</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>禁用梯度跟踪有以下原因：</p><ul><li>将神经网络中的一些参数标记为<strong>frozen parameters</strong>，在微调预训练网络中比较常见</li><li>只进行前向传播中加速计算<strong>speed up computations</strong>，没有梯度跟踪的向量计算更高效</li></ul><h2 id="计算图computational-graphs">计算图Computational Graphs</h2><p>从概念上讲，autograd在由函数对象组成的有向无环图（DAG）中记录数据（张量）和所有执行的操作（以及生成的新张量）。在这个DAG中，叶是输入张量，根是输出张量。通过从根到叶追踪此图，可以使用链式规则自动计算梯度<br />在前向传播中，autograd自动做两件事：</p><ul><li>运行请求的操作以计算结果张量</li><li>在DAG中保留操作的梯度函数_gradient function_</li></ul><p>DAG根的<code>.backward()</code>被调用时，<code>autograd</code> ：</p><ul><li>依照每个<code>.grad_fn</code>计算梯度</li><li>将其累计到各自张量的<code>.grad</code>属性中</li><li>使用链式规则传播到叶张量</li></ul><p><strong>DAGs are dynamic in PyTorch</strong>，图表是从头开始创建的，在调用<code>.backward()</code>后，autograd开始填充新图形，这也是模型中能使用控制流语句的原因，在每次迭代，都可以改变形状，大小和操作</p><h1 id="五优化模型参数">（五）优化模型参数</h1><p>有了模型和数据后需要通过优化参数进行模型训练，验证和测试。训练模型是一个迭代的过程，每次迭代（也叫一个_epoch_），模型会对输出进行预测，计算预测误差(<em>loss</em>)，收集误差对各参数的导数。使用梯度下降优化这些参数。</p><p>之前数据加载和神经网络代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision.transforms import ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">train_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=True,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=False,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=64)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64)</span><br><span class="line"></span><br><span class="line">class NeuralNetwork(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.layers = nn.Sequential(</span><br><span class="line">            nn.Linear(28*28, 512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512, 512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512, 10),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        values = self.layers(x)</span><br><span class="line">        return values</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="超参数hyperparameters">超参数Hyperparameters</h2><p>超参数是可调整的参数，用来控制模型优化过程，不同的超参数值能影响模型训练和收敛速度</p><p>定义如下的超参数用于训练：</p><ul><li><strong>Number of Epochs</strong>：迭代次数</li><li><strong>Batch Size</strong>：参数更新前通过网络传播的数据样本数量</li><li><strong>Learning Rate</strong>：学习率</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = 1e-3</span><br><span class="line">batch_size = 64</span><br><span class="line">epochs = 5</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="优化循环optimization-loop">优化循环Optimization Loop</h2><p>一旦设定了超参数，可以通过一个优化循环来训练和优化我们的模型。优化循环的每次迭代称为<strong>epoch</strong>。每个epoch包括两个主要的部分：</p><ul><li><strong>The Train Loop</strong>：迭代训练数据集，并尝试收敛到最佳参数。</li><li><strong>The Validation/Test Loop</strong>：迭代测试数据集，检查模型性能是否正在改善。</li></ul><h3 id="loss-function"><a href="https://blog.csdn.net/weixin_45526117/article/details/123997738?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368727216782429796331%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368727216782429796331&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-123997738-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=pytorch%E6%95%99%E7%A8%8B&amp;spm=1018.2226.3001.4187"></a>loss function</h3><p>当面对一些训练数据时，我们未经训练的网络可能不会给出正确的答案。损失函数衡量获得的结果与目标值的不同程度，我们希望在训练过程中最小化损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实数据标签值进行比较。</p><p>普通的损失函数包括适合回归任务的<code>nn.MSELoss</code>（均方误差），适合分类的<code>nn.NLLLoss</code>（负对数似然），<code>nn.CrossEntropyLoss</code>结合了<code>nn.LogSoftmax</code>和<code>nn.NLLLoss</code>。此处使用<code>nn.CrossEntropyLoss</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 初始化损失函数</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="optimizer">Optimizer</h3><p>优化是在每个训练步骤中调整模型参数以减少模型误差的过程。优化算法定义了该过程的执行方式（在本例中使用随机梯度下降）。所有优化逻辑都封装在优化器对象中。这里使用SGD优化器；此外，pytorch中有许多不同的优化器，例如ADAM和RMSProp，它们可以更好地用于不同类型的模型和数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 定义优化器</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>训练循环中，优化主要有三个步骤：</p><ul><li>调用<code>optimizer.zero_grad()</code>重置模型参数的梯度，梯度默认会累积，为了阻止重复计算，在每次迭代都会清零</li><li>调用<code>loss.backward()</code>进行反向传播</li><li>一旦有了梯度，就调用<code>optimizer.step()</code>来调整各参数值</li></ul><h2 id="训练循环和测试循环">训练循环和测试循环</h2><p>定义<code>train_loop</code>训练优化，定义<code>test_loop</code>评估模型在测试集的表现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">def train_loop(dataloader, model, loss_fn, optimizer):</span><br><span class="line">    size = len(dataloader.dataset) # 训练集数据总量</span><br><span class="line">    for number, (x, y) in enumerate(dataloader):</span><br><span class="line">        # number迭代次数，每次迭代输入batch=64的张量(64,1,28,28)</span><br><span class="line">        # 计算预测和误差</span><br><span class="line">        pred = model(x)</span><br><span class="line">        loss = loss_fn(pred, y)</span><br><span class="line"></span><br><span class="line">        # 反向传播</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        if number % 100 == 0:</span><br><span class="line">            # 每迭代100次，输出当前损失函数值及遍历进度</span><br><span class="line">            loss, current = loss.item(), number * len(x) # current当前已经遍历的图像数,len(x)=batch_size</span><br><span class="line">            print(f&quot;loss: &#123;loss:&gt;7f&#125;  [&#123;current:&gt;5d&#125;/&#123;size:&gt;5d&#125;]&quot;)</span><br><span class="line"></span><br><span class="line">def test_loop(dataloader, model, loss_fn):</span><br><span class="line">    size = len(dataloader.dataset) # 测试集总量</span><br><span class="line">    num_batches = len(dataloader) # 最大迭代次数</span><br><span class="line">    test_loss, correct = 0, 0</span><br><span class="line"></span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for x, y in dataloader:</span><br><span class="line">            pred = model(x)</span><br><span class="line">            test_loss += loss_fn(pred, y).item()</span><br><span class="line">            # 输出如：test_loss=torch.tensor(1.0873)</span><br><span class="line">            # pred.argmax(1)返回值最大值对应的位置，sum()求批量的正确数</span><br><span class="line">            correct += (pred.argmax(1) == y).type(torch.float).sum().item()</span><br><span class="line"></span><br><span class="line">    test_loss /= num_batches # 单次迭代的误差总和/总迭代次数=平均误差</span><br><span class="line">    correct /= size # 单次迭代的正确数总和/数据总量=准确率</span><br><span class="line">    print(f&quot;Test Error: \n Accuracy: &#123;(100*correct):&gt;0.1f&#125;%, Avg loss: &#123;test_loss:&gt;8f&#125; \n&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(len(train_dataloader.dataset))</span><br><span class="line">print(len(train_dataloader))</span><br><span class="line">print(len(test_dataloader.dataset))</span><br><span class="line">print(len(test_dataloader))</span><br><span class="line">x,y = next(iter(train_dataloader))</span><br><span class="line">print(len(x))</span><br><span class="line">print(x.size())</span><br><span class="line">print(y.size())</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">60000</span><br><span class="line">938</span><br><span class="line">10000</span><br><span class="line">157</span><br><span class="line">64</span><br><span class="line">torch.Size([64, 1, 28, 28])</span><br><span class="line">torch.Size([64])</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">epochs = 2</span><br><span class="line">for t in range(epochs):</span><br><span class="line">    print(f&quot;Epoch &#123;t+1&#125;\n-------------------------------&quot;)</span><br><span class="line">    train_loop(train_dataloader, model, loss_fn, optimizer)</span><br><span class="line">    test_loop(test_dataloader, model, loss_fn)</span><br><span class="line">print(&quot;Done!&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1</span><br><span class="line">-------------------------------</span><br><span class="line">loss: 1.040251  [    0/60000]</span><br><span class="line">loss: 1.070957  [ 6400/60000]</span><br><span class="line">loss: 0.869483  [12800/60000]</span><br><span class="line">loss: 1.033000  [19200/60000]</span><br><span class="line">loss: 0.908716  [25600/60000]</span><br><span class="line">loss: 0.930925  [32000/60000]</span><br><span class="line">loss: 0.973219  [38400/60000]</span><br><span class="line">loss: 0.913604  [44800/60000]</span><br><span class="line">loss: 0.960071  [51200/60000]</span><br><span class="line">loss: 0.904625  [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 67.1%, Avg loss: 0.911718 </span><br><span class="line"></span><br><span class="line">Epoch 2</span><br><span class="line">-------------------------------</span><br><span class="line">loss: 0.952776  [    0/60000]</span><br><span class="line">loss: 1.005409  [ 6400/60000]</span><br><span class="line">loss: 0.788150  [12800/60000]</span><br><span class="line">loss: 0.969153  [19200/60000]</span><br><span class="line">loss: 0.852390  [25600/60000]</span><br><span class="line">loss: 0.862806  [32000/60000]</span><br><span class="line">loss: 0.920238  [38400/60000]</span><br><span class="line">loss: 0.863878  [44800/60000]</span><br><span class="line">loss: 0.903000  [51200/60000]</span><br><span class="line">loss: 0.858517  [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 68.3%, Avg loss: 0.859433 </span><br><span class="line"></span><br><span class="line">Done!</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="六保存和加载模型">(六）保存和加载模型</h1><p>最后了解如何通过保存、加载和运行模型预测来保持模型状态。<code>torchvision.models</code>子包包含用于处理不同任务的模型定义，包括：图像分类、像素语义分割、对象检测、实例分割、人物关键点检测、视频分类和光流。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision.models as models</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="保存和加载模型权重">保存和加载模型权重</h2><p>pytorch将学习的参数存储在内部状态字典中，叫做<code>state_dict</code>，这些能通过<code>torch.save</code>方法被保留</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># vgg16是一种图像分类的模型结构</span><br><span class="line">import ssl</span><br><span class="line">ssl._create_default_https_context = ssl._create_unverified_context</span><br><span class="line"></span><br><span class="line">model = models.vgg16(pretrained=True) # 以vgg16模型为例</span><br><span class="line">torch.save(model.state_dict(), &#x27;model_weights.pth&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>要加载模型权重，需要先创建同一模型的实例，然后使用<code>load_state_dict()</code>方法加载参数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = models.vgg16() # 不指定 pretrained=True,也就是不加载默认参数</span><br><span class="line">model.load_state_dict(torch.load(&#x27;model_weights.pth&#x27;))</span><br><span class="line">model.eval()</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (3): ReLU(inplace=True)</span><br><span class="line">    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (6): ReLU(inplace=True)</span><br><span class="line">    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (8): ReLU(inplace=True)</span><br><span class="line">    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (13): ReLU(inplace=True)</span><br><span class="line">    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (15): ReLU(inplace=True)</span><br><span class="line">    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (18): ReLU(inplace=True)</span><br><span class="line">    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (20): ReLU(inplace=True)</span><br><span class="line">    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (22): ReLU(inplace=True)</span><br><span class="line">    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (25): ReLU(inplace=True)</span><br><span class="line">    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (27): ReLU(inplace=True)</span><br><span class="line">    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (29): ReLU(inplace=True)</span><br><span class="line">    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=4096, bias=True)</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (3): Linear(in_features=4096, out_features=4096, bias=True)</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (6): Linear(in_features=4096, out_features=1000, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在预测前一定要先调用<code>model.eval()</code>方法来设置dropout和batch normalization层为评估模型，否则会导致不一致的预测结果</p><h2 id="保存和加载模型">保存和加载模型</h2><p>加载模型权重时，我们需要首先实例化模型类，因为该类定义了网络的结构。为了将这个类的结构与模型一起保存，可以将<code>model</code>（而不是<code>model.state_dict（）</code>）传递给保存的函数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model, &#x27;model.pth&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>加载模型：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = torch.load(&#x27;model.pth&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这种方法在序列化模型时使用Python的pickle模块，因此它依赖于加载模型时可用的实际类定义。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态时间规整</title>
      <link href="/2023/01/14/DTWl/"/>
      <url>/2023/01/14/DTWl/</url>
      
        <content type="html"><![CDATA[<h2 id="dtw-定义">DTW 定义</h2><p>动态时间规整（Dynamic Time Warping，DTW）用于<strong>比较具有不同长度的两个阵列或时间序列之间的相似性或距离</strong>。</p><p>假设要计算两个等长数组的距离：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = [1, 2, 3]</span><br><span class="line">b = [3, 2, 2]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>最简单的使用欧氏距离进行计算，但如果 a 和 b 的长度不同怎么办？</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = [1, 2, 3]</span><br><span class="line">b = [2, 2, 2, 3, 4]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>DTW 解决了这个问题，正如其名，规整序列以使其匹配。比较不同长度的数组的想法是<strong>建立一对多和多对一的匹配，这样两者之间的总距离可以最小化</strong>。</p><p>假设我们有两个不同的数组，红色和蓝色，不同的长度：<br /><img src="https://img-blog.csdnimg.cn/5bc5151578d444cda1c323a70dca0402.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAZGF0YW1vbmRheQ==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" /><br /><strong>显然这两个序列遵循相同的模式，但蓝色曲线比红色曲线长</strong>。如果我们应用顶部所示的一对一匹配，则映射不会完全同步，蓝色曲线的尾部会被遗漏。DTW 通过开发一对一解决了这个问题许多匹配使得具有相同模式的波谷和波峰完美匹配，并且两条曲线都没有遗漏（图片底部）。</p><h2 id="dtw-计算过程"><a href="https://blog.csdn.net/weixin_39653948/article/details/124416873?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=dtw%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-124416873.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;spm=1018.2226.3001.4187"></a>DTW 计算过程</h2><p>DTW 是一种计算两个给定序列（例如时间序列）之间的最佳匹配的方法：</p><ul><li>第一个序列中的每个索引必须与另一个序列中的一个或多个索引匹配，反之亦然</li><li>第一个序列的第一个索引必须与另一个序列的第一个索引匹配（但它不必是唯一匹配）</li><li>第一个序列的最后一个索引必须与另一个序列的最后一个索引匹配（但不一定是唯一匹配）</li><li><strong>第一个序列的索引到另一个序列的索引的映射必须是单调递增的，并且反之亦然</strong>，即如果 j &gt; i 是来自第一个序列的索引，则在另一个序列中不能有两个索引 l &gt; k，这样索引 i 与索引 l 匹配，索引 j 与索引 k 匹配，反之亦然</li></ul><p>最佳匹配由满足所有其余部分的匹配表示 rictions 和 rules 并且具有最小代价，其中代价计算为每个匹配的索引对在它们的值之间的绝对差的总和。</p><p>总而言之，head 和 tail 必须在位置上匹配，没有交叉匹配，也没有被遗漏。</p><h1 id="dtw-形式化定义"><a href="https://blog.csdn.net/weixin_39653948/article/details/124416873?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=dtw%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-124416873.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;spm=1018.2226.3001.4187"></a>DTW 形式化定义</h1><p>动态时间规整（DTW）可以在一定的限制下找到两个给定（时间相关）序列之间的最佳对齐（图 4.1）。直观地说，序列以非线性方式规整以相互匹配。最初，DTW 已被用于比较自动语音识别中的不同语音模式，参见 [170]。在数据挖掘和信息检索等领域，DTW 已成功应用于自动处理与时间相关时变和不同速度的数据。<br /><img src="https://img-blog.csdnimg.cn/c0028087889a44ffad2981f9f1bb7eb5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAZGF0YW1vbmRheQ==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" /></p><h2 id="经典的-dtw"><a href="https://blog.csdn.net/weixin_39653948/article/details/124416873?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=dtw%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-124416873.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;spm=1018.2226.3001.4187"></a>经典的 DTW</h2><p><img src="https://img-blog.csdnimg.cn/ceafeda8ac8c42d9b6cca6f98f6ad438.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAZGF0YW1vbmRheQ==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" /><br />DTW 的目标是比较长度为 N ∈ N N ∈ \N N∈N 的两个（时间相关）序列 X : = ( x 1 , x 2 , . . . , x N ) X := (x_1, x_2, ..., x_N) X:=(x1​,x2​,...,xN​) 和长度为 Y : = ( y 1 , y 2 , . . . , y M ) M ∈ N Y := (y_1, y_2, ..., y_M)\ M ∈ \N Y:=(y1​,y2​,...,yM​) M∈N。这些序列可能是离散信号（时间序列），更一般地说，是在等距时间点采样的特征序列。在下文中，我们固定一个由 F \mathcal{F} F 表示的特征空间。对于 n ∈ [ 1 : N ] n \in [1 : N] n∈[1:N] 和 m ∈ [ 1 : M ] m \in [1 : M] m∈[1:M]， x n , y m ∈ F x_n, y_m \in \mathcal{F} xn​,ym​∈F 。为了<strong>比较两个不同的特征 x , y ∈ F x, y \in \mathcal{F} x,y∈F</strong>，需要一个局部代价度量，有时也称为<strong>局部距离度量，它被定义为一个函数</strong><br /><img src="https://img-blog.csdnimg.cn/197eab578d0f4d14a4bbd67eed40b462.png" alt="在这里插入图片描述" /><br /><strong>通常，如果 x x x 和 y y y 彼此相似，则 c ( x , y ) c(x, y) c(x,y) 小（代价低），否则 c ( x , y ) c(x, y) c(x,y) 大（代价高）</strong>。评估序列 X X X 和 Y Y Y 的每一对元素的局部代价度量，得到由 C ( n , m ) : = c ( x n , y m ) C(n, m) : = c(x_n, y_m) C(n,m):=c(xn​,ym​) 定义的<strong>代价矩阵</strong> C ∈ R N × M C ∈ \R^{N×M} C∈RN×M，见图 4.2。然后<strong>目标是在 X X X 和 Y Y Y 之间找到一个总体代价最小的对齐方式</strong>。直观地说，<strong>这样的最佳对齐沿着代价矩阵 C C C 内的低代价“谷”运行</strong>，参见图 4.4 的说明。下一个定义形式化了<strong>对齐</strong>的概念。<br /><img src="https://img-blog.csdnimg.cn/d036edd7b8f542b48e52f3242e717d72.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAZGF0YW1vbmRheQ==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" /><br /><img src="https://img-blog.csdnimg.cn/0603e1c54e8e408eae57f78d4fd25400.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAZGF0YW1vbmRheQ==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" /><br />注意，<strong>步长条件 (iii) 意味着单调性条件 (ii)</strong>。 ( N , M ) (N, M) (N,M)-warping path p = ( p 1 , . . . , p L ) p = (p_1, ..., p_L) p=(p1​,...,pL​) 定义了两个序列 X = ( x 1 , x 2 , . . . , x N ) X = (x_1, x_2, ..., x_N ) X=(x1​,x2​,...,xN​) 和 Y = ( y 1 , y 2 , . . . , y M ) Y = (y_1, y_2, ... , y_M) Y=(y1​,y2​,...,yM​) 之间的对齐，通过将 X X X 的元素 x n l x_{n_l} xnl​​ 分配给 Y Y Y 的元素 y m l y_{m_l} yml​​ 。</p><ul><li><strong>边界条件强制 X X X 和 Y Y Y 的第一个元素以及 X X X 和 Y Y Y 的最后一个元素相互对齐。换句话说，对齐（alignment）是指整个序列 X X X 和 Y Y Y</strong>。</li><li>单调性条件反映了<strong>忠实计时（faithful timing）的要求</strong>：如果 X X X 中的一个元素先于第二个元素，这也应该适用于 Y Y Y 中的相应元素，反之亦然。</li><li>步长条件表达了一种<strong>连续性条件</strong>： X X X 和 Y Y Y 中的任何元素都不能省略，并且<strong>对齐中没有重复，即规整路径 p p p 中包含的所有索引对都是成对不同的</strong>。</li></ul><p>图 4.3 说明了这三个条件。<strong>如果理解了上边提到的三个条件，这三幅图也很好明白。图 b 违反了边界条件，即第一个元素和最后一元素都没有分别对齐。图 c 违反了单调性，即序列 Y 的时刻超过了 X 的时刻（横轴的 3,4），因为 Y 是短的序列，它的元素索引不能大于较长的序列 X 否则会导致重复映射的情况，因为后边的元素不够分了，这在图 c 中也可以很清晰的看出来。图 d 违反了连续性条件，即两个序列之间的对齐不能跳过元素</strong>。<br /><img src="https://img-blog.csdnimg.cn/ff93b8aa56554828a05845dfaec37b0d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAZGF0YW1vbmRheQ==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" /><br />X X X 和 Y Y Y 之间相对于局部代价度量 c c c 的总代价 c p ( X , Y ) c_p(X, Y ) cp​(X,Y) 定义为<br /><img src="https://img-blog.csdnimg.cn/3462863c5a8b47b598a8da00952665a6.png" alt="在这里插入图片描述" /><br />此外， <strong>X X X 和 Y Y Y 之间的最佳规整路径（wraping path）是在所有可能的规整路径中总代价最小的规整路径 p ∗ p^* p∗</strong>。然后将 X X X 和 Y Y Y 之间的 DTW 距离 D T W ( X , Y ) DTW(X, Y) DTW(X,Y) 定义为 p ∗ p^* p∗ 的总代价：<br /><img src="https://img-blog.csdnimg.cn/aebd655c18414c1480fe69c03edfe3a6.png" alt="在这里插入图片描述" /><br />我们继续对 DTW 距离进行一些评论。</p><ul><li>首先，请注意 DTW 距离是明确定义的，即使可能存在若干条总代价最低的规整路径（wrapping path）。</li><li><strong>其次，在局部代价度量 c c c 对称的情况下，很容易看出 DTW 距离是对称的</strong>。然而，<strong>即使 c c c 成立，DTW 距离通常也不是正定的</strong>。例如，在 c ( x 1 ， x 1 ) = c ( x 2 ， x 2 ) = 0 c(x_1，x_1) = c(x_2，x_2) = 0 c(x1​，x1​)=c(x2​，x2​)=0 的情况下，对于序列 X : = ( x 1 ， x 2 ) X := (x_1，x_2) X:=(x1​，x2​) 和 Y : = ( x 1 ， x 1 ， x 2 ， x 2 ) Y := (x_1，x_1，x_2，x_2) Y:=(x1​，x1​，x2​，x2​)，可以得到 D T W ( X ， Y ) = 0 DTW(X，Y ) = 0 DTW(X，Y)=0。</li><li>此外，即使在 c c c 是度量的情况下，<strong>DTW 距离通常也不满足三角形不等式</strong>。</li></ul><p>下面的例子说明了这一事实。</p><p>例 4.2。设 F : = { α ， β ， γ } F := \{α，β，γ\} F:={α，β，γ} 是由三个特征组成的特征空间。我们定义一个代价度量 c : F × F → { 0 , 1 } c : \mathcal{F} × \mathcal{F} \rightarrow \{0, 1\} c:F×F→{0,1}，通过以下规则：对于 x , y ∈ F x, y \in \mathcal{F} x,y∈F，如果 x = y x = y x=y，则 c ( x ， y ) : = 0 c(x，y) : = 0 c(x，y):=0 ；如果 x ≠ y x \neq y x​=y ，则 c ( x ， y ) : = 1 c(x，y) : = 1 c(x，y):=1。<strong>注意， c c c 定义了 F \mathcal{F} F上的一个度量，特别满足三角不等式</strong>。现在考虑 X : = ( α ， β ， γ ) , Y : = ( α ， β ， β ， γ ) , Z : = ( α ， γ ， γ ) X := (α，β，γ), Y := (α，β，β，γ), Z := (α，γ，γ) X:=(α，β，γ),Y:=(α，β，β，γ),Z:=(α，γ，γ)。那么很容易检查出 D T W ( X , Y ) = 0 DTW(X, Y ) = 0 DTW(X,Y)=0， D T W ( X , Z ) = 1 DTW (X, Z) = 1 DTW(X,Z)=1， D T W ( Y , Z ) = 2 DTW(Y, Z) = 2 DTW(Y,Z)=2。因此， D T W ( Y , Z ) &gt; D T W ( X , Y ) + D T W ( X , Z ) DTW(Y, Z) &gt; DTW(X, Y ) + D T W (X, Z) DTW(Y,Z)&gt;DTW(X,Y)+DTW(X,Z)，即 DTW 距离不满足三角形不等式。最后，注意路径 p 1 = ( ( 1 , 1 ) , ( 2 , 2 ) , ( 3 , 2 ) , ( 4 , 3 ) ) p^1 = ((1, 1), (2, 2), (3, 2), (4, 3)) p1=((1,1),(2,2),(3,2),(4,3))， p 2 = ( ( 1 , 1 ) , ( 2 , 1 ) , ( 3 , 2 ) , ( 4 , 3 ) ) p^2 = ((1, 1), (2, 1), (3, 2), (4, 3)) p2=((1,1),(2,1),(3,2),(4,3))， p 3 = ( ( 1 , 1 ) , ( 2 , 2 ) , ( 3 , 3 ) , ( 4 , 3 ) ) p^3 = ((1, 1), (2, 2), (3, 3), (4, 3)) p3=((1,1),(2,2),(3,3),(4,3)) 是总代价 2 的 Y 和 Z 之间的不同最优规整路径。<strong>这表明最优规整路径通常不是唯一的</strong>。</p><p><strong>为了确定最优路径 p ∗ p^* p∗，可以测试 X X X 和 Y Y Y 之间的每条可能的规整路径</strong>。然而，这样的过程会导致计算复杂度在长度 N N N 和 M M M 上呈指数增长。我们现在将介绍基于动态规划的 O ( N M ) O(NM) O(NM) 算法。为此，我们<strong>定义前缀序列</strong> X ( 1 : n ) : = ( x 1 , . . . x n ) X(1 :n) : = (x_1, . . . x_n) X(1:n):=(x1​,...xn​)， n ∈ [ 1 : N ] n ∈ [1 : N] n∈[1:N] 和 Y ( 1 : m ) : = ( y 1 , . . . y m ) Y (1:m) : = (y_1, . . . y_m) Y(1:m):=(y1​,...ym​)， m ∈ [ 1 : M ] m∈ [1 : M] m∈[1:M]，并设置<br /><img src="https://img-blog.csdnimg.cn/d11f03200d1545bb985374ed9fa16505.png" alt="在这里插入图片描述" /><br />值 D ( n , m ) D(n, m) D(n,m) 定义了一个 N × M N × M N×M 矩阵 D D D，也称为<strong>累积代价矩阵（accumulated cost matrix）</strong>。显然，有 D ( N , M ) = D T W ( X , Y ) D(N, M) = DTW (X, Y) D(N,M)=DTW(X,Y)。在下文中，<strong>表示代价矩阵 C C C 或累积代价矩阵 D D D 的矩阵元素的元组 ( n , m ) (n,m) (n,m) 将被称为单元</strong>。下一个定理展示了如何有效地计算 D D D。<br /><img src="https://img-blog.csdnimg.cn/a03fd4d0d91a4f628d5c31d541e0b373.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAZGF0YW1vbmRheQ==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" /><br />证明。设 m = 1 m = 1 m=1 和 n ∈ [ 1 : N ] n ∈ [1 : N] n∈[1:N]。那么在 Y ( 1 : 1 ) Y (1 : 1) Y(1:1) 和 X ( 1 : n ) X(1 : n) X(1:n) 之间只有一个可能的规整路径，总代价为 ∑ k = 1 n c ( x k , y 1 ) \sum^n_{k=1} c(x_k, y_1) ∑k=1n​c(xk​,y1​)。这证明了 D ( n , 1 ) D(n, 1) D(n,1) 的公式。类似地，可以得到 D ( 1 , m ) D(1, m) D(1,m) 的公式。现在，让 n &gt; 1 n &gt; 1 n&gt;1 和 m &gt; 1 m &gt; 1 m&gt;1 并让 q = ( q 1 , . . . . , q L − 1 , q L ) q = (q_1, ...., q_{L−1}, q_L) q=(q1​,....,qL−1​,qL​) 是 X ( 1 : n ) X(1:n) X(1:n) 和 Y ( 1 : m ) Y(1:m) Y(1:m) 的最佳规整路径。那么边界条件意味着 q L = ( n , m ) q_L = (n, m) qL​=(n,m)。设置 ( a , b ) : = q L − 1 (a, b) : = q_{L-1} (a,b):=qL−1​，步长条件意味着 ( a , b ) ∈ { ( n − 1 , m − 1 ) , ( n − 1 , m ) , ( n , m − 1 ) } (a, b) \in \{(n - 1, m - 1), (n - 1, m), (n, m - 1)\} (a,b)∈{(n−1,m−1),(n−1,m),(n,m−1)}。此外， ( q 1 , . . . , q L − 1 ) (q_1, . . . , q_{L−1}) (q1​,...,qL−1​) 必须是 X ( 1 : a ) X(1:a) X(1:a) 和 Y ( 1 : b ) Y(1:b) Y(1:b) 的最优规整路径。否则， q q q 对于 X ( 1 : n ) X(1 :n) X(1:n) 和 Y ( 1 : m ) Y(1:m) Y(1:m) 不是最优的。由于 D ( n , m ) = c ( q 1 , . . . , q L − 1 ) ( X ( 1 : a ) , Y ( 1 : b ) ) + c ( x n , y m ) D(n, m) = c(q_1,...,q_{L−1})(X(1 : a), Y (1 : b)) + c(x_n, y_m) D(n,m)=c(q1​,...,qL−1​)(X(1:a),Y(1:b))+c(xn​,ym​)， q q q 的最优性意味着断言（4.5）。</p><p>定理 4.3 有助于矩阵 D D D 的递归计算。初始化可以将矩阵 D D D 扩展为额外的行和列，并设置：对于 n ∈ [ 1 : N ] n \in [1:N] n∈[1:N]， D ( n , 0 ) : = ∞ D(n, 0) := \infin D(n,0):=∞；对于 m ∈ [ 1 : M ] m ∈ [1 : M] m∈[1:M]， D ( 0 , m ) : = ∞ D( 0, m) : = \infin D(0,m):=∞； D ( 0 , 0 ) : = 0 D(0, 0) := 0 D(0,0):=0。那么 (4.5) 的递归对于 n ∈ [ 1 : N ] n \in [1:N] n∈[1:N] 和 m ∈ [ 1 : M ] m ∈ [1 : M] m∈[1:M] 成立。此外，请注意 D D D 可以按列方式计算，其中第 m m m 列的计算只需要第 ( m − 1 ) (m - 1) (m−1) 列的值。这意味着，如果只对值 D T W ( X , Y ) = D ( N , M ) DTW(X, Y ) = D(N, M) DTW(X,Y)=D(N,M) 感兴趣，则存储需求为 O ( N ) O(N) O(N)。类似地，可以以逐行方式进行，导致 O ( M ) O(M) O(M)。但是无论哪种情况，运行时间都是 O ( N M ) O(NM) O(NM)。此外，为了计算最优的规整路径 p ∗ p^∗ p∗，需要整个 ( N × M ) (N × M) (N×M)-matrix D D D。作为练习，下面的 Optimal Warping Path 算法可以完成这项任务。<br /><img src="https://img-blog.csdnimg.cn/f9578171d04f4f43a650bd5926c2a551.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAZGF0YW1vbmRheQ==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" /><br />图 4.4 显示了图 4.2 中序列的最佳规整路径 p ∗ p^∗ p∗ (白线)。请注意， p ∗ p^∗ p∗ 仅涵盖 C 中表现出低代价的单元格 (参见图 4.4 a)。最终的累积代价矩阵 D D D 如图 4.4 b 所示。<br /><img src="https://img-blog.csdnimg.cn/2901c11e69ef42518ae8c81ccbae7dd9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAZGF0YW1vbmRheQ==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" /></p><h2 id="dtw-的变体"><a href="https://blog.csdn.net/weixin_39653948/article/details/124416873?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=dtw%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-124416873.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;spm=1018.2226.3001.4187"></a>DTW 的变体</h2><p>动态时间规整算法及其变体（本表基于 Time2Graph ）</p><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th>Method</th><th>Paper (Author)</th></tr></thead><tbody><tr class="odd"><td>DTW</td><td>Dynamic time warping. (Müller, 2007)</td></tr><tr class="even"><td>WDTW</td><td>Weighted dynamic time warping for time series classification. (Jeong, Jeong, and Omitaomu 2011)</td></tr><tr class="odd"><td>CID</td><td>Cid: an efficient complexity-invariant distance for time series. (Batista et al . 2014)</td></tr><tr class="even"><td>Derivative DTW (DDDTW)</td><td>Using derivatives in time series classification. (Górecki and Łuczak 2013)</td></tr></tbody></table><p>已经提出了各种修改来加速 DTW 计算以及更好地控制规整路径的可能路线。在本节中，我们将讨论其中的一些变化，并参考[170]了解更多细节。</p><p>更多细节可查阅参考资料 1。</p><ul><li>Step Size Condition</li><li>Local Weights</li><li>Global Constraints</li><li>Approximations</li></ul><h2 id="multiscale-dtw"><a href="https://blog.csdn.net/weixin_39653948/article/details/124416873?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=dtw%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-124416873.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;spm=1018.2226.3001.4187"></a>Multiscale DTW</h2><p>更多细节可查阅参考资料 1。</p><h2 id="subsequence-dtw"><a href="https://blog.csdn.net/weixin_39653948/article/details/124416873?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=dtw%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-124416873.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;spm=1018.2226.3001.4187"></a>Subsequence DTW</h2><p>更多细节可查阅参考资料 1。</p><h1 id="python-代码实现"><a href="https://blog.csdn.net/weixin_39653948/article/details/124416873?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=dtw%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-124416873.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;spm=1018.2226.3001.4187"></a>Python 代码实现</h1><p>该算法的实现看起来非常简洁：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def dtw(s, t, window):</span><br><span class="line">    n, m = len(s), len(t)</span><br><span class="line">    w = np.max([window, abs(n-m)])</span><br><span class="line">    dtw_matrix = np.zeros((n+1, m+1))</span><br><span class="line">    </span><br><span class="line">    for i in range(n+1):</span><br><span class="line">        for j in range(m+1):</span><br><span class="line">            dtw_matrix[i, j] = np.inf</span><br><span class="line">    dtw_matrix[0, 0] = 0</span><br><span class="line">    </span><br><span class="line">    for i in range(1, n+1):</span><br><span class="line">        for j in range(np.max([1, i-w]), np.min([m, i+w])+1):</span><br><span class="line">            dtw_matrix[i, j] = 0</span><br><span class="line">    </span><br><span class="line">    for i in range(1, n+1):</span><br><span class="line">        for j in range(np.max([1, i-w]), np.min([m, i+w])+1):</span><br><span class="line">            cost = abs(s[i-1] - t[j-1])</span><br><span class="line">            </span><br><span class="line">            # take last min from a square box</span><br><span class="line">            last_min = np.min([dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1]])</span><br><span class="line">            dtw_matrix[i, j] = cost + last_min</span><br><span class="line">            </span><br><span class="line">    return dtw_matrix</span><br><span class="line"></span><br><span class="line">a = [1, 2, 3, 3, 5]</span><br><span class="line">b = [1, 2, 2, 2, 2, 2, 2, 4]</span><br><span class="line"></span><br><span class="line">dtw(a, b, window=3)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0., inf, inf, inf, inf, inf, inf, inf, inf],</span><br><span class="line">       [inf,  0.,  1.,  2.,  3., inf, inf, inf, inf],</span><br><span class="line">       [inf,  1.,  0.,  0.,  0.,  0., inf, inf, inf],</span><br><span class="line">       [inf,  3.,  1.,  1.,  1.,  1.,  1., inf, inf],</span><br><span class="line">       [inf,  5.,  2.,  2.,  2.,  2.,  2.,  2., inf],</span><br><span class="line">       [inf, inf,  5.,  5.,  5.,  5.,  5.,  5.,  3.]])</span><br><span class="line"></span><br><span class="line"># A和B之间的距离是矩阵的最后一个元素，即3。</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>其中 <code>DTW[i, j]</code> 是 <code>s[1:i]</code> 和 <code>t[1:j]</code> 之间具有最佳对齐的距离。也就是说，长度为 <code>i</code> 和 <code>j</code> 的两个数组之间的代价等于尾部之间的距离 + 长度为 <code>i-1, j</code> <em>,</em> <code>i, j-1</code> 和 <code>i-1, j-1</code> 的数组中代价的最小值。为了防止一个数组中的一个元素匹配另一个数组中的无限个元素（只要尾部可以匹配到最后），可以添加一个窗口约束来限制一个可以匹配的元素数量。这样，每个元素都被限制为匹配 i - w 和 i + w 范围内的元素。 w := max(w, abs(n-m)) 保证所有索引都可以匹配。</p><p>调用第三方库实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from fastdtw import fastdtw</span><br><span class="line">from scipy.spatial.distance import euclidean</span><br><span class="line"></span><br><span class="line">x = np.array([1, 2, 3, 3, 7])</span><br><span class="line">y = np.array([1, 2, 2, 2, 2, 2, 2, 4])</span><br><span class="line"></span><br><span class="line">distance, path = fastdtw(x, y, dist=euclidean)</span><br><span class="line"></span><br><span class="line">print(distance)</span><br><span class="line">print(path)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">5.0</span><br><span class="line">[(0, 0), (1, 1), (1, 2), (1, 3), (1, 4), (2, 5), (3, 6), (4, 7)]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>它提供了两个数组的距离和索引映射（该示例可以扩展到多维数组）。</p><h1 id="参考资料"><a href="https://blog.csdn.net/weixin_39653948/article/details/124416873?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=dtw%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-124416873.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;spm=1018.2226.3001.4187"></a>参考资料</h1><blockquote><ul><li><a href="https://link.springer.com/chapter/10.1007/978-3-540-74048-3_4">Dynamic Time Warping (book, high cited in paper)</a></li><li><a href="https://databricks.com/blog/2019/04/30/understanding-dynamic-time-warping.html">Understanding Dynamic Time Warping (tutorial)</a></li><li><a href="https://towardsdatascience.com/dynamic-time-warping-3933f25fcdd">Dynamic Time Warping: Explanation and Code Implementation (tutorial)</a></li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 语音特征 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音特征 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>总结</title>
      <link href="/2023/01/14/cmath_zj/"/>
      <url>/2023/01/14/cmath_zj/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>前言</h1><p>这是当初考离散时整理的笔记，其中包含了自己对于一些知识点的体悟。纸质笔记只有一份，不易分享。因此把它拍照整理后放到这里。这样既能帮助到更多被离散折磨的学弟学妹们🥺，又能当作纸质笔记的一个备份。</p><p>本文主要是助离散期末考试只求低分飘过的小可爱们一臂之力~ 如若对自己的成绩有更高的要求，那么本文能做的最多也只是帮助你回顾一下基础知识而已。</p><p>一般来说，离散考试的难度主要是在于题目所涉及的知识点数量太过繁杂，并不在于知识点会考多深。所以考前最重要的是了解每个知识的概念和简单运用，这样基本就能够通过考试。不过为了防止某次考试老师把许多题目难度都设定得很高，在看完本文后若是还有一些的复习时间，可以去做一做更难的题目。</p><hr /><h1 id="一理逻辑"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>一、🌳理逻辑</h1><h2 id="命题逻辑"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>1. 命题逻辑</h2><p><img src="https://img-blog.csdnimg.cn/20210402102711628.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h2 id="等值演算"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>2. 等值演算</h2><p><img src="https://img-blog.csdnimg.cn/20210402102912324.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h2 id="主合取范式主析取范式"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>3. 主合取范式、主析取范式</h2><p><img src="https://img-blog.csdnimg.cn/20210402103222167.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /> <img src="https://img-blog.csdnimg.cn/20210402103317734.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h2 id="题推理证明"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>4. 【题】推理证明</h2><p><img src="https://img-blog.csdnimg.cn/20210402103426676.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h2 id="辖域自由变元约束变元"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>5. 辖域、自由变元、约束变元</h2><p><img src="https://img-blog.csdnimg.cn/20210402103455230.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h1 id="二合"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>二、🐔合</h1><h2 id="环和环积幂集笛卡尔乘积"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>1. 环和、环积，幂集，笛卡尔乘积</h2><p><img src="https://img-blog.csdnimg.cn/20210402103544435.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h1 id="三-关系"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>三、 🐒🐒关系</h1><h2 id="自反对称传递"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>1. 自反、对称、传递</h2><p><img src="https://img-blog.csdnimg.cn/20210402103629338.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h2 id="合成运算逆关系闭包"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>2&amp;3. 合成运算，逆关系、闭包</h2><p><img src="https://img-blog.csdnimg.cn/20210402103716392.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h2 id="哈斯图上下界偏序"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>4. 哈斯图、上下界、偏序</h2><p><img src="https://img-blog.csdnimg.cn/20210402103803288.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h2 id="等价关系划分"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>5. 等价关系、划分</h2><p><img src="https://img-blog.csdnimg.cn/20210402103854995.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h1 id="四"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>四、🦘</h1><h2 id="幺元么元单位元零元逆元半群群阿尔贝群独异点循环独异点"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>1. 幺元（么元、单位元）、零元、逆元，半群、群、阿尔贝群、独异点、循环独异点</h2><p><img src="https://img-blog.csdnimg.cn/20210402103921865.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /> <img src="https://img-blog.csdnimg.cn/20210402103951340.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h2 id="题由表判断"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>2. 【题】由表判断</h2><p><img src="https://img-blog.csdnimg.cn/20210402104107333.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h2 id="题证明"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>3. 【题】证明</h2><p><img src="https://img-blog.csdnimg.cn/20210402104149894.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h1 id="五论"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>五、🐰论</h1><h2 id="简单图平凡图多重图子图补图出度入度同构"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>1. 简单图、平凡图、多重图、子图、补图、出度、入度、同构</h2><p><img src="https://img-blog.csdnimg.cn/20210402104253172.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h2 id="路径回路连通度欧拉"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>2. 路径、回路、连通度、欧拉</h2><p><img src="https://img-blog.csdnimg.cn/20210402104320553.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p><hr /><h2 id="矩阵表示矩阵转置欧拉公式"><a href="https://blog.csdn.net/Sherlooock/article/details/115392090?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167368649416800192296180%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=167368649416800192296180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-115392090-null-null.142%5Ev71%5Ejs_top,201%5Ev4%5Eadd_ask&amp;utm_term=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E7%AC%94%E8%AE%B0&amp;spm=1018.2226.3001.4187"></a>3. 矩阵表示、矩阵转置、欧拉公式</h2><p><img src="https://img-blog.csdnimg.cn/2021040210434713.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NoZXJsb29vY2s=,size_16,color_FFFFFF,t_70#pic_center" /></p>]]></content>
      
      
      <categories>
          
          <category> 离散数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离散数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mysql教程</title>
      <link href="/2023/01/13/Mysql/"/>
      <url>/2023/01/13/Mysql/</url>
      
        <content type="html"><![CDATA[<h1 id="mysql学习笔记">MySQL学习笔记</h1><h2 id="一.-mysql简介">一. MySQL简介</h2><h3 id="mysql基础">1.1 MySQL基础</h3><p><strong>sql分类</strong>：</p><ol type="1"><li>DDL（Data Definition Language）数据定义语言，用来定义数据库对象：数据库，表，列等。create，drop，alter等。</li><li>DML(Data Manipulation Language)数据操作语言用来对数据库中表的数据进行增删改。insert, delete, update 等。</li><li>DQL(Data Query Language)数据查询语言用来查询数据库中表的记录(数据)。select, where 等。</li><li>DCL(Data Control Language)数据控制语言(了解)用来定义数据库的访问权限和安全级别，及创建用户。grant，revoke 等。</li></ol><p><strong>mysql基本操作：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">启动mysql服务:</span><br><span class="line">net <span class="keyword">start</span> mysql</span><br><span class="line">连接<span class="keyword">sql</span>服务</span><br><span class="line">mysql <span class="operator">-</span>h 地址 <span class="operator">-</span>p 端口 <span class="operator">-</span>u 用户名 <span class="operator">-</span>p 密码</span><br><span class="line">Enter password:<span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span></span><br><span class="line">退出：</span><br><span class="line">mysql<span class="operator">&gt;</span>exit</span><br><span class="line">Bye</span><br></pre></td></tr></table></figure><p><strong>mysql建表规范</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">/* 建表规范 */ ------------------</span><br><span class="line">    -- Normal Format, NF</span><br><span class="line">        - 每个表保存一个实体信息</span><br><span class="line">        - 每个具有一个ID字段作为主键</span><br><span class="line">        - ID主键 + 原子表</span><br><span class="line">    -- 1NF, 第一范式</span><br><span class="line">        字段不能再分，就满足第一范式。通俗理解即一个字段只存储一项信息。</span><br><span class="line">    -- 2NF, 第二范式</span><br><span class="line">        满足第一范式的前提下，不能出现部分依赖。</span><br><span class="line">        消除复合主键就可以避免部分依赖。增加单列关键字。</span><br><span class="line">        第二范式（2NF）要求数据库表中的每个实例或行必须可以被惟一地区分。为实现区分通常需要我们设计一个主键来实现(这里的主键不包含业务逻辑)。</span><br><span class="line">    -- 3NF, 第三范式</span><br><span class="line">        满足第二范式的前提下，不能出现传递依赖。</span><br><span class="line">        某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。</span><br><span class="line">        将一个实体信息的数据放在一个表内实现。</span><br><span class="line">        简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主键字段。就是说，表的信息，如果能够被推导出来，就不应该单独的设计一个字段来存放(能尽量外键join就用外键join)。很多时候，我们为了满足第三范式往往会把一张表分成多张表。</span><br></pre></td></tr></table></figure><h3 id="mysql架构演变">1.2 mysql架构演变</h3><p><strong>单机单库</strong></p><p>小型网站，只需一个mysql就能满足数据读取和写入需求。</p><p>瓶颈：</p><ul><li>数据量不能太大，超出一台服务器承受</li><li>读写操作量不能太大，超出一台服务器承受</li><li>可用性差</li></ul><p><strong>主从架构</strong></p><p>主要解决单机单库下的高可用和读扩展问题，主库宕机可以通过主从切换保障高可用。mysql也可以通过主从结构完成读写分离即主库用来写，从库用来读。</p><p>瓶颈：</p><ul><li>数据量不能过大，超出一台服务器承受</li><li>写操作不能太大，超出一台服务器承受</li></ul><p><strong>分库分表</strong></p><p>对于存储和写瓶颈，可以通过水平拆分来解决，水平和垂直拆分有较大区别，垂直拆分的结果是每一个实例都有全部数据，而水平拆分之后，任何实例都只有全量的1/n。</p><h3 id="mysql架构">1.3 mysql架构</h3><figure><img src="https://camo.githubusercontent.com/2731b6c6c4b5c52b896aea5ab86107b829d0c571523c4552abf3e48ab664883c/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f706963676f2f696d6167652d32303231313033303135323233393030372e706e67" alt="image-20211030152239007" /><figcaption>image-20211030152239007</figcaption></figure><p>MySQL Server架构自顶向下大致可以分网络连接层、服务层、存储引擎层和系统文件层。</p><ol type="1"><li><p>网络连接层</p><p>客户端连接器（Client Connectors）：提供与MySQL服务器建立的支持。目前几乎支持所有主流的服务端编程技术，例如常见的 Java、C、Python、.NET等，它们通过各自API技术与MySQL建立连接</p></li><li><p>服务层</p><p>服务层是MySQL Server的核心，主要包含系统管理和控制工具、连接池、SQL接口、解析器、查询优化器和缓存六个部分。</p><ol type="1"><li><p>连接池：负责存储和管理客户端与数据库的连接，一个线程负责管理一个连接</p></li><li><p>系统管理和控制工具：例如备份恢复、安全管理、集群管理</p></li><li><p>SQL接口：用于接收客户端发送的各种SQL命令，并且返回用户需要查询的结果。比如DML、DDL、存储过程、视图、触发器等</p></li><li><p>解析器：负责将请求的SQL解析生成一个&quot;解析树&quot;。然后根据一些MySQL规则进一步检查解析树是否合法。</p></li><li><p>查询优化器:</p><p>当“解析树”通过解析器语法检查后，将交由优化器将其转化成执行计划，然后与存储引擎交互</p></li><li><p>缓存:</p><p>缓存机制是由一系列小缓存组成的。比如表缓存，记录缓存，权限缓存，引擎缓存等。如果查询缓存有命中的查询结果，查询语句就可以直接去查询缓存中取数据</p></li></ol></li><li><p>存储引擎层</p><p>存储引擎负责MySQL中数据的存储与提取，与底层系统文件进行交互。MySQL存储引擎是插件式的，服务器中的查询执行引擎通过接口与存储引擎进行通信，接口屏蔽了不同存储引擎之间的差异 。现在有很多种存储引擎，各有各的特点，最常见的是MyISAM和InnoDB</p></li><li><p>系统文件层</p><p>该层负责将数据库的数据和日志存储在文件系统之上，并完成与存储引擎的交互，是文件的物理存储层。主要包含日志文件，数据文件，配置文件，pid 文件，socket 文件等</p></li></ol><h3 id="mysql运行机制">1.4 mysql运行机制</h3><figure><img src="https://camo.githubusercontent.com/667ffd7311a02d3ce64fa6761d0999368b6ccdb97ac8b8168f91da4aad2d22c8/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f706963676f2f6d7973716c2545362542352538312545372541382538422e706e67" alt="mysql流程" /><figcaption>mysql流程</figcaption></figure><p>建立连接（Connectors&amp;Connection Pool），通过客户端/服务器通信协议与MySQL建立连接。MySQL 客户端与服务端的通信方式是 “ 半双工 ”。对于每一个 MySQL 的连接，时刻都有一个线程状态来标识这个连接正在做什么</p><p>查询缓存（Cache&amp;Buffer）,这是Mysql一个可优化查询的地方，开启了查询缓存且在查询缓存过程中查询到完全相同的sql语句，则直接将查询结果返回给客户端，如果没有开启或没有查询到完全相同的sql语句则由解析器进行语法解析，并生成解析树。</p><p>解析器，将客户端发送的SQL进行语法解析，生成&quot;解析树&quot;。预处理器根据一些MySQL规则进一步检查“解析树”是否合法，例如这里将检查数据表和数据列是否存在，还会解析名字和别名，看看它们是否有歧义，最后生成新的“解析树</p><p>查询优化器：根据“解析树”生成最优的执行计划。MySQL使用很多优化策略生成最优的执行计划，可以分为两类：静态优化（编译时优化）、动态优化（运行时优化）</p><ul><li>等价变化策略<ul><li><code>5=5 and a&gt;5</code>改成<code>a&gt;5</code></li><li><code>a&lt;b and a=5</code>改成<code>b&gt;5 and a=5</code></li><li>基于联合索引调整条件位置</li></ul></li><li>优化count、min、max函数</li><li>提前终止查询<ul><li>使用limit，则获取所需的数据，不在遍历后面的数据</li></ul></li></ul><p>查询执行引擎，负责执行 SQL 语句，此时查询执行引擎会根据 SQL 语句中表的存储引擎类型，以及对应的API接口与底层存储引擎缓存或者物理文件的交互，得到查询结果并返回给客户端。若开启用查询缓存，这时会将SQL 语句和结果完整地保存到查询缓存（Cache&amp;Buffffer）中，以后若有相同的 SQL 语句执行则直接返回结果。</p><h2 id="二.-mysql-ddl">二. MySQL-DDL</h2><h3 id="数据库"><strong>数据库：</strong></h3><p>查看所有数据库</p><blockquote><p>SHOW DATABASES;</p></blockquote><p>选择数据库</p><blockquote><p>USE xxx（数据库名）</p></blockquote><p><img src="https://camo.githubusercontent.com/089a4728666775c18478358ee48d55a16cdfe19eaa4fd8fbd125f4277a2a1e9e/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f6a6176615f646174612f6d7973716c64646c312e6a7067" /></p><p>查看当前数据库：</p><blockquote><p>SELECT DATABASE();</p></blockquote><p>查看当前时间、用户名、数据库版本</p><blockquote><p>SELECT now(); SELECT user(); SELECT version();</p></blockquote><p>创建数据库</p><blockquote><p>create database 数据库名;</p><p>create database if not exists 数据库名称;</p><p>create database 数据库名 character set 字符集名;</p></blockquote><p>修改数据库</p><blockquote><p>修改数据库编码</p><p>alter database 数据库名称 character set 字符集;</p></blockquote><p>删除数据库</p><blockquote><p>drop database 数据库名称;</p><p>drop database if exists 数据库名称;</p></blockquote><h3 id="表"><strong>表：</strong></h3><p>创建表</p><blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> [TEMPORARY] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> EXITS] [库名.]表名 （表的结构定义）[表选项]</span><br><span class="line"></span><br><span class="line">每个字段必须有数据类型</span><br><span class="line">最后一个字段不能有逗号</span><br><span class="line">TEMPORARY 临时表，会话结束时表自动消失</span><br><span class="line">对字段的定义：</span><br><span class="line">字段名 数据类型 [<span class="keyword">NOT</span> <span class="keyword">NULL</span> <span class="operator">|</span> <span class="keyword">NULL</span>] [<span class="keyword">DEFAULT</span> default_value] [AUTO_INCREMENT] [<span class="keyword">UNIQUE</span> [KEY] <span class="operator">|</span> [<span class="keyword">PRIMARY</span>] KEY] [COMMENT <span class="string">&#x27;备注&#x27;</span>]</span><br></pre></td></tr></table></figure><p>表选项有：</p><ul><li>ENGINE= engine_name (引擎名，常用引擎InnoDB、MyISAM)</li><li>CHARSET = charset_name(字符集，未设置使用数据库字符集)</li><li>AUTO_INCREMENT = 行数</li><li>COMMENT = ‘表注释’</li><li>DATA DIRECTORY ='数据文件目录'</li><li>INDEX DIRECTORY ='索引文件目录'</li></ul><p>例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `tb_ad` (</span><br><span class="line">`id` <span class="type">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> AUTO_INCREMENT COMMENT <span class="string">&#x27;ID&#x27;</span>,</span><br><span class="line">`name` <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;广告名称&#x27;</span>,</span><br><span class="line">`position` <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;广告位置&#x27;</span>,</span><br><span class="line">`start_time` datetime <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;开始时间&#x27;</span>,</span><br><span class="line">`end_time` datetime <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;到期时间&#x27;</span>,</span><br><span class="line">`status` <span class="type">char</span>(<span class="number">1</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;状态&#x27;</span>,</span><br><span class="line">`image` <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;图片地址&#x27;</span>,</span><br><span class="line">`url` <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;URL&#x27;</span>,</span><br><span class="line">`remarks` <span class="type">varchar</span>(<span class="number">1000</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;备注&#x27;</span>,</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (`id`)</span><br><span class="line">) ENGINE<span class="operator">=</span>InnoDB AUTO_INCREMENT<span class="operator">=</span><span class="number">4</span> <span class="keyword">DEFAULT</span> CHARSET<span class="operator">=</span>utf8</span><br></pre></td></tr></table></figure></blockquote><p>删除表</p><blockquote><p>drop table customer;</p></blockquote><p>修改表</p><blockquote><p>重命名：</p><p>RENAME TABLE 原表名 TO 新表名</p><p>添加单列：</p><p>alter table 表名 add 列名 字段类型 AFTER 字段名/FIRST;（在xx字段后加入或在第一个加入）</p><p>添加多列：</p><p>alter table 表名 add 列名1 字段类型1,add 列名2 字段类型2...</p><p>例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> customer <span class="keyword">add</span> name <span class="type">varchar</span>(<span class="number">20</span>);</span><br></pre></td></tr></table></figure></blockquote><p>查看当前数据库中的所有表</p><blockquote><p>SHOW TABLES;</p></blockquote><p>查看指定表的结构(建表语句)</p><blockquote><p>SHOW CREATE TABLE 指定表;</p></blockquote><p><img src="https://camo.githubusercontent.com/23dbe75af575aee08cd24a867c9e299eafc84cb9221ded3de4541eb4c74e1dcb/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f6a6176615f646174612f6d7973716c64646c322e6a7067" /></p><p>查看指定表的结构</p><blockquote><p>DESC 表名;</p></blockquote><p><img src="https://camo.githubusercontent.com/174cbaecd42179c703cf197a28e802615e75ce49de67e07323c180b4acbe9059/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f6a6176615f646174612f6d7973716c64646c332e6a7067" /></p><p>复制表结构</p><blockquote><p>CREATE TABLE 表名 LIKE 要复制的表名</p><p>复制表结构和数据</p><p>CREATE TABLE 表名 LIKE 要复制的表名 AS SELECT * FROM 要复制的表名</p></blockquote><p>修改列</p><blockquote><p>修改类型和列名</p><p>ALTER TABLE 表名 CHANGE 原列名 新列名 新类型;</p><p>只修改类型 不修改列名</p><p>ALTER TABLE 表名 MODIFY 原列名 新类型;</p></blockquote><p>修改表名称</p><blockquote><p>alter table 表名 rename 新表名;</p></blockquote><p>删除列</p><blockquote><p>alter table 表名 drop 列名;</p></blockquote><h2 id="三.-mysql-dml">三. MySQL-DML</h2><h3 id="增加数据">增加数据</h3><blockquote><p>insert into 表名（列名1，列名2，。。。列名n）values（值1，值2，。。。值n）</p><p>insert into 表名 set 字段名=值</p><p>列名和值要一一对应。如果表名后，不定义列名，则默认给所有列添加值insert into 表名 values(值1,值2,...值n)。除了数字类型，其他类型需要使用引号(单双都可以)引起来。</p><p>例子：INSERT INTO usertable (id,username,sex) VALUES (5,'da','男')</p><p>INSERT INTO usertable SET id=99,username='dadda'</p></blockquote><h3 id="删除数据">删除数据</h3><blockquote><p>delete from 表名 条件</p><p>若没有条件则全部删除</p><p>例子：</p><p>DELETE FROM usertable WHERE id=99</p></blockquote><h3 id="修改数据">修改数据</h3><blockquote><p>UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件]</p><p>例子：</p><p>UPDATE usertable SET username='daxiao',sex='女' WHERE id=1</p></blockquote><h2 id="四.-mysql-dql">四. MySQL-DQL</h2><h3 id="select">select</h3><p>SELECT [ALL|DISTINCT] select_expr FROM-&gt; WHERE -&gt;GROUP BY [合计函数] -&gt;HAVING -&gt;ORDER BY -&gt;LIMIT</p><ol type="1"><li>select_expr<ul><li>可以使用 * 表示所有字段：select * from tb;</li><li>可以使用表达式（计算公式、函数调用、字段也是表达式）：select stu, 25+35, now() from tb</li><li>可以为每列使用别名，可以使用或省略as：select stu+10 as add10 from tb</li></ul></li><li>form<ul><li>可以为表起别名：select * from tb as t,tb1 as b;</li><li>多个表横向叠加，数据形成笛卡尔积：select * from tb1,tb2</li></ul></li><li>where<ul><li>表达式由运算符和运算数组成。</li><li>运算数：字段、值、函数返回值</li><li>运算符：=等于,&lt;&gt;不等于,&lt;=&gt; 安全等于可用于null比较，&lt;=小于等于,&gt;=大于等于,&gt;大于，&lt;小于，（not）between and （不在）在...之间, （not） in （不）在集合中，like 模糊匹配,is (not) null (不)为空。</li></ul></li><li>group by<ul><li>group by 字段/别名 [排序方式]</li><li>asc升序，desc降序</li><li>以下合计函数需配合group by使用：<ul><li>count 返回数目:count(*)、count(字段)</li><li>sum：求和</li><li>max：求最大值</li><li>min：求最小值</li><li>avg：求平均值</li></ul></li><li>例子：<code>select name,COUNT(*) from tb1 group by name</code></li></ul></li><li>having<ul><li>与where功能用法相同，执行时机不同</li><li>where是在开始时检测数据，对原数据进行过滤；having对筛选出来的结果进行过滤</li><li>having必须是查询出来的；where必须是数据表存在的</li><li>where不可以使用合计函数，having主要就是使用合计函数时使用的。</li><li>例子：<code>select region,sum(population),sum()arer from tb group by region having sun(area)&gt;10000</code></li></ul></li><li>order by<ul><li>排序 asc 升序，desc降序</li><li>默认升序</li></ul></li><li>limit<ul><li>对结果进行数量限制，索引从0开始</li><li>用法：limit 起始位置，获取条数</li><li>或者limit 获取条数 则直接从0开始</li></ul></li><li>distinct 去除重复记录</li></ol><h3 id="union">union</h3><p>用于连接两个以上的 SELECT 语句的结果组合到一个结果集合中。</p><p>select ... union [all|distinct] select ...</p><p>默认distinct，建议对每一个select查询加上小括号</p><p><strong>每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。</strong></p><p>例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> country <span class="keyword">from</span> websites</span><br><span class="line"><span class="keyword">union</span> </span><br><span class="line"><span class="keyword">select</span> country <span class="keyword">from</span> apps</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> country;</span><br></pre></td></tr></table></figure><p>查询websites表和apps表中所有不同的country</p><h3 id="子查询">子查询</h3><h4 id="form形式">form形式</h4><p>把内层的查询结果供外层再次查询</p><p>form后面是一个表，必须给子查询取别名</p><p>form型需将结果生成临时表格，子查询返回一个表，<strong>表型子查询</strong></p><p>eg：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> (<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb <span class="keyword">where</span> id<span class="operator">&gt;</span><span class="number">0</span>) <span class="keyword">as</span> t <span class="keyword">where</span> id<span class="operator">&gt;</span><span class="number">1</span>;</span><br></pre></td></tr></table></figure><h4 id="where形式">where形式</h4><p>把内层查询结果当作外层查询的比较条件</p><p>返回的结果集是一个标量集合，一行一列，也就是一个标量值，<strong>标量子查询</strong></p><blockquote><p>从定义上讲，每个标量子查询也是一个行子查询和一个列子查询，反之则不是；每个行子查询和列子查询也是一个表子查询，反之也不是。</p></blockquote><p>不需要起别名</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb <span class="keyword">where</span> money <span class="operator">=</span>(<span class="keyword">select</span> <span class="built_in">max</span>(money) <span class="keyword">from</span> tb); </span><br></pre></td></tr></table></figure><p>子查询返回的结果是一列，<strong>列子查询</strong></p><p>使用in、not in、exists、not exists条件</p><p>eg：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb1 </span><br><span class="line"><span class="keyword">where</span> <span class="keyword">exists</span>(</span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb2 <span class="keyword">where</span> tb1.id <span class="operator">=</span> tb2.id</span><br><span class="line">);</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb1 </span><br><span class="line"><span class="keyword">where</span> tb1.id <span class="keyword">in</span> (</span><br><span class="line">    <span class="keyword">select</span> id <span class="keyword">from</span> tb2</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>查询条件是一个行,<strong>行子查询</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb1 </span><br><span class="line"><span class="keyword">where</span> (id,gender) <span class="keyword">in</span> (</span><br><span class="line">    <span class="keyword">select</span> id,gender <span class="keyword">from</span> tb2</span><br><span class="line">);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="连接查询">连接查询</h3><p>将多个表的字段进行连接，可以指定连接条件。三张图片来自菜鸟教程</p><h4 id="内连接-inner-join">内连接 inner join</h4><p>获取两个表中字段匹配关系的记录</p><p>默认，可省略inner，连接结果不能出现空行，on表示连接条件，与where类似，using也可用于连接条件</p><figure><img src="https://www.runoob.com/wp-content/uploads/2014/03/img_innerjoin.gif" alt="img" /><figcaption>img</figcaption></figure><p>eg：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb1 a <span class="keyword">inner</span> <span class="keyword">join</span> tb2 b <span class="keyword">on</span> a.id<span class="operator">=</span>b.id</span><br><span class="line">等价于</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb1,tb2 <span class="keyword">where</span> tb1.id <span class="operator">=</span>tb2.id</span><br><span class="line">改为<span class="keyword">using</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb1 <span class="keyword">as</span> a <span class="keyword">join</span> tb2 b <span class="keyword">using</span>(id)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>交叉连接，</p><h4 id="外连接">外连接</h4><p>如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充</p><figure><img src="https://www.runoob.com/wp-content/uploads/2014/03/img_leftjoin.gif" alt="img" /><figcaption>img</figcaption></figure><p>-- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充</p><figure><img src="https://www.runoob.com/wp-content/uploads/2014/03/img_rightjoin.gif" alt="img" /><figcaption>img</figcaption></figure><p>eg：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">左外连接</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb1 a <span class="keyword">left</span> <span class="keyword">join</span> tb2 b <span class="keyword">on</span> a.id <span class="operator">=</span>b.id</span><br><span class="line">右外连接</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb1 a <span class="keyword">left</span> <span class="keyword">join</span> tb2 b <span class="keyword">on</span> a.id <span class="operator">=</span>b.id</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="truncate">truncate</h3><p>用法：truncate [table] tb_name</p><p>清空表中数据</p><p>区别：</p><p>1 truncate 是删除表再创建，delete 是逐条删除 2 truncate 重置auto_increment的值。而delete不会 3 当被用于带分区的表时，truncate 会保留分区</p><h2 id="五mysql-对用户的操作">五、MySQL-对用户的操作</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">-- 刷新权限</span><br><span class="line">FLUSH PRIVILEGES;</span><br><span class="line">-- 增加用户</span><br><span class="line">CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串)</span><br><span class="line">必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。</span><br><span class="line">用户名，注意引号：如 &#x27;user_name&#x27;@&#x27;192.168.1.1&#x27;可以用%代替ip地址表示全部ip</span><br><span class="line">-- 重命名用户</span><br><span class="line">RENAME USER old_user TO new_user</span><br><span class="line">-- 分配权限/添加用户</span><br><span class="line">GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] &#x27;password&#x27;]</span><br><span class="line">     all privileges 表示所有权限</span><br><span class="line">     *.* 表示所有库的所有表</span><br><span class="line">     库名.表名 表示某库下面的某表</span><br><span class="line">    GRANT ALL PRIVILEGES ON `pms`.* TO &#x27;pms&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;pms0817&#x27;;</span><br><span class="line">-- 撤消权限</span><br><span class="line">REVOKE 权限列表 ON 表名 FROM 用户名</span><br><span class="line">REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名   -- 撤销所有权限</span><br><span class="line">-- 查看权限</span><br><span class="line">SHOW GRANTS FOR 用户名 </span><br><span class="line">-- 权限列表</span><br><span class="line">ALL [PRIVILEGES]    -- 设置除GRANT OPTION之外的所有简单权限</span><br><span class="line">ALTER   -- 允许使用ALTER TABLE</span><br><span class="line">ALTER ROUTINE   -- 更改或取消已存储的子程序</span><br><span class="line">CREATE  -- 允许使用CREATE TABLE</span><br><span class="line">CREATE ROUTINE  -- 创建已存储的子程序</span><br><span class="line">CREATE TEMPORARY TABLES     -- 允许使用CREATE TEMPORARY TABLE</span><br><span class="line">CREATE USER     -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。</span><br><span class="line">CREATE VIEW     -- 允许使用CREATE VIEW</span><br><span class="line">DELETE  -- 允许使用DELETE</span><br><span class="line">DROP    -- 允许使用DROP TABLE</span><br><span class="line">EXECUTE     -- 允许用户运行已存储的子程序</span><br><span class="line">FILE    -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILE</span><br><span class="line">INDEX   -- 允许使用CREATE INDEX和DROP INDEX</span><br><span class="line">INSERT  -- 允许使用INSERT</span><br><span class="line">LOCK TABLES     -- 允许对您拥有SELECT权限的表使用LOCK TABLES</span><br><span class="line">PROCESS     -- 允许使用SHOW FULL PROCESSLIST</span><br><span class="line">REFERENCES  -- 未被实施</span><br><span class="line">RELOAD  -- 允许使用FLUSH</span><br><span class="line">REPLICATION CLIENT  -- 允许用户询问从属服务器或主服务器的地址</span><br><span class="line">REPLICATION SLAVE   -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）</span><br><span class="line">SELECT  -- 允许使用SELECT</span><br><span class="line">SHOW DATABASES  -- 显示所有数据库</span><br><span class="line">SHOW VIEW   -- 允许使用SHOW CREATE VIEW</span><br><span class="line">SHUTDOWN    -- 允许使用mysqladmin shutdown</span><br><span class="line">SUPER   -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。</span><br><span class="line">UPDATE  -- 允许使用UPDATE</span><br><span class="line">USAGE   -- “无权限”的同义词</span><br><span class="line">GRANT OPTION    -- 允许授予权限</span><br></pre></td></tr></table></figure><h2 id="六索引">六、索引</h2><p>索引是数据库用来提高性能的最常用工具，是帮助mysql搞笑获取数据的数据结构。</p><h3 id="概述">概述</h3><p><strong>使用索引的原因</strong></p><ol type="1"><li>通过创建唯一性索引，可以保证数据库表中的每一行数据的唯一性。</li><li>可以大大加快数据的检索速度，这也是创建索引的最主要的原因。</li><li>帮助服务器避免排序和临时表</li><li>将随机IO变为顺序IO</li><li>可以加速表之间的连接，特别是在实现数据的参考完整性方面有意义。</li></ol><p><strong>为什么不对表中的每一个列创建一个索引</strong></p><ol type="1"><li>当对表中的数据进行增删改时，索引也需要维护，这样就降低了数据的维护速度。</li><li>索引需要占用物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。</li><li>创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。</li></ol><p><strong>使用索引的注意事项</strong></p><ol type="1"><li>在经常需要搜索的列上，可以加快搜索的速度；</li><li>在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。</li><li>在经常需要排序的列上创 建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间；</li><li>对于中到大型表索引都是非常有效的，但是特大型表的话维护开销会很大，不适合建索引</li><li>在经常用在连接的列上，这 些列主要是一些外键，可以加快连接的速度；</li><li>避免 where 子句中对字段施加函数，这会造成无法命中索引。</li></ol><h3 id="索引提高查询速度和降低增删改速度的原因">索引提高查询速度和降低增删改速度的原因</h3><blockquote><p>本小节<strong>索引提高查询速度和降低增删改速度的原因</strong>图片和内容来自于Java3y大佬，地址 https://juejin.im/post/5b55b842f265da0f9e589e79</p></blockquote><p>首先了解mysql的基本存储结构（记录都存在页里面）：</p><p><img src="https://camo.githubusercontent.com/7270d42ac72167632458994a3e3c79fdfa8419424b09d53a4b03096dcf2e9c1e/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f6d7973716c2f6d7973716c5f63756e6368752e6a7067" /></p><p><img src="https://camo.githubusercontent.com/b0a5d503b41d0e84d09643a88dd113bf5336de3572799f62bed40f6637de2e15/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f6d7973716c2f6d7973716c5f63756e636875322e6a7067" /></p><ul><li>各个数据也可以组成双向链表</li><li>每个数据也中的记录又可以组成单向链表<ul><li>每个数据页都会为存储在它里面的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后在遍历该槽对应分组中的记录即可快速找到指定的记录。</li><li>以其他列（非主键）作为搜索条件：只能从最小记录开始一次遍历单链表中的每条记录。</li></ul></li></ul><p>所以，在使用<code>select * from user where indexname ='xxx'</code>这样没有进行任何优化的sql语句，默认会这样做：</p><ol type="1"><li>定位到记录所在的页：需要遍历双向链表，找到所在的页</li><li>从所在的页内中查找相应的记录：由于不是根据主键查询，只能遍历所在页的单链表</li></ol><p>这样的时间复杂度为O(n)</p><p><strong>使用索引之后</strong></p><p><img src="https://camo.githubusercontent.com/594c100a5baf9a033f03cc8f644c983acd17fc8d05d4c932d10fed6a20ee40cd/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f6d7973716c2f6d7973716c5f73756f79696e312e6a7067" /></p><p><img src="https://camo.githubusercontent.com/22874e74dbc9b4a51fbefa0ad9f1672f03121d8c9fcb4addf4ff3647296ba9c4/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f6d7973716c2f6d7973716c5f73756f79696e322e6a7067" /></p><p>没有用索引是需要遍历双向链表来定位对应的页，现在通过&quot;目录&quot;就可以很快的定位到对应的页上。(二分查找，时间复杂度近似O(logn))</p><p>它的底层结构就是B+树。</p><p>但是如果一棵普通的树在<strong>极端</strong>的情况下，是能<strong>退化成链表</strong>的(树的优点就不复存在了)</p><p>B+树是平衡树的一种，是不会退化成链表的，树的高度都是相对比较低的(基本符合<strong>矮矮胖胖(均衡)的结构</strong>)【这样一来我们检索的时间复杂度就是O(logn)】！从上一节的图我们也可以看见，建立索引实际上就是建立一颗B+树。</p><ul><li>B+树是一颗平衡树，如果我们对这颗树增删改的话，那肯定会<strong>破坏它的原有结构</strong>。</li><li><strong>要维持平衡树，就必须做额外的工作</strong>。正因为这些额外的工作<strong>开销</strong>，导致索引会降低增删改的速度</li></ul><h3 id="mysql索引数据结构">mysql索引数据结构</h3><p><strong>哈希索引</strong></p><p>对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。哈希索引只有Memory引擎支持。</p><p><strong>BTree索引</strong></p><p>最常见的索引类型，大部分都支持B树索引。</p><p><strong>R-tree 索引</strong>（空间索引）：</p><p>空间索引是MyISAM引擎的一个特殊索引类型，主要用于地理空间数据类型，通常使用较少，不做特别介绍。</p><p><strong>Full-text</strong> （全文索引） ：</p><p>全文索引也是MyISAM的一个特殊索引类型，主要用于全文索引，InnoDB从Mysql5.6版本开始支持全文索引。</p><p><em>平常所说的索引，如果没有特别指明，都是指B+树（多路搜索树，并不一定是二叉的）结构组织的索引。其中聚集索引、复合索引、前缀索引、唯一索引默认都是使用 B+tree 索引，统称为 索引。</em></p><h3 id="索引分类">索引分类</h3><p><strong>普通索引</strong></p><p>这是一种最基本的索引类型，而且他没有唯一性之类的限制。普通索引可以通过以下方式创建：</p><p>创建索引：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> index indexName <span class="keyword">ON</span> tableName (columnName)</span><br></pre></td></tr></table></figure><p>修改表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tablename <span class="keyword">ADD</span> INDEX indexName(columnName) </span><br></pre></td></tr></table></figure><p>创建表时指定：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tablename(</span><br><span class="line">id <span class="type">int</span> <span class="keyword">not</span> <span class="keyword">null</span>,</span><br><span class="line">    name verchar(<span class="number">10</span>) <span class="keyword">not</span> <span class="keyword">null</span>,</span><br><span class="line">    index [indexname](name(length))</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p><strong>唯一性索引</strong></p><p>和普通索引基本相同，但唯一性索引列的所有值都只能出现一次，即必须唯一。唯一性索引可以通过以下方式创建：</p><p>创建索引：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create unique index indexName ON tableName (columnName)</span><br></pre></td></tr></table></figure><p>修改表：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table tablename ADD unique INDEX indexName(columnName) </span><br></pre></td></tr></table></figure><p>创建表时指定：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create table tablename(</span><br><span class="line">id int not null,</span><br><span class="line">    name verchar(10) not null,</span><br><span class="line">    unique [indexname](name(length))</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p><strong>主键</strong></p><p>主键是一种唯一性索引，但它必须指定为“PRIMARY KEY”。如果你曾经用过AUTO_INCREMENT类型的列，你可能已经熟悉主键之类的概念了。</p><p><strong>全文索引</strong></p><p>全文索引的索引类型为FULLTEXT。全文索引可以在VARCHAR或者TEXT类型的列上创建。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table tablename add fulltext indexname(columnname)</span><br></pre></td></tr></table></figure><h3 id="索引语法">索引语法</h3><p><strong>创建索引</strong></p><p>见上</p><p><strong>删除索引</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop index indexname on mytable</span><br><span class="line">alter table tablename drop index indexname</span><br></pre></td></tr></table></figure><p>删除主键时只需指定PRIMARY KEY，但在删除索引时，你必须知道索引名。</p><p><strong>显示索引信息</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show index from tablename;</span><br></pre></td></tr></table></figure><h3 id="索引原理">索引原理</h3><p>MySQL官方对索引定义：是存储引擎用于快速查找记录的一种数据结构。需要额外开辟空间和数据维护工作。</p><p>索引是物理数据页存储，在数据文件中（InnoDB，ibd文件）利用数据页存储。索引可以加快检索速度，但是同时也会降低增删改速度，索引维护需要代价。</p><p>索引设计的理论知识：二分查找、Hash和B+Tree</p><p><strong>B+Tree</strong>:</p><p>MySQL数据库索引采用的是B+Tree结构，在B-Tree结构上做了优化改造。</p><p>B-Tree结构：</p><ul><li>索引值和data数据分布在整颗树的结构中</li><li>每个节点可以存放多个索引值以及对应的data数据</li><li>树节点中的多个索引值从左到右升序排列</li></ul><figure><img src="https://camo.githubusercontent.com/8012405924b497f164b8fc2b840d67edc9c97b75c614ff825b4e9f4ae24596ea/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f706963676f2f425452454532303231313130312e706e67" alt="BTREE20211101" /><figcaption>BTREE20211101</figcaption></figure><p>B树的搜索：从根节点开始，对节点内的索引值序列采用二分法查找，如果命中就结束查找。没有命中会进入子节点重复查找过程，直到所对应的的节点指针为空，或已经是叶子节点了才结束。</p><p>B+Tree结构：</p><ul><li>非叶子节点不存储data数据，只存储索引值，这样便于存储更多的索引值</li><li>叶子节点包含了所有的索引值和data数据</li><li>叶子节点用指针连接，提高区间的访问性能</li></ul><p><img src="https://camo.githubusercontent.com/ad8ee1bdfc2b767803d2b8ec3e8b28e40181f8bab53c02893aa417ce0b798e0d/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f706963676f2f42504c55535452454532303231313130312e706e67" /></p><p>相比B树，B+树进行范围查找时，只需要查找定位两个节点的索引值，然后利用叶子节点的指针进行遍历即可。而B树需要遍历范围内所有的节点和数据，显然B+Tree效率高</p><p><strong>聚簇索引和辅助索引</strong></p><p>聚簇索引和非聚簇索引：B+Tree的叶子节点存放主键索引值和行记录就属于聚簇索引；如果索引值和行记录分开存放就属于非聚簇索引。</p><p>主键索引和辅助索引：B+Tree的叶子节点存放的是主键字段值就属于主键索引；如果存放的是非主键值就属于辅助索引（二级索引）。</p><p>在InnoDB引擎中，主键索引采用的就是聚簇索引结构存储。</p><ul><li><p>聚簇索引（聚集索引）</p><p>聚簇索引是一种数据存储方式，InnoDB的聚簇索引就是按照主键顺序构建 B+Tree结构。B+Tree的叶子节点就是行记录，行记录和主键值紧凑地存储在一起。 这也意味着 InnoDB 的主键索引就是数据表本身，它按主键顺序存放了整张表的数据，占用的空间就是整个表数据量的大小。通常说的<strong>主键索引</strong>就是聚集索引。</p><p>InnoDB的表要求必须要有聚簇索引：</p><blockquote><p>如果表定义了主键，则主键索引就是聚簇索引</p><p>如果表没有定义主键，则第一个非空unique列作为聚簇索引</p><p>否则InnoDB会从建一个隐藏的row-id作为聚簇索引</p></blockquote></li><li><p>辅助索引</p><p>InnoDB辅助索引，也叫作二级索引，是根据索引列构建 B+Tree结构。但在 B+Tree 的叶子节点中只存了索引列和主键的信息。二级索引占用的空间会比聚簇索引小很多， 通常创建辅助索引就是为了提升查询效率。一个表InnoDB只能创建一个聚簇索引，但可以创建多个辅助索引</p></li></ul><p><img src="https://camo.githubusercontent.com/ec3d14d666ce267eb125a922ca80abacd11e648a1cac4b1e59609f38d8608c55/68747470733a2f2f6d797069632d31323133382e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f626c6f672f706963676f2f696d6167652d32303231313130313139333131303430382e706e67" /></p><p>非聚簇索引</p><p>与InnoDB表存储不同，MyISAM数据表的索引文件和数据文件是分开的，被称为非聚簇索引结构。</p>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Maven教程</title>
      <link href="/2023/01/13/maven/"/>
      <url>/2023/01/13/maven/</url>
      
        <content type="html"><![CDATA[<h1 id="maven教程">maven教程</h1><h2 id="简介">1.简介</h2><p>Maven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。</p><p>Maven 也可被用于构建和管理各种项目，例如 C#，Ruby，Scala 和其他语言编写的项目。Maven 曾是 Jakarta 项目的子项目，现为由 Apache 软件基金会主持的独立 Apache 项目。</p><h4 id="maven-功能">Maven 功能</h4><p>Maven 能够帮助开发者完成以下工作：</p><ul><li>构建</li><li>文档生成</li><li>报告</li><li>依赖</li><li>SCMs</li><li>发布</li><li>分发</li><li>邮件列表</li></ul><h4 id="约定配置">约定配置</h4><p>Maven 提倡使用一个共同的标准目录结构，Maven 使用约定优于配置的原则，大家尽可能的遵守这样的目录结构。如下所示：</p><table><colgroup><col style="width: 36%" /><col style="width: 63%" /></colgroup><thead><tr class="header"><th style="text-align: left;">目录</th><th style="text-align: left;">目的</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">${basedir}</td><td style="text-align: left;">存放pom.xml和所有的子目录</td></tr><tr class="even"><td style="text-align: left;">${basedir}/src/main/java</td><td style="text-align: left;">项目的java源代码</td></tr><tr class="odd"><td style="text-align: left;">${basedir}/src/main/resources</td><td style="text-align: left;">项目的资源，比如说property文件，springmvc.xml</td></tr><tr class="even"><td style="text-align: left;">${basedir}/src/test/java</td><td style="text-align: left;">项目的测试类，比如说Junit代码</td></tr><tr class="odd"><td style="text-align: left;">${basedir}/src/test/resources</td><td style="text-align: left;">测试用的资源</td></tr><tr class="even"><td style="text-align: left;">${basedir}/src/main/webapp/WEB-INF</td><td style="text-align: left;">web应用文件目录，web项目的信息，比如存放web.xml、本地图片、jsp视图页面</td></tr><tr class="odd"><td style="text-align: left;">${basedir}/target</td><td style="text-align: left;">打包输出目录</td></tr><tr class="even"><td style="text-align: left;">${basedir}/target/classes</td><td style="text-align: left;">编译输出目录</td></tr><tr class="odd"><td style="text-align: left;">${basedir}/target/test-classes</td><td style="text-align: left;">测试编译输出目录</td></tr><tr class="even"><td style="text-align: left;">Test.java</td><td style="text-align: left;">Maven只会自动运行符合该命名规则的测试类</td></tr><tr class="odd"><td style="text-align: left;">~/.m2/repository</td><td style="text-align: left;">Maven默认的本地仓库目录位置</td></tr></tbody></table><h4 id="maven-环境配置">Maven 环境配置</h4><p>Maven 是一个基于 Java 的工具，所以要做的第一件事情就是安装 JDK。</p><h4 id="设置-maven-环境变量">设置 Maven 环境变量</h4><p>添加环境变量 MAVEN_HOME：</p><table><colgroup><col style="width: 10%" /><col style="width: 89%" /></colgroup><thead><tr class="header"><th style="text-align: left;">系统</th><th style="text-align: left;">配置</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Windows</td><td style="text-align: left;">右键 &quot;计算机&quot;，选择 &quot;属性&quot;，之后点击 &quot;高级系统设置&quot;，点击&quot;环境变量&quot;，来设置环境变量，有以下系统变量需要配置：新建系统变量 <strong>MAVEN_HOME</strong>，变量值：<strong>E:-maven-3.3.9</strong><img src="https://www.runoob.com/wp-content/uploads/2018/09/1536057115-1481-20151218175411912-170761788.png" alt="img" />编辑系统变量 <strong>Path</strong>，添加变量值：**;%MAVEN_HOME%*<img src="https://www.runoob.com/wp-content/uploads/2018/09/1536057115-7470-20151218175417006-1644078150.png" alt="img" /><strong>注意：</strong>注意多个值之间需要有分号隔开，然后点击确定。</td></tr></tbody></table><h2 id="pom">2.POM</h2><p>POM( Project Object Model，项目对象模型 ) 是 Maven 工程的基本工作单元，是一个XML文件，包含了项目的基本信息，用于描述项目如何构建，声明项目依赖，等等。</p><p>执行任务或目标时，Maven 会在当前目录中查找 POM。它读取 POM，获取所需的配置信息，然后执行目标。</p><p>POM 中可以指定以下配置：</p><ul><li>项目依赖</li><li>插件</li><li>执行目标</li><li>项目构建 profile</li><li>项目版本</li><li>项目开发者列表</li><li>相关邮件列表信息</li></ul><p>所有 POM 文件都需要 project 元素和三个必需字段：groupId，artifactId，version。</p><table><colgroup><col style="width: 16%" /><col style="width: 83%" /></colgroup><thead><tr class="header"><th style="text-align: left;">节点</th><th style="text-align: left;">描述</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">project</td><td style="text-align: left;">工程的根标签。</td></tr><tr class="even"><td style="text-align: left;">modelVersion</td><td style="text-align: left;">模型版本需要设置为 4.0。</td></tr><tr class="odd"><td style="text-align: left;">groupId</td><td style="text-align: left;">这是工程组的标识。它在一个组织或者项目中通常是唯一的。例如，一个银行组织 com.companyname.project-group 拥有所有的和银行相关的项目。</td></tr><tr class="even"><td style="text-align: left;">artifactId</td><td style="text-align: left;">这是工程的标识。它通常是工程的名称。例如，消费者银行。groupId 和 artifactId 一起定义了 artifact 在仓库中的位置。</td></tr><tr class="odd"><td style="text-align: left;">version</td><td style="text-align: left;">这是工程的版本号。在 artifact 的仓库中，它用来区分不同的版本。例如：<code>com.company.bank:consumer-banking:1.0 com.company.bank:consumer-banking:1.1</code></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span> = <span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">    <span class="attr">xmlns:xsi</span> = <span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">    <span class="attr">xsi:schemaLocation</span> = <span class="string">&quot;http://maven.apache.org/POM/4.0.0</span></span></span><br><span class="line"><span class="string"><span class="tag">    http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!-- 模型版本 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 公司或者组织的唯一标志，并且配置时生成的路径也是由此生成， 如com.companyname.project-group，maven会将该项目打成的jar包放本地路径：/com/companyname/project-group --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.companyname.project-group<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!-- 项目的唯一ID，一个groupId下面可能多个项目，就是靠artifactId来区分的 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>project<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!-- 版本号 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="构建生命周期及配置文件">3.构建生命周期及配置文件</h2><h4 id="构建生命周期">构建生命周期</h4><p>一个典型的 Maven 构建（build）生命周期是由以下几个阶段的序列组成的：</p><figure><img src="https://camo.githubusercontent.com/89f75dee8103ab14089f9c7c5f5c0204348baff74b4e5b9f7ac359ffa474e48b/68747470733a2f2f7777772e72756e6f6f622e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031382f30392f373634323235362d633936376232633166616562613963652e706e67" alt="img" /><figcaption>img</figcaption></figure><table><thead><tr class="header"><th style="text-align: left;">阶段</th><th style="text-align: left;">处理</th><th style="text-align: left;">描述</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">验证 validate</td><td style="text-align: left;">验证项目</td><td style="text-align: left;">验证项目是否正确且所有必须信息是可用的</td></tr><tr class="even"><td style="text-align: left;">编译 compile</td><td style="text-align: left;">执行编译</td><td style="text-align: left;">源代码编译在此阶段完成</td></tr><tr class="odd"><td style="text-align: left;">测试 Test</td><td style="text-align: left;">测试</td><td style="text-align: left;">使用适当的单元测试框架（例如JUnit）运行测试。</td></tr><tr class="even"><td style="text-align: left;">包装 package</td><td style="text-align: left;">打包</td><td style="text-align: left;">创建JAR/WAR包如在 pom.xml 中定义提及的包</td></tr><tr class="odd"><td style="text-align: left;">检查 verify</td><td style="text-align: left;">检查</td><td style="text-align: left;">对集成测试的结果进行检查，以保证质量达标</td></tr><tr class="even"><td style="text-align: left;">安装 install</td><td style="text-align: left;">安装</td><td style="text-align: left;">安装打包的项目到本地仓库，以供其他项目使用</td></tr><tr class="odd"><td style="text-align: left;">部署 deploy</td><td style="text-align: left;">部署</td><td style="text-align: left;">拷贝最终的工程包到远程仓库中，以共享给其他开发人员和工程</td></tr></tbody></table><p>为了完成 default 生命周期，这些阶段（包括其他未在上面罗列的生命周期阶段）将被按顺序地执行。</p><p>Maven 有以下三个标准的生命周期：</p><ul><li><strong>clean</strong>：项目清理的处理</li><li><strong>default(或 build)</strong>：项目部署的处理</li><li><strong>site</strong>：项目站点文档创建的处理</li></ul><h4 id="构建配置文件">构建配置文件</h4><p>构建配置文件是一系列的配置项的值，可以用来设置或者覆盖 Maven 构建默认值。</p><p>使用构建配置文件，你可以为不同的环境，比如说生产环境（Production）和开发（Development）环境，定制构建方式。</p><p>配置文件在 pom.xml 文件中使用 activeProfiles 或者 profiles 元素指定，并且可以通过各种方式触发。配置文件在构建时修改 POM，并且用来给参数设定不同的目标环境（比如说，开发（Development）、测试（Testing）和生产环境（Production）中数据库服务器的地址）。</p><hr /><h5 id="构建配置文件的类型">构建配置文件的类型</h5><p>构建配置文件大体上有三种类型:</p><table><thead><tr class="header"><th style="text-align: left;">类型</th><th style="text-align: left;">在哪定义</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">项目级（Per Project）</td><td style="text-align: left;">定义在项目的POM文件pom.xml中</td></tr><tr class="even"><td style="text-align: left;">用户级 （Per User）</td><td style="text-align: left;">定义在Maven的设置xml文件中 (%USER_HOME%/.m2/settings.xml)</td></tr><tr class="odd"><td style="text-align: left;">全局（Global）</td><td style="text-align: left;">定义在 Maven 全局的设置 xml 文件中 (%M2_HOME%/conf/settings.xml)</td></tr></tbody></table><h2 id="maven仓库">4.maven仓库</h2><p>在 Maven 的术语中，仓库是一个位置（place）。</p><p>Maven 仓库是项目中依赖的第三方库，这个库所在的位置叫做仓库。</p><p>在 Maven 中，任何一个依赖、插件或者项目构建的输出，都可以称之为构件。</p><p>Maven 仓库能帮助我们管理构件（主要是JAR），它就是放置所有JAR文件（WAR，ZIP，POM等等）的地方。</p><p>Maven 仓库有三种类型：</p><ul><li>本地（local）</li><li>中央（central）</li><li>远程（remote）</li></ul><h4 id="本地仓库">本地仓库</h4><p>Maven 的本地仓库，在安装 Maven 后并不会创建，它是在第一次执行 maven 命令的时候才被创建。</p><p>运行 Maven 的时候，Maven 所需要的任何构件都是直接从本地仓库获取的。如果本地仓库没有，它会首先尝试从远程仓库下载构件至本地仓库，然后再使用本地仓库的构件。</p><p>默认情况下，不管Linux还是 Windows，每个用户在自己的用户目录下都有一个路径名为 .m2/respository/ 的仓库目录。</p><p>Maven 本地仓库默认被创建在 %USER_HOME% 目录下。要修改默认位置，在 %M2_HOME%目录中的 Maven 的 settings.xml 文件中定义另一个路径。</p><p><settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"    xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0     http://maven.apache.org/xsd/settings-1.0.0.xsd"> <localRepository>C:/MyLocalRepository</localRepository> </settings></p><p>当你运行 Maven 命令，Maven 将下载依赖的文件到你指定的路径中。</p><h4 id="中央仓库">中央仓库</h4><p>Maven 中央仓库是由 Maven 社区提供的仓库，其中包含了大量常用的库。</p><p>中央仓库包含了绝大多数流行的开源Java构件，以及源码、作者信息、SCM、信息、许可证信息等。一般来说，简单的Java项目依赖的构件都可以在这里下载到。</p><p>中央仓库的关键概念：</p><ul><li>这个仓库由 Maven 社区管理。</li><li>不需要配置。</li><li>需要通过网络才能访问。</li></ul><p>要浏览中央仓库的内容使用这个仓库，开发人员可以搜索所有可以获取的代码库。</p><h4 id="远程仓库">远程仓库</h4><p>如果 Maven 在中央仓库中也找不到依赖的文件，它会停止构建过程并输出错误信息到控制台。为避免这种情况，Maven 提供了远程仓库的概念，它是开发人员自己定制仓库，包含了所需要的代码库或者其他工程中用到的 jar 文件。</p><h4 id="maven-依赖搜索顺序">Maven 依赖搜索顺序</h4><p>当我们执行 Maven 构建命令时，Maven 开始按照以下顺序查找依赖的库：</p><ul><li><strong>步骤 1</strong> － 在本地仓库中搜索，如果找不到，执行步骤 2，如果找到了则执行其他操作。</li><li><strong>步骤 2</strong> － 在中央仓库中搜索，如果找不到，并且有一个或多个远程仓库已经设置，则执行步骤 4，如果找到了则下载到本地仓库中以备将来引用。</li><li><strong>步骤 3</strong> － 如果远程仓库没有被设置，Maven 将简单的停滞处理并抛出错误（无法找到依赖的文件）。</li><li><strong>步骤 4</strong> － 在一个或多个远程仓库中搜索依赖的文件，如果找到则下载到本地仓库以备将来引用，否则 Maven 将停止处理并抛出错误（无法找到依赖的文件）。</li></ul><h4 id="maven-阿里云aliyun仓库">Maven 阿里云(Aliyun)仓库</h4><p>Maven 仓库默认在国外， 国内使用难免很慢，我们可以更换为阿里云的仓库。</p><p>第一步:修改 maven 根目录下的 conf 文件夹中的 setting.xml 文件，在 mirrors 节点上，添加内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirrors</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>alimaven<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span>        </span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirrors</span>&gt;</span></span><br></pre></td></tr></table></figure><figure><img src="https://www.runoob.com/wp-content/uploads/2018/09/455FA277-3216-4BA5-94D3-0F650C763D6F.png" alt="img" /><figcaption>img</figcaption></figure><p>第二步: pom.xml文件里添加：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span>  </span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>alimaven<span class="tag">&lt;/<span class="name">id</span>&gt;</span>  </span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span>  </span><br><span class="line">            <span class="tag">&lt;<span class="name">releases</span>&gt;</span>  </span><br><span class="line">                <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span>  </span><br><span class="line">            <span class="tag">&lt;/<span class="name">releases</span>&gt;</span>  </span><br><span class="line">            <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span>  </span><br><span class="line">                <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span>  </span><br><span class="line">            <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="maven插件">5.maven插件</h2><h2 id="常用maven命令">6.常用maven命令</h2><ol type="1"><li><p>创建Maven的普通java项目： mvn archetype:create -DgroupId=packageName -DartifactId=projectName</p></li><li><p>创建Maven的Web项目：<br />mvn archetype:create -DgroupId=packageName<br />-DartifactId=webappName -DarchetypeArtifactId=maven-archetype-webapp</p></li><li><p>编译源代码： mvn compile</p></li><li><p>编译测试代码：mvn test-compile</p></li><li><p>运行测试：mvn test</p></li><li><p>产生site：mvn site</p></li><li><p>打包：mvn package</p></li><li><p>在本地Repository中安装jar：mvn install</p></li><li><p>清除产生的项目：mvn clean</p></li><li><p>生成eclipse项目：mvn eclipse:eclipse</p></li><li><p>生成idea项目：mvn idea:idea</p></li><li><p>组合使用goal命令，如只打包不测试：mvn -Dtest package</p></li><li><p>编译测试的内容：mvn test-compile</p></li><li><p>只打jar包: mvn jar:jar</p></li><li><p>只测试而不编译，也不测试编译：mvn test -skipping compile -skipping test-compile ( -skipping 的灵活运用，当然也可以用于其他组合命令)</p></li><li><p>清除eclipse的一些系统设置:mvn eclipse:clean</p></li></ol><h2 id="maven依赖管理">7.maven依赖管理</h2><h4 id="依赖的传递">依赖的传递</h4><p>传递性依赖是Maven2.0的新特性。假设你的项目依赖于一个库，而这个库又依赖于其他库。你不必自己去找出所有这些依赖，你只需要加上你直接依赖的库，Maven会隐式的把这些库间接依赖的库也加入到你的项目中。这个特性是靠解析从远程仓库中获取的依赖库的项目文件实现的。一般的，这些项目的所有依赖都会加入到项目中，或者从父项目继承，或者通过传递性依赖。 传递性依赖的嵌套深度没有任何限制，只是在出现循环依赖时会报错。</p><h4 id="exclude排除jar包冲突">exclude排除jar包冲突</h4><p>eg:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.struts<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>struts2-spring-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.24<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--排除spring-beans --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springfreamwork<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-beans<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="第一声明者优先依赖调解原则">第一声明者优先（依赖调解原则）</h4><p>在pom文件中定义，先声明的依赖为准</p><h4 id="路径近者优先依赖调解原则">路径近者优先（依赖调解原则）</h4><p>在本工程中的pom加入spring-beans-4.2.4的依赖，根据路径近者优先原则，系统将导入spring-beans-4.2.4</p><h4 id="版本锁定">版本锁定</h4><p>eg:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springfreamwork<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-beans<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">vension</span>&gt;</span>4.2.4.RELEASE<span class="tag">&lt;/<span class="name">vension</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencyManagement</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在工程中锁定依赖的版本并不代表在工程中添加了依赖，如果工程需要添加锁定版本的依赖则需单独添加<dependencies>标签</p><p>如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springfreamwork<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-beans<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>此处的版本并没有添加依赖，原因是已在<dependencyManagement>中锁定了版本，所以在此处不需要再指定版本。</p>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后端 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基音周期估计-pYin</title>
      <link href="/2023/01/13/pyin/"/>
      <url>/2023/01/13/pyin/</url>
      
        <content type="html"><![CDATA[<h1 id="总述">总述</h1><p>pYin的主要步骤如下：</p><p><img src="" /></p><ul><li>计算ACF（自相关函数） <span class="math display">\[d_t(\tau)=\sum_{j=1}^W\left(x_j-x_{j+\tau}  \right)^2\]</span> 根据自相关函数的定义，可以将上式改写成 <span class="math display">\[\begin{aligned}&amp; d_t(\tau)=r_t(0)+r_{t+\tau}(0)-2 r_t(\tau) \\&amp; r_t(\tau)=\sum_{j=t+2}^{t+W} x_j x_{j+\tau},   \\&amp;\end{aligned}\]</span></li><li>计算累计均值归一化插值函数 <span class="math display">\[d_t^{\prime}(\tau)=\left\{\begin{array}{l}1, \quad \text { if } \tau=0, \\d_t(\tau) /\left[(1 / \tau) \sum_{j=1}^\tau d_t  (j)\right] \quad \text { otherwise. }\end{array}\right.\]</span></li></ul><p>至此与Yin算法一样，而不一样的地方在于阈值门限的处理方式：Yin固定了阈值门限而pYin中的门限是服从Beta分布的。好处在于：Yin预设的门限可能会小于所有的谷值，此时仍然会出现半频误差。而pYin的阈值取值取决于它所服从的概率分布，一定程度上能够避免门限值都小于谷值。 * 计算基频点及其对应的频率 阈值服从的概率分布如下所示，下图展示的三个不同均值的beta分布。</p><h2 id="代码实现">代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> librosa</span><br><span class="line"><span class="keyword">import</span> soundfile <span class="keyword">as</span> sf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy.lib.stride_tricks <span class="keyword">import</span> as_strided</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">import</span> librosa.display</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">speech_frame</span>(<span class="params">x, frame_length, hop_length, axis=-<span class="number">1</span></span>):</span><br><span class="line">    x = np.ascontiguousarray(x)</span><br><span class="line">    n_frames = <span class="number">1</span> + (x.shape[axis] - frame_length) // hop_length</span><br><span class="line">    strides = np.asarray(x.strides)</span><br><span class="line">    new_stride = np.prod(strides[strides &gt; <span class="number">0</span>] // x.itemsize) * x.itemsize</span><br><span class="line">    <span class="keyword">if</span> axis == -<span class="number">1</span>:</span><br><span class="line">        shape = <span class="built_in">list</span>(x.shape)[:-<span class="number">1</span>] + [frame_length, n_frames]</span><br><span class="line">        strides = <span class="built_in">list</span>(strides) + [hop_length * new_stride]</span><br><span class="line">    <span class="keyword">elif</span> axis == <span class="number">0</span>:</span><br><span class="line">        shape = [n_frames, frame_length] + <span class="built_in">list</span>(x.shape)[<span class="number">1</span>:]</span><br><span class="line">        strides = [hop_length * new_stride] + <span class="built_in">list</span>(strides)</span><br><span class="line">    <span class="keyword">return</span> as_strided(x, shape=shape, strides=strides)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumulative_mean_normalized_difference</span>(<span class="params">y_frames, frame_length, win_length, min_period, max_period</span>):</span><br><span class="line">    a = np.fft.rfft(y_frames, frame_length, axis=<span class="number">0</span>)</span><br><span class="line">    b = np.fft.rfft(y_frames[win_length:: -<span class="number">1</span>, :], frame_length, axis=<span class="number">0</span>)</span><br><span class="line">    acf_frames = np.fft.irfft(a * b, frame_length, axis=<span class="number">0</span>)[win_length:]</span><br><span class="line">    acf_frames[np.<span class="built_in">abs</span>(acf_frames) &lt; <span class="number">1e-6</span>] = <span class="number">0</span></span><br><span class="line">    energy_frames = np.cumsum(y_frames ** <span class="number">2</span>, axis=<span class="number">0</span>)</span><br><span class="line">    energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]</span><br><span class="line">    energy_frames[np.<span class="built_in">abs</span>(energy_frames) &lt; <span class="number">1e-6</span>] = <span class="number">0</span></span><br><span class="line">    y_yin_frames = energy_frames[<span class="number">0</span>, :] + energy_frames -  <span class="number">2</span> * acf_frames</span><br><span class="line">    yin_numerator = y_yin_frames[min_period: max_period + <span class="number">1</span>, :]</span><br><span class="line">    tau_range = np.arange(<span class="number">1</span>, max_period + <span class="number">1</span>)[:, <span class="literal">None</span>]</span><br><span class="line">    cumulative_mean = np.cumsum(y_yin_frames[<span class="number">1</span> : max_period + <span class="number">1</span>, :], axis=<span class="number">0</span>) / tau_range</span><br><span class="line">    yin_denominator = cumulative_mean[min_period - <span class="number">1</span>: max_period, :]</span><br><span class="line">    yin_frames  = yin_numerator / (yin_denominator + tiny(yin_denominator))</span><br><span class="line">    <span class="keyword">return</span> yin_frames</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parabolic_interpolation</span>(<span class="params">y_frames</span>):</span><br><span class="line">    parabolic_shifts = np.zeros_like(y_frames)</span><br><span class="line">    parabola_a = (y_frames[:-<span class="number">2</span>, :] + y_frames[<span class="number">2</span>:, :] - <span class="number">2</span> * y_frames[<span class="number">1</span>:-<span class="number">1</span>, :]) / <span class="number">2</span></span><br><span class="line">    parabola_b = (y_frames[<span class="number">2</span>:, :] - y_frames[:-<span class="number">2</span>, :]) / <span class="number">2</span></span><br><span class="line">    parabolic_shifts[<span class="number">1</span>:-<span class="number">1</span>, :] = -parabola_b / (<span class="number">2</span> * parabola_a + tiny(parabola_a))</span><br><span class="line">    parabolic_shifts[np.<span class="built_in">abs</span>(parabolic_shifts) &gt; <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> parabolic_shifts</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tiny</span>(<span class="params">x</span>):</span><br><span class="line">    x = np.asarray(x)</span><br><span class="line">    <span class="keyword">if</span> np.issubdtype(x.dtype, np.floating) <span class="keyword">or</span> np.issubdtype(</span><br><span class="line">        x.dtype, np.complexfloating</span><br><span class="line">    ):</span><br><span class="line">        dtype = x.dtype</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dtype = np.float32</span><br><span class="line">    <span class="keyword">return</span> np.finfo(dtype).tiny</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">localmin</span>(<span class="params">x,axis = <span class="number">0</span></span>):</span><br><span class="line">    paddings = [(<span class="number">0</span>, <span class="number">0</span>)] * x.ndim</span><br><span class="line">    paddings[axis] = (<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    x_pad = np.pad(x, paddings, mode=<span class="string">&quot;edge&quot;</span>)</span><br><span class="line">    inds1 = [<span class="built_in">slice</span>(<span class="literal">None</span>)] * x.ndim</span><br><span class="line">    inds1[axis] = <span class="built_in">slice</span>(<span class="number">0</span>, -<span class="number">2</span>)</span><br><span class="line">    inds2 = [<span class="built_in">slice</span>(<span class="literal">None</span>)] * x.ndim</span><br><span class="line">    inds2[axis] = <span class="built_in">slice</span>(<span class="number">2</span>, x_pad.shape[axis])</span><br><span class="line">    <span class="keyword">return</span> (x &lt; x_pad[<span class="built_in">tuple</span>(inds1)]) &amp; (x &lt;= x_pad[<span class="built_in">tuple</span>(inds2)])</span><br><span class="line">y, sr = librosa.load(librosa.ex(<span class="string">&#x27;trumpet&#x27;</span>))</span><br><span class="line">frame_length = <span class="number">2048</span></span><br><span class="line">win_length = frame_length // <span class="number">2</span></span><br><span class="line">hop_length = frame_length // <span class="number">4</span></span><br><span class="line">y_frame = speech_frame(y, frame_length, hop_length)</span><br><span class="line">fmax = librosa.note_to_hz(<span class="string">&#x27;C7&#x27;</span>)</span><br><span class="line">fmin = librosa.note_to_hz(<span class="string">&#x27;C2&#x27;</span>)</span><br><span class="line">min_period = <span class="built_in">max</span>(<span class="built_in">int</span>(np.floor(sr / fmax)), <span class="number">1</span>)</span><br><span class="line">max_period = <span class="built_in">min</span>(<span class="built_in">int</span>(np.ceil(sr / fmin)), frame_length - win_length + <span class="number">1</span>)</span><br><span class="line">yin_frames = cumulative_mean_normalized_difference(y_frame, frame_length, win_length, min_period, max_period)</span><br><span class="line">parabolic_shifts = parabolic_interpolation(yin_frames)</span><br><span class="line"><span class="comment"># pYin algorithms</span></span><br><span class="line"><span class="comment"># Find Yin candidates and probabilities</span></span><br><span class="line"><span class="comment"># Step 1: 定义阈值的先验分布</span></span><br><span class="line">n_threshold = <span class="number">100</span></span><br><span class="line">beta_parameters=(<span class="number">2</span>, <span class="number">18</span>)</span><br><span class="line">boltzmann_parameter=<span class="number">2</span></span><br><span class="line">no_trough_prob=<span class="number">0.01</span></span><br><span class="line">resolution = <span class="number">0.1</span></span><br><span class="line">switch_prob = <span class="number">0.01</span></span><br><span class="line">fill_na=np.nan</span><br><span class="line">max_transition_rate=<span class="number">35.92</span></span><br><span class="line">threshold = np.linspace(<span class="number">0</span>, <span class="number">1</span>, n_threshold+<span class="number">1</span>)</span><br><span class="line">beta_cdf = scipy.stats.beta.cdf(threshold, beta_parameters[<span class="number">0</span>], beta_parameters[<span class="number">1</span>])</span><br><span class="line">beta_probs = np.diff(beta_cdf)</span><br><span class="line">yin_probs = np.zeros_like(yin_frames)</span><br><span class="line"><span class="keyword">for</span> i, yin_frame <span class="keyword">in</span> <span class="built_in">enumerate</span>(yin_frames.T):</span><br><span class="line">    <span class="comment"># Step 2: 找到帧的谷值</span></span><br><span class="line">    is_trough = localmin(yin_frame, axis=<span class="number">0</span>)</span><br><span class="line">    is_trough[<span class="number">0</span>] = yin_frame[<span class="number">0</span>] &lt; yin_frame[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># np.nonzeros(): 得到数组中非零元素的位置。即峰谷的位置</span></span><br><span class="line">    (trough_index, ) = np.nonzero(is_trough)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(trough_index) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># Step 3: 找到低于门限值的峰谷</span></span><br><span class="line">    trough_heights = yin_frame[trough_index] </span><br><span class="line">    trough_thresholds = trough_heights[:, <span class="literal">None</span>] &lt; threshold[<span class="literal">None</span>, <span class="number">1</span> : ]</span><br><span class="line">    <span class="comment"># Step 4: </span></span><br><span class="line">    trough_positions = np.cumsum(trough_thresholds, axis=<span class="number">0</span>) - <span class="number">1</span></span><br><span class="line">    n_troughs = np.count_nonzero(trough_thresholds, axis=<span class="number">0</span>)</span><br><span class="line">    trough_prior = scipy.stats.boltzmann.pmf(</span><br><span class="line">        trough_positions, boltzmann_parameter, n_troughs</span><br><span class="line">    )</span><br><span class="line">    trough_prior[~trough_thresholds] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    probs = np.<span class="built_in">sum</span>(trough_prior * beta_probs, axis=<span class="number">1</span>)</span><br><span class="line">    global_min = np.argmin(trough_heights)</span><br><span class="line">    n_threshold_below_min = np.count_nonzero(~trough_thresholds[global_min, :])</span><br><span class="line">    probs[global_min] +=  no_trough_prob * np.<span class="built_in">sum</span>(beta_probs[:n_threshold_below_min])</span><br><span class="line">    yin_probs[trough_index, i] = probs</span><br><span class="line">yin_period, frame_index = np.nonzero(yin_probs)</span><br><span class="line">period_candidates = min_period + yin_period</span><br><span class="line">period_candidates = period_candidates + parabolic_shifts[yin_period, frame_index]</span><br><span class="line">fo_candidates = sr / period_candidates</span><br><span class="line">n_bins_per_semitone = <span class="built_in">int</span>(np.ceil(<span class="number">1.0</span> / resolution))</span><br><span class="line">n_pitch_bins = <span class="built_in">int</span>( np.floor(<span class="number">12</span> * n_bins_per_semitone * np.log2(fmax/fmin))) + <span class="number">1</span></span><br><span class="line"><span class="comment"># Construct transition matirx</span></span><br><span class="line">max_semitones_per_frame = <span class="built_in">round</span>(max_transition_rate * <span class="number">12</span> * hop_length / sr)</span><br><span class="line">transition_width = max_semitones_per_frame * n_bins_per_semitone + <span class="number">1</span></span><br><span class="line">transition = librosa.sequence.transition_local(n_pitch_bins, transition_width,</span><br><span class="line">window=<span class="string">&quot;triangle&quot;</span>, wrap=<span class="literal">False</span>)</span><br><span class="line">transition = np.block([</span><br><span class="line">    [(<span class="number">1</span> - switch_prob) * transition, switch_prob * transition],</span><br><span class="line">    [switch_prob * transition, (<span class="number">1</span> - switch_prob) * transition],</span><br><span class="line">])</span><br><span class="line">bin_index = <span class="number">12</span> * n_bins_per_semitone * np.log2(fo_candidates / fmin)</span><br><span class="line">bin_index = np.clip(np.<span class="built_in">round</span>(bin_index), <span class="number">0</span>, n_pitch_bins).astype(<span class="built_in">int</span>)</span><br><span class="line"><span class="comment"># obseavation probabilities</span></span><br><span class="line">observation_probs = np.zeros((<span class="number">2</span> * n_pitch_bins, yin_frames.shape[<span class="number">1</span>]))</span><br><span class="line">observation_probs[bin_index, frame_index] = yin_probs[yin_period, frame_index]</span><br><span class="line">vocied_prob = np.clip(np.<span class="built_in">sum</span>(observation_probs[: n_pitch_bins, :], axis=<span class="number">0</span>), <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">observation_probs[n_pitch_bins:, :] = (<span class="number">1</span> - vocied_prob[<span class="literal">None</span>,:]) / n_pitch_bins</span><br><span class="line">p_init = np.zeros(<span class="number">2</span> * n_pitch_bins)</span><br><span class="line">p_init[n_pitch_bins:] = <span class="number">1</span> / n_pitch_bins</span><br><span class="line"><span class="comment"># Viterbi decoding</span></span><br><span class="line">states = librosa.sequence.viterbi(observation_probs, transition, p_init=p_init)</span><br><span class="line"><span class="comment"># Find fo corresponding to each decoded pitch bin</span></span><br><span class="line">freqs = fmin * <span class="number">2</span> ** (np.arange(n_pitch_bins) / (<span class="number">12</span> * n_bins_per_semitone))</span><br><span class="line">f0 = freqs[states % n_pitch_bins]</span><br><span class="line">voiced_flag = states &lt; n_pitch_bins</span><br><span class="line"><span class="keyword">if</span> fill_na <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    f0[~voiced_flag] = fill_na</span><br><span class="line"><span class="built_in">print</span>(f0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行结果为</span></span><br><span class="line"><span class="comment"># 622.25396744 625.85864801 625.85864801 625.85864801...</span></span><br></pre></td></tr></table></figure><p>参考librosa的一个画图示例，可以得到下图的基频曲线 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">times = librosa.times_like(f0)</span><br><span class="line">D = librosa.amplitude_to_db(np.<span class="built_in">abs</span>(librosa.stft(y)), ref=np.<span class="built_in">max</span>)</span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">img = librosa.display.specshow(D, x_axis=<span class="string">&#x27;time&#x27;</span>, y_axis=<span class="string">&#x27;log&#x27;</span>, ax=ax)</span><br><span class="line">ax.<span class="built_in">set</span>(title=<span class="string">&#x27;pYin F0&#x27;</span>)</span><br><span class="line">fig.colorbar(img, ax=ax, <span class="built_in">format</span>=<span class="string">&quot;%+2.f dB&quot;</span>)</span><br><span class="line">ax.plot(times, f0, label=<span class="string">&#x27;f0&#x27;</span>, color=<span class="string">&#x27;cyan&#x27;</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">ax.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 语音特征 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音特征 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown使用说明</title>
      <link href="/2023/01/13/useMarkdown/"/>
      <url>/2023/01/13/useMarkdown/</url>
      
        <content type="html"><![CDATA[<h1 id="什么是-markdown">什么是 Markdown?</h1><ol type="1"><li><strong>Markdown</strong> 是一款轻量级标记语言，不同于HTML <strong>(Hypertext Markup Language)</strong>，<strong>Markdown</strong> 的语法非常简单，且容易上手</li><li><strong>Markdown</strong> 以 <strong>纯文本格式</strong> 编写文档，依赖键盘而非鼠标，专注于<strong>写作本身</strong>，感受<strong>书写</strong>的魅力</li><li><strong>Markdown</strong> 的通过添加一些简单的 <strong>标识符</strong>，让文本具有<strong>恰到好处</strong>的格式</li><li><strong>Markdown</strong> 核心特征就是 <strong>删繁剪芜</strong>， <strong>简扼</strong> + <strong>精炼</strong></li><li><strong>Markdown</strong> 是 <strong>笔记</strong> 与 <strong>网页文章</strong> 的最佳载体</li><li><strong>Down</strong> 的核心：坐 <strong>下</strong> 来，就能把思维写 <strong>下</strong> 来<ul><li><strong>牛津高阶英汉双解词典第九版</strong> 中，关于 <strong>down</strong> 的释义：</li></ul></li></ol><p><br></p><figure><img src="https://z3.ax1x.com/2021/10/12/5mqsGF.png" title="牛津9 down释义" alt="牛津9 down释义" /><figcaption>牛津9 down释义</figcaption></figure><p><br></p><h2 id="为什么要使用-markdown">为什么要使用 Markdown?</h2><p>有朋友问我 ，<strong>Markdown</strong> 的效果 用Word 完全可以复现，甚至功能更多，那为何要用 <strong>Markdown</strong> 呢？</p><p><strong>答：</strong> - 功能多，不一定是好事 - 功能一多，选择就会变多，然后你会开始纠结…… - 这个字号是不是该大一点呢？ - 这个颜色好像有点不太搭呢？ - 这个粗体，是不是该再加点颜色呢？ - 这个图片的位置看起来有点不大对劲呢？ - 结果，写了半天，就憋出一点点东西 - 写出来的内容...好像...也不咋滴</p><p><strong>MD的优势：</strong></p><ol type="1"><li><strong>Markdown</strong> 让我们免于 <strong>被繁杂臃肿的功能晃花了眼</strong> 的困扰</li><li><strong>Markdown</strong> 让我们回归内容本身，拥抱笔记的<strong>内核</strong>，而非浮于<strong>表象</strong>的样式，写出<strong>高效精练</strong>的笔记！</li></ol><p>用 <strong>Markdown</strong> 写东西，记住一个原则</p><blockquote><p>能用10个字搞定的，绝不用11个字</p></blockquote><p>经常使用 <strong>Markdown</strong> 书写的朋友，也许会有一种奇妙的感触 - 书写，会==倒逼==思维的跃进。像是有东西拽着你的思绪往前冲 - 倒逼：逆向逼迫，反向推动</p><p><strong>关于标识符的滥用</strong></p><p>这个其实是写在最后的，之所以放在这里，是因为它<strong>很重要</strong>！</p><p>如果你有一定的MD语法基础，可以直接[[#19 避免标识符的滥用|点击跳转]]</p><p><br></p><h2 id="markdown-相关软件推荐">Markdown 相关软件推荐</h2><ul><li><strong>Markdown</strong> <strong>书写软件</strong> 推荐：<strong>Typora</strong> 优秀的 MD网页文章 书写软件<ul><li><a href="https://www.typora.io/" title="Typora编辑器">点击跳转下载地址</a><ul><li>#提示 以前是免费的，现在收费了，不过是买断制</li></ul></li></ul></li><li><strong>Markdown</strong> <strong>笔记软件</strong> 推荐：<strong>Obsidian</strong> <strong>银河系最强</strong> <strong>MD+双向链</strong> 笔记软件<ul><li><a href="https://obsidian.md/" title="银河系第一笔记软件 Obsidian">点击跳转下载地址</a></li></ul></li></ul><p><br><br></p><h1 id="markdown-语法">Markdown 语法</h1><ul><li><strong>提示1：</strong> 本教程推荐使用 <strong>Obsidian</strong> 打开阅读</li><li><strong>提示2：</strong> 下文提到的所有标识符都是 <strong>英文状态</strong> 的 <strong>！</strong></li></ul><h2 id="标题目录">1. 标题&amp;目录</h2><p><br></p><h3 id="标题">1.1 标题</h3><ul><li>Markdown标题共有 <strong>六级</strong>，和 HTML 一样</li><li>区分 <strong>一级标题 → 六级标题</strong><ul><li><strong>标题 的格式：</strong><ul><li><strong><kbd>#</kbd></strong> × 标题级数 + <strong><kbd>空格</kbd></strong> + 文本内容</li></ul></li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">这是一段普通的文本</span><br><span class="line"></span><br><span class="line"><span class="section"># 这是一级标题</span></span><br><span class="line"><span class="section">## 这是二级标题</span></span><br><span class="line"><span class="section">### 这是三级标题</span></span><br><span class="line"><span class="section">#### 这是四级标题</span></span><br><span class="line"><span class="section">##### 这是五级标题</span></span><br><span class="line"><span class="section">###### 这是六级标题 </span></span><br></pre></td></tr></table></figure><p><br></p><h3 id="目录">1.2 目录</h3><ul><li><strong>目录的 格式：</strong><ul><li>在文档的顶部 输入 <strong><code>[toc]</code></strong> ，会根据 <strong>标题</strong> 自动生成目录 ( <strong>Table of Content</strong> )</li></ul></li><li>不是所有 <strong>MD编辑器</strong> 都支持目录生成<ul><li><strong>Obsidian</strong> 就不支持，不过 <strong>OB</strong> 是自带大纲的，就是目录的效果</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入下方内容会生成一个目录：</span><br><span class="line"></span><br><span class="line">[toc]</span><br></pre></td></tr></table></figure><p><br><br></p><h2 id="斜体粗体">2. 斜体&amp;粗体</h2><p><br></p><h3 id="斜体">2.1 斜体</h3><ul><li><strong>斜体 的格式：</strong><ol type="1"><li><code>*</code> + 文本内容 + <code>*</code></li><li><code>_</code> + 文本内容 + <code>_</code> ( 下划线 )</li></ol></li><li><strong>说明：</strong><ul><li>斜体文本，首尾只有 <strong>单个</strong> 标识符</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这是一段普通文本</span><br><span class="line"></span><br><span class="line"><span class="emphasis">*这里是一段斜体文本*</span></span><br><span class="line"><span class="emphasis">_这也是一段斜体文本_</span></span><br></pre></td></tr></table></figure><h4 id="示范">示范</h4><p>这是一段普通文本</p><p><em>这里是一段斜体文本</em> <em>这也是一段斜体文本</em></p><p><br></p><h3 id="粗体">2.2 粗体</h3><ul><li><strong>粗体 的格式：</strong><ol type="1"><li><code>**</code> + 文本内容 + <code>**</code></li><li><code>__</code> + 文本内容 + <code>__</code> (这里是两个 **<kbd>_</kbd>** )</li></ol></li><li><strong>说明：</strong><ul><li>粗体文本，首尾各有 <strong>两个</strong> 标识符</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这是一段普通文本</span><br><span class="line"></span><br><span class="line"><span class="strong">**这里是一段加粗文本**</span></span><br><span class="line"><span class="strong">__这也是一段加粗文本__</span></span><br></pre></td></tr></table></figure><h4 id="示范-1">示范</h4><p>这是一段普通文本</p><p><strong>这里是一段加粗文本</strong> <strong>这也是一段加粗文本</strong></p><p><br></p><h3 id="粗斜体-斜粗体">2.3 粗斜体 (斜粗体)</h3><ul><li><strong>粗斜体 的格式：</strong><ol type="1"><li><code>***</code> + 文本内容 + <code>***</code></li><li><code>___</code> + 文本内容 + <code>___</code> （ 这里是3个 <kbd>_</kbd> )</li><li><code>**_</code> + 文本内容 + <code>_**</code></li><li><code>__*</code> + 文本内容 + <code>*__</code></li><li><code>*__</code> + 文本内容 + <code>__*</code></li><li><code>_**</code> + 文本内容 + <code>**_</code></li></ol></li><li><strong>说明：</strong><ul><li>粗斜体文本，首尾各有 <strong>三个</strong> 标识符</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">这是一段普通文本</span><br><span class="line"></span><br><span class="line"><span class="strong">**<span class="emphasis">*粗斜体文本1*</span>**</span></span><br><span class="line"><span class="strong">__<span class="emphasis">_粗斜体文本2_</span>__</span></span><br><span class="line"><span class="strong">**<span class="emphasis">_粗斜体文本3_</span>**</span></span><br><span class="line"><span class="strong">__<span class="emphasis">*粗斜体文本4*</span>__</span></span><br><span class="line"><span class="emphasis">*<span class="strong">__粗斜体文本5__</span>*</span></span><br><span class="line"><span class="emphasis">_<span class="strong">**粗斜体文本6**</span>_</span></span><br></pre></td></tr></table></figure><h4 id="示范-2">示范</h4><p>这是一段普通文本</p><p><strong><em>粗斜体文本1</em></strong> <strong><em>粗斜体文本2</em></strong> <strong><em>粗斜体文本3</em></strong> <strong><em>粗斜体文本4</em></strong> <em><strong>粗斜体文本5</strong></em> <em><strong>粗斜体文本6</strong></em></p><p><br></p><h3 id="斜体包含粗体">2.4 斜体包含粗体</h3><ul><li><strong>斜体中包含粗体 的格式：</strong><ol type="1"><li><code>*</code> + 斜体文本 + <code>**</code> + 粗体文本 + <code>**</code> + 斜体文本 + <code>*</code></li><li><code>_</code> + 斜体文本 + <strong><code>__</code></strong> + 粗体文本 + <code>__</code> + 斜体文本 + <code>_</code> （ 这里是两个 **<kbd>_</kbd>** )</li><li><code>*</code> + 斜体文本 + <code>__</code> + 粗体文本 + <code>__</code> + 斜体文本 + <code>*</code></li><li><code>_</code> + 斜体文本 + <code>**</code> + 粗体文本 + <code>**</code> + 斜体文本 + <code>_</code></li></ol></li><li><strong>说明：</strong><ul><li><strong>斜体</strong> 中包含 <strong>粗体</strong>，其实就是嵌套的关系，<strong>外层</strong> 是 <strong>斜体</strong>，<strong>内层</strong> 是 <strong>粗体</strong></li><li>外层是<strong>斜体</strong>，标识符是<strong>单个</strong>；内层是<strong>粗体</strong>，标识符是<strong>两个</strong></li><li>因为 <strong>粗体</strong> 是被包裹在 <strong>斜体</strong> 中的，所以显示效果为 <strong>斜粗体</strong></li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这是一段普通文本</span><br><span class="line"></span><br><span class="line"><span class="emphasis">*这里是一段斜体中<span class="strong">**包含粗体**</span>的文字*</span></span><br><span class="line"><span class="emphasis">_这也是一段斜体中<span class="strong">**包含粗体**</span>的文字_</span></span><br><span class="line"><span class="emphasis">*这又是一段斜体中<span class="strong">__包含粗体__</span>的文字*</span></span><br><span class="line"><span class="emphasis">_这还是一段斜体中<span class="strong">**包含粗体**</span>的文字_</span></span><br></pre></td></tr></table></figure><h4 id="示范-3">示范</h4><p>这是一段普通文本</p><p><em>这里是一段斜体中<strong>包含粗体</strong>的文字</em> <em>这也是一段斜体中<strong>包含粗体</strong>的文字</em> *这又是一段斜体中__包含粗体__的文字* <em>这还是一段斜体中<strong>包含粗体</strong>的文字</em></p><p><br></p><h3 id="粗体包含斜体">2.5 粗体包含斜体</h3><ul><li><strong>粗体中包含斜体 的格式：</strong><ol type="1"><li><code>**</code> + 粗体文本 + <code>*</code> + 斜体文本 + <code>*</code> + 粗体文本 + <code>**</code></li><li><code>__</code> + 粗体文本 + <code>_</code> + 斜体文本 + <code>_</code> + 粗体文本 + <code>__</code> （ 这里是两个 **<kbd>_</kbd>** )</li><li><code>**</code> + 粗体文本 + <code>_</code> + 斜体文本 + <code>_</code> + 粗体文本 + <code>**</code></li><li><code>__</code> + 粗体文本 + <code>*</code> + 斜体文本 + <code>*</code> + 粗体文本 + <code>__</code></li></ol></li><li><strong>说明：</strong><ul><li><strong>粗体</strong> 中包含 <strong>斜体</strong>，也就是嵌套的关系，<strong>外层</strong> 是 <strong>粗体</strong>，<strong>内层</strong> 是 <strong>斜体</strong></li><li>外层是<strong>粗体</strong>，标识符是<strong>两个</strong>；内层是<strong>斜体</strong>，标识符是<strong>单个</strong></li><li>因为 <strong>斜体</strong> 是被包裹在 <strong>粗体</strong> 中的，所以显示效果为 <strong>粗斜体</strong></li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这是一段普通文本</span><br><span class="line"></span><br><span class="line"><span class="strong">**这里是一段粗体中<span class="emphasis">*包含斜体*</span>的文字**</span></span><br><span class="line"><span class="strong">__这也是一段粗体中<span class="emphasis">_包含斜体_</span>的文字__</span></span><br><span class="line"><span class="strong">**这又是一段粗体中<span class="emphasis">_包含斜体_</span>的文字**</span></span><br><span class="line"><span class="strong">__这还是一段粗体中<span class="emphasis">*包含斜体*</span>的文字__</span></span><br></pre></td></tr></table></figure><h4 id="示范-4">示范</h4><p>这是一段普通文本</p><p><strong>这里是一段粗体中<em>包含斜体</em>的文字</strong> <strong>这也是一段粗体中_包含斜体_的文字</strong> <strong>这又是一段粗体中_包含斜体_的文字</strong> <strong>这还是一段粗体中<em>包含斜体</em>的文字</strong></p><p><br><br></p><h2 id="线">3. 线</h2><p><br></p><h3 id="水平分割线">3.1 水平分割线</h3><ul><li>水平分割线由至少 <strong>3</strong> 个 <code>*</code> 或 <code>-</code> 组成</li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">下面是一条水平分割线：</span></span><br><span class="line"><span class="section">---</span></span><br><span class="line"><span class="strong">**<span class="emphasis">*</span></span></span><br></pre></td></tr></table></figure><h4 id="示范-5">示范</h4><hr /><hr /><p><br></p><h3 id="文本删除线">3.2 文本删除线</h3><ul><li><strong>删除线 的格式：</strong><ul><li><strong><code>~~</code></strong> + 文本内容 +<strong><code>~~</code></strong> 首尾各加两个 <strong><kbd>~</kbd></strong> 波浪号</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~~这是一段加了删除线的文本~~</span><br></pre></td></tr></table></figure><h4 id="示范-6">示范</h4><p><del>这是一段加了删除线的文本</del></p><p><br></p><h3 id="文本下划线">3.3 文本下划线</h3><ul><li>下划线的格式，和 HTML 是一样的<ul><li><strong><code>&lt;u&gt;</code></strong> + 文本内容 + <strong><code>&lt;/u&gt;</code></strong></li></ul></li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">u</span>&gt;</span>这是一段加了下划线的文本<span class="tag">&lt;/<span class="name">u</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="示范-7">示范</h4><p><u>这是一段加了下划线的文本</u></p><p><br><br></p><h2 id="列表引用">4. 列表&amp;引用</h2><p><br></p><h3 id="有序列表">4.1 有序列表</h3><ul><li><strong>有序列表 的格式：</strong><ul><li><strong><code>1.</code></strong> + <strong><kbd>空格</kbd></strong> + 文本内容</li></ul></li><li><strong>说明：</strong><ul><li>输入文本内容后，敲击 <strong><kbd>Enter</kbd></strong> 自动补全格式，并进入 <strong>下个</strong> 有序列表</li><li>若需要在同个列表内，增加 <strong>换行显示</strong> 的内容 (<strong>但不进入下个列表</strong>) 敲击 <strong><kbd>Shift</kbd></strong> + <strong><kbd>Enter</kbd></strong> ，即可另起一行输入文本</li><li>在有序列表的中间，插入一个新的列表，后面列表的 <strong>数字序号</strong> 会自动 <strong>递进</strong> 一层</li><li>即便在源代码模式中修改了数字序号，渲染界面依然是 <strong>依照顺序</strong> 显示的</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> 这是第一个有序列表 &lt;!-- (Enter) --&gt;</span><br><span class="line"><span class="bullet">2.</span> 这是第二个有序列表 &lt;!-- (Enter) --&gt;</span><br><span class="line"><span class="bullet">3.</span> 这是第三个有序列表 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="bullet">1.</span> 这是第一个有序列表 &lt;!-- (Shift + Enter) --&gt;</span><br><span class="line">   这是同个列表下，另起一行的文本内容 &lt;!-- (Enter) --&gt;</span><br><span class="line"><span class="bullet">2.</span> 这是第二个有序列表 &lt;!-- (Shift + Enter) --&gt;</span><br><span class="line">   这是同个列表下，另起一行的文本内容 </span><br></pre></td></tr></table></figure><h4 id="示范-8">示范</h4><ol type="1"><li>这是第一个有序列表</li><li>这是第二个有序列表</li><li><p>这是第三个有序列表</p></li><li>这是第一个有序列表 这是同个列表下，另起一行的文本内容</li><li><p>这是第二个有序列表 这是同个列表下，另起一行的文本内容</p></li></ol><h4 id="补充">补充</h4><ul><li>由于有序列表存在<strong>强制排序性</strong>，它的数字序号必然是<strong>逐一递进</strong>的 若你希望内容前的数字，不依照<strong>递进顺序</strong>排序，或者以 <strong>整百</strong>，<strong>整十数</strong> 排序</li><li>可以配合<strong>无序列表</strong>，在无序列表中输入：<ul><li><code>数字</code> + <code>.</code> + 内容<br />#注意 点号 与 内容 之间，<strong>没有空格</strong> (其实有空格也行，就是会感觉有点奇怪)</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> 10.这是无序列表下，整十数排列的内容</span><br><span class="line"><span class="bullet">-</span> 20.这是无序列表下，整十数排列的内容</span><br><span class="line"><span class="bullet">-</span> 30.这是无序列表下，整十数排列的内容</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> 100.这是无序列表下，整百数排列的内容</span><br><span class="line"><span class="bullet">-</span> 200.这是无序列表下，整百数排列的内容</span><br><span class="line"><span class="bullet">-</span> 300.这是无序列表下，整百数排列的内容</span><br></pre></td></tr></table></figure><p><strong>效果：</strong></p><ul><li>10.这是无序列表下，整十数排列的内容</li><li>20.这是无序列表下，整十数排列的内容</li><li>30.这是无序列表下，整十数排列的内容</li></ul><p><br></p><ul><li>100.这是无序列表下，整百数排列的内容</li><li>200.这是无序列表下，整百数排列的内容</li><li>300.这是无序列表下，整百数排列的内容</li></ul><p><br></p><h3 id="无序列表">4.2 无序列表</h3><ul><li><strong>无序列表 的格式：</strong></li><li><strong><kbd>-</kbd></strong> + <strong><kbd>空格</kbd></strong> + 文本内容</li><li><strong>说明：</strong><ul><li>输入文本内容后，敲击 <strong><kbd>Enter</kbd></strong> 自动补全格式，并进入 <strong>下个</strong> 无序列表</li><li>若需要在同个列表内，增加<strong>换行</strong>显示的内容 (<strong>但不进入下个列表</strong>) 敲击 <strong><kbd>Shift</kbd></strong> + <strong><kbd>Enter</kbd></strong> ，即可另起一行输入文本</li></ul></li><li><strong>补充：</strong><ul><li>在<strong>Obsidian</strong>中，按下 <strong><kbd>Ctrl</kbd></strong> + <strong><kbd>Enter</kbd></strong></li><li>即可快速生成一个无序列表</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> 这是第1个无序列表 &lt;!-- (Enter) --&gt;</span><br><span class="line"><span class="bullet">-</span> 这是第2个无序列表 &lt;!-- (Enter) --&gt;</span><br><span class="line"><span class="bullet">-</span> 这是第3个无序列表</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> 这是第一个无序列表 &lt;!-- (Shift + Enter) --&gt;</span><br><span class="line">  这是同个列表下，另起一行的文本内容</span><br><span class="line"><span class="bullet">-</span> 这是第二个无序列表 &lt;!-- (Shift + Enter) --&gt;</span><br><span class="line">  这是同个列表下，另起一行的文本内容 </span><br></pre></td></tr></table></figure><h4 id="示范-9">示范</h4><ul><li>这是第1个无序列表</li><li>这是第2个无序列表</li><li>这是第3个无序列表</li></ul><p><br></p><ul><li>这是第一个无序列表 这是同个列表下，另起一行的文本内容</li><li>这是第二个无序列表 这是同个列表下，另起一行的文本内容</li></ul><p><br></p><h3 id="引用">4.3 引用</h3><ul><li><strong>引用 的格式：</strong><ul><li><strong><kbd>&gt;</kbd></strong> + 文本内容 （<strong>不需要空格</strong>)</li></ul></li><li><strong>说明：</strong><ul><li><strong>同个引用段落</strong>内的换行直接敲击 <strong><kbd>Enter</kbd></strong> 即可</li><li>若需添加 <strong>第二个独立引用段落</strong> ，连续敲击 <strong>两下</strong> <strong><kbd>Enter</kbd></strong> 即可</li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;这是第一段引用文本的第1行 &lt;!-- (Enter) --&gt;</span><br><span class="line">&gt;这是第一段引用文本的第2行 &lt;!-- (Enter) --&gt;</span><br><span class="line">&lt;!-- (Enter) --&gt;</span><br><span class="line">&gt;这是第二段引用文本的第1行 &lt;!-- (Enter) --&gt;</span><br><span class="line">&gt;这是第二段引用文本内第2行</span><br></pre></td></tr></table></figure><h4 id="示范-10">示范</h4><blockquote><p>这是第一段引用文本的第1行 这是第一段引用文本的第2行</p></blockquote><blockquote><p>这是第二段引用文本的第1行 这是第二段引用文本的第2行</p></blockquote><p><br></p><h3 id="缩进退格">4.4 缩进&amp;退格</h3><p><strong>在列表和引用的书写过程中，我们需要利用 ==缩进== 与 ==退格== ，让文章肌理分明，更具层级</strong></p><ul><li><strong>缩进：</strong><ol type="1"><li><strong><kbd>Tab</kbd></strong></li><li><strong><kbd>Ctrl</kbd></strong> + <strong><kbd>[</kbd></strong>   (左中括号)</li></ol></li><li><strong>退格：</strong><ol type="1"><li><strong><kbd>Shift</kbd></strong> + <strong><kbd>Tab</kbd></strong></li><li><strong><kbd>Ctrl</kbd></strong> + <strong><kbd>]</kbd></strong> （右中括号）</li></ol></li></ul><p><br></p><h4 id="有序列表的缩退">4.4.1 有序列表的缩&amp;退</h4><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> 第一级有序列表1 &lt;!-- (Enter) --&gt;</span><br><span class="line"><span class="bullet">1.</span> 第二级有序列表1    &lt;!-- 写文本之前，先( Tab 或 Ctrl + ] ) ；写完文本后，再(Enter) --&gt;</span><br><span class="line"><span class="bullet">2.</span> 第二级有序列表2 &lt;!-- (Enter) --&gt;</span><br><span class="line"><span class="bullet">2.</span> 第一级有序列表2    &lt;!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --&gt; </span><br></pre></td></tr></table></figure><ul><li><strong>补充说明：</strong><ul><li>有序列表的<strong>数字序号</strong>，即便你在源代码模式里 强行改掉 数字，它仍然会 <strong>依照顺序</strong> 显示</li></ul></li></ul><h5 id="示范-11">示范</h5><ol type="1"><li>第一级有序列表1<ol type="1"><li>第二级有序列表1</li><li>第二级有序列表2</li></ol></li><li>第一级有序列表2</li></ol><p><br></p><h4 id="无序列表的缩退">4.4.2 无序列表的缩&amp;退</h4><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> 第一级无序列表1 &lt;!-- (Enter) --&gt;</span><br><span class="line"><span class="bullet">-</span> 第二级无序列表1  &lt;!-- 写文本前，先( Tab 或 Ctrl + ] ) ；写完后，再(Enter) --&gt;</span><br><span class="line"><span class="bullet">-</span> 第二级无序列表2 &lt;!-- (Enter) --&gt;</span><br><span class="line"><span class="bullet">-</span> 第一级无序列表2  &lt;!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --&gt;</span><br></pre></td></tr></table></figure><h5 id="示范-12">示范</h5><ul><li>第一级无序列表1<ul><li>第二级无序列表1</li><li>第二级无序列表2</li></ul></li><li>第一级无序列表2</li></ul><p><br></p><h4 id="引用的缩退">4.4.3 引用的缩&amp;退</h4><ul><li>引用的 <strong>缩进</strong> 和列表 <strong>不同</strong><ul><li>引用需另起一行，并额外多打一个 <kbd>&gt;</kbd> 来完成 <strong>缩进</strong></li></ul></li><li>引用的 <strong>退格</strong> 与列表 <strong>相同</strong><ol type="1"><li><strong><kbd>Shift</kbd></strong> + <strong><kbd>Tab</kbd></strong></li><li><strong><kbd>Ctrl</kbd></strong> + <strong><kbd>]</kbd></strong> （右中括号）</li></ol></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;第一级引用1 &lt;!-- (enter) --&gt;</span><br><span class="line">&gt;&gt;第二级引用1 &lt;!-- 先打1个 &gt; (这里的第一个 &gt; 是会自动补充的，只需额外增补1个即可) ，再(enter) --&gt;</span><br><span class="line">&gt;&gt;第二级引用2 &lt;!-- (enter) --&gt;</span><br><span class="line">&gt;第一级引用2   &lt;!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --&gt;</span><br></pre></td></tr></table></figure><h5 id="示范-13">示范</h5><blockquote><p>第一级引用1 &gt;第二级引用1 &gt;第二级引用2</p><p>第一级引用2</p></blockquote><p><br></p><ul><li><strong>补充：</strong> 在 <strong>Obsidian</strong> 中，引用的退格是不太一样的</li><li><strong>Obsidian </strong>中，如果想让已经缩进的引用 <strong>退回一层</strong><ul><li>得使用 <strong><code>Shift</code></strong> + <strong><code>Enter</code></strong> ，配合方向键，在多个 <strong><code>&gt;</code></strong> 之间灵活断行 并在下一行 根据需要 选择性补充 <strong><code>&gt;</code></strong></li></ul></li><li>这个用文字比较难以描述，这里选择用2个带键位的 <strong>Gif图</strong> 来描述</li></ul><p><strong>Gif演示1：</strong></p><p><br></p><figure><img src="https://z3.ax1x.com/2021/08/09/fGPDVs.gif" alt="引用退格1" /><figcaption>引用退格1</figcaption></figure><p><br></p><ul><li><strong>效果1：</strong></li></ul><blockquote><p>111 &gt;222 &gt;&gt;333 &gt; &gt;444</p><p>555</p></blockquote><p><br></p><p><strong>Gif演示2：</strong> <br></p><figure><img src="https://z3.ax1x.com/2021/08/09/fGA1Qf.gif" alt="引用退格2" /><figcaption>引用退格2</figcaption></figure><p><br></p><ul><li><strong>效果2：</strong></li></ul><blockquote><p>111 &gt;222 &gt;&gt;333</p><blockquote><p>444 &gt;555</p><p>666</p></blockquote><p>777</p></blockquote><p><br></p><h4 id="有序无序引用-连续套娃">4.4.4 有序&amp;无序&amp;引用 连续套娃</h4><ul><li><strong>有序列表</strong>、<strong>无序列表</strong>、<strong>引用</strong> 三者之间，可以相互嵌套</li><li><strong>核心键</strong> ： <strong><kbd>Shift</kbd></strong> + <strong><kbd>Enter</kbd></strong> <strong>&amp;</strong> <strong><kbd>Enter</kbd></strong> <strong>&amp;</strong> <strong><kbd>Shift</kbd></strong> + <strong><kbd>Tab</kbd></strong> ( 或 <strong><kbd>Ctrl</kbd></strong> + <strong><kbd>[</kbd></strong> )<ul><li><strong><kbd>Shift</kbd></strong> + <strong><kbd>Enter</kbd></strong> 在切换格式的嵌套中，是 自带一层 <strong>缩进</strong> 效果的</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> 第一级 有序列表1 &lt;!-- (Shift + Enter) --&gt; </span><br><span class="line"><span class="bullet">-</span> 第二级 无序列表1 &lt;!-- (Shift + Enter) --&gt;</span><br><span class="line"><span class="code">&gt;第三级 引用1  &lt;!-- (Enter) --&gt;</span></span><br><span class="line"><span class="code">- 第四级 无序列表2 &lt;!-- (Shift + Enter) --&gt;</span></span><br><span class="line"><span class="code">            1. 第五级 有序列表2 &lt;!-- (Enter) --&gt;</span></span><br><span class="line"><span class="code">            - 第四级 无序列表3   &lt;!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) ；写完后再 (Enter) --&gt;</span></span><br><span class="line"><span class="code">        &gt;第三级 引用2  &lt;!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) ；写完后再 (Enter × 2) --&gt;</span></span><br><span class="line"><span class="code">    - 第二级 无序列表4  &lt;!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) --&gt;</span></span><br><span class="line"><span class="code">2. 第一级 有序列表3  &lt;!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) --&gt;</span></span><br></pre></td></tr></table></figure><h5 id="示范-14">示范</h5><ol type="1"><li><p>第一级 有序列表1</p><ul><li><p>第二级 无序列表1</p><blockquote><p>第三级 引用1</p><ul><li>第四级 无序列表2<ol type="1"><li>第五级 有序列表2</li></ol></li><li>第四级 无序列表3</li></ul><p>第三级 引用2</p></blockquote></li><li><p>第二级 无序列表4</p></li></ul></li><li><p>第一级 有序列表3</p></li></ol><h4 id="obsidian-的一些缩退问题">4.4.5 Obsidian 的一些缩退问题</h4><ul><li><strong>Obsidian</strong> 在列表首行使用缩进的时候，后续的列表会出现一些问题<ul><li><code>Tab</code> 和 <code>Shift + tab</code> 会无法 缩进 退格<ul><li>可以使用 <code>Ctrl + ]</code> 与 <code>Ctrl + [</code> 来解决问题</li></ul></li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> - 这是第一段就被缩进的列表</span><br><span class="line"><span class="bullet">-</span> 这是第二段被再次缩进的列表  &lt;!-- 这里需按两次 Ctrl + ] ,Tab键是无效的 --&gt;</span><br><span class="line"><span class="bullet">  -</span> 这是第三段列表  &lt;!-- Ctrl + [ --&gt;</span><br></pre></td></tr></table></figure><ul><li><ul><li>这是第一段就被缩进的列表 - 这是第二段被再次缩进的列表<ul><li>这是第三段列表</li></ul></li></ul></li></ul><p><br><br></p><h2 id="网页链接与图像">5. 网页链接与图像</h2><p><br></p><h3 id="网页链接">5.1 网页链接</h3><ul><li><strong>网页链接的 格式：</strong><ul><li><strong><kbd>[</kbd>** + 显示文本内容 + **<kbd>]</kbd></strong> + <strong><kbd>(</kbd></strong> + 链接地址 + <strong><kbd>空格</kbd></strong> + <strong><kbd>&quot;</kbd></strong> + 提示信息文本 + <strong><kbd>&quot;</kbd></strong> + <strong><kbd>)</kbd></strong></li></ul></li><li><strong>说明：</strong><ul><li>显示文本内容，是在渲染界面实际 <strong>可见</strong> 的文本，用以 <strong>说明</strong> 链接</li><li>提示信息文本，需鼠标悬停于 <strong>显示文本内容</strong> 方可触发，用于增加额外提示信息<ul><li>#注意 <strong><code>&quot;提示信息文本&quot;</code></strong> 是<strong>可选项</strong>，一般不会填</li><li>一般来讲，需按住 <strong><kbd>Ctrl</kbd></strong> + <strong><code>鼠标左键点击</code></strong> 才可跳转链接，不过也有 <strong>直接鼠标点击</strong> 就能跳转的</li></ul></li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">显示文本内容</span>](<span class="link">链接地址 &quot;提示信息文本&quot;</span>)</span><br><span class="line"></span><br><span class="line">[<span class="string">百度一下，你就知道</span>](<span class="link">http://www.baidu.com &quot;按住Ctrl点击跳转百度&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>示范：</strong></p><p><a href="http://www.baidu.com" title="按住Ctrl点击跳转百度">百度一下，你就知道</a></p><p><br></p><h4 id="链接的加粗">5.1.1链接的加粗</h4><ul><li><strong>格式有两种：</strong><ol type="1"><li>把一对 <kbd>**</kbd> 加在 ==显示文本内容==的首尾<ul><li><strong>格式1：</strong><code>[**显示文本内容**](链接地址)</code></li><li><strong>效果：</strong> <a href="http://www.baidu.com"><strong>百度一下，你就知道</strong></a></li></ul></li><li>把一对 <kbd>**</kbd> 加在 链接格式==整体== 的首尾<ul><li><strong>格式2：</strong><code>**[显示文本内容](链接地址)**</code></li><li><strong>效果：</strong> <strong><a href="http://www.baidu.com">百度一下，你就知道</a></strong></li></ul></li></ol></li></ul><p><br><br></p><h3 id="图像">5.2 图像</h3><ul><li><strong>图像格式：</strong><ul><li>图像格式，就是在网页链接前面加个 <strong><kbd>!</kbd></strong> (英文格式的)，<strong><code>!</code></strong> 代表 <strong>可见</strong></li><li>图片的提示信息，和网页链接一样，写在 <strong><code>&quot; &quot;</code></strong> 内</li><li><strong><code>[ ]</code></strong> 方括号里的文字信息在 <strong>Markdown</strong> 没啥实质的作用，只是方便在源代码模式下，知道这个图片是什么，在渲染界面是不会显示的。有点类似于HTML <strong>img标签</strong> 里的 <strong>alt属性</strong>。</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">![<span class="string">文字信息</span>](<span class="link">图片链接 &quot;提示文本信息&quot;</span>)</span><br><span class="line"></span><br><span class="line">![<span class="string">湘湖1</span>](<span class="link">https://z3.ax1x.com/2021/08/06/fuNkXq.jpg &quot;湘湖一角&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li><p><strong>补充：</strong></p><ul><li>图像链接可以是<strong>本地</strong>的，也可以是<strong>在线</strong>的<ul><li>本地图像直接 <strong><code>Ctrl + C</code></strong> 黏贴，<strong><code>Ctrl + V</code></strong> 复制 就可以</li><li>在线图像推荐使用 <a href="https://imgtu.com/" title="这是一个在线图床网址">图床</a></li></ul></li><li>调整图像的大小需要使用 HTML 和 CSS，在 <strong>Typora编辑器</strong> 中右键可以直接缩放图片 本质是转成了HTML的格式，最后会有一个 <code>style=&quot;zoom: %;&quot;</code> ，这里数值可以自己修改</li><li>如果有使用 <strong>Obsidian</strong> 的朋友，在线图片链接是通用的。不过，因为 <strong>Obsidian</strong> 是双向链笔记 它的<strong>本地图片</strong>格式不太一样<ul><li><strong><code>![[图片名]]</code></strong><ul><li><strong>Obsidian</strong> 中的图片是以<strong>双链</strong>的格式引用在目标笔记中，用 <strong><kbd>!</kbd></strong> 使它可见</li><li><strong>Obsidian</strong>的图片设置大小是用 <strong><kbd>|</kbd></strong> 分隔，后面写宽度数值，单位是px。 设定好宽度，高度会自动<strong>等比例调整</strong><ul><li><code>![[图片名|宽度数值]]</code> - 若想自主调整图片宽高，则用： - <code>![[图片名|宽度数值x高度数值]]</code> - #提示 这里的 <code>x</code> 是 英文字母<strong>x</strong></li></ul></li></ul></li><li>如果是<strong>在线图床</strong>，需要调整图片大小：<ul><li><code>![图床|宽度数值](链接地址)</code> #### 示范</li></ul></li></ul></li></ul></li></ul><figure><img src="https://z3.ax1x.com/2021/08/06/fuNkXq.jpg" title="湘湖一角" alt="湘湖1|400" /><figcaption>湘湖1|400</figcaption></figure><p><br><br></p><h2 id="表格">6. 表格</h2><ul><li>Markdown的表格，比HTML简单很多<ul><li><strong><kbd>|</kbd></strong> 是构成表格的主要 <strong>框架</strong></li><li><strong><kbd>-</kbd></strong> 区分 <strong>表头</strong> 和 <strong>表格主体</strong></li><li><strong><kbd>:</kbd></strong> 控制 表格内 <strong>文本内容</strong> 的 <strong>对齐方式</strong></li><li><strong>Typora编辑器中 </strong> 输入 <strong><code>Ctrl + T</code></strong> 即可快速插入表格，自由定义样式</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">|这里是表头1|这里是表头2|这里是表头3|</span><br><span class="line">|:-|:-:|-:|    &lt;!--区分表头和表格主体，:代表文本对齐方式，分别是左对齐，居中对齐，右对齐--&gt;</span><br><span class="line">|单元格数据1|单元格数据2|单元格数据3|</span><br><span class="line">|单元格数据4|单元格数据5|单元格数据6|</span><br></pre></td></tr></table></figure><h4 id="示范-15">示范</h4><table><thead><tr class="header"><th style="text-align: left;">这里是表头1</th><th style="text-align: center;">这里是表头2</th><th style="text-align: right;">这里是表头3</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">单元格数据1</td><td style="text-align: center;">单元格数据2</td><td style="text-align: right;">单元格数据3</td></tr><tr class="even"><td style="text-align: left;">单元格数据4</td><td style="text-align: center;">单元格数据5</td><td style="text-align: right;">单元格数据6</td></tr></tbody></table><p><br></p><h3 id="表格中文本内容的换行">6.1 表格中文本内容的换行</h3><ul><li>Mardown中表格，它的宽高是由 单元格数据内的文本内容 <strong>撑开</strong> 的</li><li>当我们输入一段很长很长的文本，它所在的单元格会变得过宽</li></ul><p><strong>如下图所示：</strong></p><table><thead><tr class="header"><th style="text-align: center;">表头1</th><th style="text-align: center;">表头2</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">这是一段很长很长很长很长很长很长很长很长很长很长很长很长很长很长的文本</td><td style="text-align: center;">普通文本</td></tr></tbody></table><ul><li>若想对一段长文本进行换行，可以在 <strong>中间</strong> 插入一个 <strong><code>&lt;br&gt;</code></strong> （ 换行标签 )</li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">| 表头1 |  表头2 |</span><br><span class="line">|:-:|:-:|</span><br><span class="line">|这是第一行文本<span class="language-xml"><span class="tag">&lt;<span class="name">br</span>&gt;</span></span>这是另起一行的文本|普通文本|</span><br></pre></td></tr></table></figure><h4 id="示范-16">示范</h4><table><thead><tr class="header"><th style="text-align: center;">表头1</th><th style="text-align: center;">表头2</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">这是第一行文本<br>这是另起一行的文本</td><td style="text-align: center;">普通文本</td></tr></tbody></table><p><br><br></p><h2 id="代码域">7. 代码域</h2><p><br></p><h3 id="行内代码">7.1 行内代码</h3><ul><li><strong>行内代码 的格式：</strong><ul><li>输入两个 <strong><kbd>`</kbd></strong> 反引号 ，在中间写代码内容</li></ul></li><li><strong>补充：</strong><ul><li>行内代码不一定非得写代码，也可以作为<strong><code>着重标记</code></strong>，突出显示内容</li><li>行内代码中，源代码界面和渲染界面是完全一致的，标识符会失效</li><li><strong>所谓行内代码：</strong> 只要你的屏幕足够宽，它就不会换行</li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">`这是一段行内代码`</span></span><br><span class="line"></span><br><span class="line"><span class="code">`&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; width=&quot;500&quot; height=&quot;500&quot;&gt;`</span></span><br><span class="line"></span><br><span class="line"><span class="code">`print(&quot;Hello, World!&quot;)`</span></span><br><span class="line"></span><br><span class="line"><span class="code">`这是一行突出显示的文本内容`</span></span><br></pre></td></tr></table></figure><h4 id="示范-17">示范</h4><p><code>&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; width=&quot;500&quot; height=&quot;500&quot;&gt;</code></p><p><br></p><p><code>print(&quot;Hello, World!&quot;)</code></p><p><br></p><p><code>这是一行突出显示的文本内容</code></p><p><br></p><h3 id="代码块">7.2 代码块</h3><ul><li><strong>代码块 的格式：</strong><ol type="1"><li>在首行和末行各加 <strong>三个</strong> <strong><kbd>`</kbd></strong> 反引号</li></ol><ul><li><strong><code>```</code></strong> + 语言种类 代码内容 <strong><code>```</code></strong></li></ul><ol start="2" type="1"><li>在首行和末行各加 <strong>三个</strong> <strong><kbd>~</kbd></strong> 波浪号<ul><li><strong><code>~~~</code></strong> + 语言种类 代码内容 <strong><code>~~~</code></strong></li></ul></li></ol></li><li><strong>补充：</strong><ul><li>在代码块也不一定要写代码，可以写<strong>一段</strong>突出的文本内容，语言类型可以填写 <strong>txt</strong> 或者 <strong>干脆不写</strong></li><li>代码块中，源代码界面和渲染界面是完全一致的，标识符会失效</li><li>在 <strong>Typora编辑器</strong> ，用键盘按键脱离代码块区域，需输入： <strong><kbd>Ctrl</kbd> + <kbd>Enter</kbd></strong></li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">```语言种类</span></span><br><span class="line"><span class="code">代码内容</span></span><br><span class="line"><span class="code">代码内容</span></span><br><span class="line"><span class="code">代码内容</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line"></span><br><span class="line">下面是HTML代码块</span><br><span class="line"></span><br><span class="line"><span class="code">```html</span></span><br><span class="line"><span class="code">&lt;table border=&quot;1&quot;&gt;</span></span><br><span class="line"><span class="code">    &lt;tr&gt;</span></span><br><span class="line"><span class="code">        &lt;td&gt;row 1, cell 1&lt;/td&gt;</span></span><br><span class="line"><span class="code">        &lt;td&gt;row 1, cell 2&lt;/td&gt;</span></span><br><span class="line"><span class="code">    &lt;/tr&gt;</span></span><br><span class="line"><span class="code">    &lt;tr&gt;</span></span><br><span class="line"><span class="code">        &lt;td&gt;row 2, cell 1&lt;/td&gt;</span></span><br><span class="line"><span class="code">        &lt;td&gt;row 2, cell 2&lt;/td&gt;</span></span><br><span class="line"><span class="code">    &lt;/tr&gt;</span></span><br><span class="line"><span class="code">&lt;/table&gt;</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line"></span><br><span class="line">下面是CSS代码块</span><br><span class="line"></span><br><span class="line"><span class="code">```css</span></span><br><span class="line"><span class="code">.box &#123;</span></span><br><span class="line"><span class="code">width: 600px;</span></span><br><span class="line"><span class="code">height: 400px;</span></span><br><span class="line"><span class="code">margin: 100px auto;</span></span><br><span class="line"><span class="code">background-image: linear-gradient(black 33.3%,red 33.3%, red 66.6%, yellow 66.6%, yellow);</span></span><br><span class="line"><span class="code">&#125;</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line"></span><br><span class="line">下面是JavaScript代码块</span><br><span class="line"></span><br><span class="line"><span class="code">```js</span></span><br><span class="line"><span class="code">    // 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程</span></span><br><span class="line"><span class="code">    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]</span></span><br><span class="line"><span class="code">    let newarr = [];</span></span><br><span class="line"><span class="code">    for (let i = 0, count = 0, sum = 0, len = arr.length; i &lt; len; i++) &#123;</span></span><br><span class="line"><span class="code">        sum += arr.shift();</span></span><br><span class="line"><span class="code">        count++;</span></span><br><span class="line"><span class="code">        if (count % 5 === 0) &#123;</span></span><br><span class="line"><span class="code">            newarr.push(sum / 5);</span></span><br><span class="line"><span class="code">            sum =  0;</span></span><br><span class="line"><span class="code">        &#125;</span></span><br><span class="line"><span class="code">    &#125;</span></span><br><span class="line"><span class="code">    console.log(newarr);</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]</span></span><br><span class="line"><span class="code">    let newarr = [];</span></span><br><span class="line"><span class="code">    for (let i = 0, len = arr.length; i &lt; len / 5; i++) &#123;</span></span><br><span class="line"><span class="code">        let subarr = arr.splice(0, 5)</span></span><br><span class="line"><span class="code">        for (let j = 0, sum = 0; j &lt; subarr.length; j++) &#123;</span></span><br><span class="line"><span class="code">            sum += subarr[j];</span></span><br><span class="line"><span class="code">        &#125;</span></span><br><span class="line"><span class="code">        newarr.push(sum / 5);</span></span><br><span class="line"><span class="code">    &#125;</span></span><br><span class="line"><span class="code">    console.log(newarr);</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">下面是Python代码块</span><br><span class="line"></span><br><span class="line"><span class="code">```python</span></span><br><span class="line"><span class="code">#!/usr/bin/python</span></span><br><span class="line"><span class="code"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">i = 2</span></span><br><span class="line"><span class="code">while(i &lt; 100):</span></span><br><span class="line"><span class="code">   j = 2</span></span><br><span class="line"><span class="code">   while(j &lt;= (i/j)):</span></span><br><span class="line"><span class="code">      if not(i%j): break</span></span><br><span class="line"><span class="code">      j = j + 1</span></span><br><span class="line"><span class="code">   if (j &gt; i/j) : print i, &quot; 是素数&quot;</span></span><br><span class="line"><span class="code">   i = i + 1</span></span><br><span class="line"><span class="code"> </span></span><br><span class="line"><span class="code">print &quot;Good bye!&quot;</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line"></span><br><span class="line">下面是一块突出显示的文本</span><br><span class="line"></span><br><span class="line"><span class="code">```txt</span></span><br><span class="line"><span class="code">这是一段</span></span><br><span class="line"><span class="code">突出显示的</span></span><br><span class="line"><span class="code">文本内容</span></span><br><span class="line"><span class="code">```</span></span><br></pre></td></tr></table></figure><h4 id="示范-18">示范</h4><p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span> <span class="attr">border</span>=<span class="string">&quot;1&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>row 1, cell 1<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>row 1, cell 2<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>row 2, cell 1<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>row 2, cell 2<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br></pre></td></tr></table></figure></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.box</span> &#123;</span><br><span class="line"><span class="attribute">width</span>: <span class="number">600px</span>;</span><br><span class="line"><span class="attribute">height</span>: <span class="number">400px</span>;</span><br><span class="line"><span class="attribute">margin</span>: <span class="number">100px</span> auto;</span><br><span class="line"><span class="attribute">background-image</span>: <span class="built_in">linear-gradient</span>(black <span class="number">33.3%</span>, red <span class="number">33.3%</span>, red <span class="number">66.6%</span>, yellow <span class="number">66.6%</span>, yellow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程</span></span><br><span class="line"><span class="keyword">let</span> arr = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">20</span>, <span class="number">22</span>, <span class="number">24</span>, <span class="number">26</span>, <span class="number">28</span>, <span class="number">30</span>, <span class="number">32</span>, <span class="number">34</span>, <span class="number">36</span>, <span class="number">38</span>, <span class="number">40</span>, <span class="number">42</span>, <span class="number">44</span>, <span class="number">46</span>, <span class="number">48</span>, <span class="number">50</span>, <span class="number">52</span>, <span class="number">54</span>, <span class="number">56</span>, <span class="number">58</span>, <span class="number">60</span>]</span><br><span class="line"><span class="keyword">let</span> newarr = [];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">let</span> i = <span class="number">0</span>, count = <span class="number">0</span>, sum = <span class="number">0</span>, len = arr.<span class="property">length</span>; i &lt; len; i++) &#123;</span><br><span class="line">sum += arr.<span class="title function_">shift</span>();</span><br><span class="line">count++;</span><br><span class="line"><span class="keyword">if</span> (count % <span class="number">5</span> === <span class="number">0</span>) &#123;</span><br><span class="line">newarr.<span class="title function_">push</span>(sum / <span class="number">5</span>);</span><br><span class="line">sum =  <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(newarr);</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> arr = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">20</span>, <span class="number">22</span>, <span class="number">24</span>, <span class="number">26</span>, <span class="number">28</span>, <span class="number">30</span>, <span class="number">32</span>, <span class="number">34</span>, <span class="number">36</span>, <span class="number">38</span>, <span class="number">40</span>, <span class="number">42</span>, <span class="number">44</span>, <span class="number">46</span>, <span class="number">48</span>, <span class="number">50</span>, <span class="number">52</span>, <span class="number">54</span>, <span class="number">56</span>, <span class="number">58</span>, <span class="number">60</span>]</span><br><span class="line"><span class="keyword">let</span> newarr = [];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">let</span> i = <span class="number">0</span>, len = arr.<span class="property">length</span>; i &lt; len / <span class="number">5</span>; i++) &#123;</span><br><span class="line"><span class="keyword">let</span> subarr = arr.<span class="title function_">splice</span>(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">let</span> j = <span class="number">0</span>, sum = <span class="number">0</span>; j &lt; subarr.<span class="property">length</span>; j++) &#123;</span><br><span class="line">sum += subarr[j];</span><br><span class="line">&#125;</span><br><span class="line">newarr.<span class="title function_">push</span>(sum / <span class="number">5</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(newarr);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line">i = <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span>(i &lt; <span class="number">100</span>):</span><br><span class="line">   j = <span class="number">2</span></span><br><span class="line">   <span class="keyword">while</span>(j &lt;= (i/j)):</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span>(i%j): <span class="keyword">break</span></span><br><span class="line">      j = j + <span class="number">1</span></span><br><span class="line">   <span class="keyword">if</span> (j &gt; i/j) : <span class="built_in">print</span> i, <span class="string">&quot; 是素数&quot;</span></span><br><span class="line">   i = i + <span class="number">1</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;Good bye!&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这是一段</span><br><span class="line">突出显示的</span><br><span class="line">文本内容</span><br></pre></td></tr></table></figure><p><br></p><h4 id="代码块的嵌套">7.2.1 代码块的嵌套</h4><p><br></p><p><strong>格式：</strong></p><ul><li>使用<strong>4</strong>个 <code>`</code> 包裹 <strong>3</strong>个 <code>`</code></li></ul><h5 id="示范-19">示范</h5><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">````txt</span></span><br><span class="line"><span class="code">```js</span></span><br><span class="line"><span class="code">// 3. 输出 100以内(不包括100) 所有偶数的和</span></span><br><span class="line"><span class="code">// 这类求和问题的核心 ： 利用循环  (总和 = 旧数的和 + 新数)</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">let sum = 0;</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">for (let i = 1, sum = 0; i &lt; 100; i++) &#123;</span></span><br><span class="line"><span class="code"> if (i % 2 == 0) &#123;</span></span><br><span class="line"><span class="code"> // 筛选偶数</span></span><br><span class="line"><span class="code"> sum += i; // sum = sum + i // 累加偶数并赋值给sum</span></span><br><span class="line"><span class="code"> // sum为(旧的，已经进入循环的数)的和，i 为新进入循环的数。当加到(最后一个新数i)时，sum就是最后的 总和</span></span><br><span class="line"><span class="code"> &#125;</span></span><br><span class="line"><span class="code">&#125;</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">console.log(sum); // 打印总和</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line"><span class="code">````</span></span><br></pre></td></tr></table></figure><p><br></p><p>如果要再套一层，就在最外层 加 <strong>5</strong>个 <code>`</code> ，以此类推……</p><p><br></p><h3 id="如何在行内代码里显示反引号">7.3 如何在行内代码里显示反引号</h3><p>首尾各用 两个反引号<code>`</code>+ <code>空格</code> 包裹</p><p><strong>格式：</strong></p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">``+空格+带`</span>的内容+空格+``  &lt;!-- 不要忘记前后的两个空格 --&gt;</span><br><span class="line"></span><br><span class="line"><span class="code">`` 这是一段能显示`</span>反引号<span class="code">`的行内代码 `</span>`</span><br></pre></td></tr></table></figure><p><strong>效果：</strong></p><p><code>这是一段能显示`反引号`的行内代码</code></p><p><br><br></p><h2 id="任务列表待办">8. 任务列表（待办）</h2><ul><li><strong>任务列表 的格式：</strong><ul><li><strong><kbd>-</kbd></strong> + <strong><kbd>空格</kbd></strong> +<strong><code>[ ]</code></strong> +<strong><kbd>空格</kbd></strong> + 任务列表内容 ( 中括号<code>[ ]</code> 里面必须有个空格)</li><li>给待办任务列表打 <strong><code>√</code></strong> ，变成 <strong>已办</strong><ol type="1"><li>在渲染界面，直接鼠标左键点击框框</li><li>在源代码界面，在中括号内输入 <strong>英文字母x</strong><ul><li>部分编辑器，在 中括号内 输入<strong>任意字符</strong>都可以打 <strong><code>√</code></strong> ( 例如 <strong>Obsidian</strong> )</li></ul></li></ol></li></ul></li><li><strong>补充：</strong><ul><li>大部分 MD编辑器 支持输入第一个任务列表后，按下 <strong><kbd>Enter</kbd></strong> 进入下一行会 <strong>自动补全待办格式</strong></li><li>在<strong>Obsidian</strong>中，连续输入<strong>两次</strong> <code>Ctrl + Enter</code> ，即可生成一个待办列表<ul><li>再输入一次 <code>Ctrl + Enter</code> ，会在待办列表 打 <strong><code>√</code></strong></li></ul></li></ul></li><li><strong>格式：</strong> <figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> [ ] 待办任务列表1</span><br><span class="line"><span class="bullet">-</span> [ ] 待办任务列表2</span><br><span class="line"><span class="bullet">-</span> [x] 已办任务列表1    &lt;!-- 英文字母X --&gt;</span><br><span class="line"><span class="bullet">-</span> [x] 已办任务列表2</span><br></pre></td></tr></table></figure></li></ul><h3 id="示范-20">示范</h3><ul><li>[ ] 待办任务列表1</li><li>[ ] 待办任务列表2</li><li>[x] 已办任务列表1<br /></li><li>[x] 已办任务列表2</li></ul><p><br></p><ul><li>在 <strong>Obsidian</strong> 中，可以利用 <strong><kbd>Ctrl</kbd></strong> + <strong><kbd>Enter</kbd></strong> ，快速生成任务列表<ol type="1"><li><strong><code>-</code></strong> + <strong><kbd>空格</kbd></strong> + <strong><kbd>Ctrl</kbd></strong> + <strong><kbd>Enter</kbd></strong> +待办文本内容</li><li>待办文本内容 + <strong><kbd>Ctrl</kbd></strong> + <strong><kbd>Enter</kbd></strong> <strong>×2</strong>   ( 输入文本后，连续2次 <code>Ctrl + enter</code> )</li></ol></li></ul><p><br></p><ul><li><strong>任务列表也是可以缩进+退格的，操作跟 无序、有序列表一样</strong></li></ul><h3 id="示范-21">示范</h3><ul><li>[ ] 第一级待办列表1<ul><li>[ ] 第二级待办列表1 另起一行的第二级待办列表1<ul><li>[x] 第三级已办列表1</li><li>[x] 第三级已办列表2</li></ul></li><li>[ ] 第二级待办列表2 另起一行的第二级待办列表2</li></ul></li><li>[ ] 第一级待办列表2</li></ul><p><br> <br></p><h2 id="注释">9. 注释</h2><p><strong>Markdown</strong> 的 <strong>注释</strong> 和 <strong>HMTL</strong> 一样，注释的内容在 <strong>渲染界面</strong> <strong>不可见</strong> （部分编辑器可见)</p><ul><li><strong>注释 的格式：</strong><ul><li><code>&lt;!-- 这里是注释的内容 --&gt;</code><ul><li>注释可以是单行，也可以是多行</li></ul></li><li>如果有在使用 <strong>Obsidian</strong> 的，它的注释格式是不一样的<ul><li><strong><code>%%这是Obsidian的注释内容%%</code></strong></li></ul></li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 这里是一行注释 --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--</span><br><span class="line">这里是</span><br><span class="line">一段</span><br><span class="line">假装有</span><br><span class="line">很多行的</span><br><span class="line">注释</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">%%这是一行Obsidian里的注释%%</span><br><span class="line"></span><br><span class="line">%%</span><br><span class="line">这里是</span><br><span class="line">一段</span><br><span class="line">假装有</span><br><span class="line">很多行的</span><br><span class="line">Obsidian里的</span><br><span class="line">注释</span><br><span class="line">%%</span><br></pre></td></tr></table></figure><h3 id="示范-只有切换至-编辑模式-才能看到喔">示范 <small>(只有切换至 编辑模式 才能看到喔) </small></h3><!-- 这里是一行注释 --><!--这里是一段假装有很多行的注释--><p>%%这是一行Obsidian里的注释%%</p><p>%% 这里是 一段 假装有 很多行的 Obsidian里的 注释 %%</p><p><br><br></p><h2 id="变量">10. 变量</h2><p><br></p><h3 id="网页链接变量">10.1 网页链接变量</h3><ul><li><strong>网页链接变量 的格式：</strong><ol type="1"><li>首先输入<ul><li><strong><code>[显示文本内容]</code></strong> + <strong><code>[变量名]</code></strong><ul><li>变量名可以自己取，没啥限制，任意字符都可以</li></ul></li></ul></li><li>在文档任意一个区域，输入：<ul><li><strong><code>[变量名]</code></strong> + <strong><kbd>:</kbd></strong> + <strong><kbd>空格</kbd></strong> + 链接地址 （这个<strong><kbd>空格</kbd></strong> 不打也没事)</li></ul></li></ol></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">百度一下，你就知道</span>][<span class="symbol">度娘</span>]</span><br><span class="line">[<span class="string">知乎-有问题，就会有答案</span>][<span class="symbol">知乎</span>]</span><br><span class="line"></span><br><span class="line">&lt;!-- 这里是变量区域 --&gt;</span><br><span class="line">[<span class="symbol">度娘</span>]: <span class="link">http://www.baidu.com </span></span><br><span class="line">[<span class="symbol">知乎</span>]: <span class="link">https://www.zhihu.com    </span></span><br></pre></td></tr></table></figure><h4 id="示范-22">示范</h4><p>[百度一下，你就知道][度娘]</p><p>[知乎-有问题，就会有答案][知乎]</p><p><br></p><h3 id="脚注">10.2 脚注</h3><ul><li><strong>脚注 的格式：</strong><ul><li>在需要脚注的地方，输入：<ul><li><strong><code>[^脚注代号]</code></strong> ( 脚注代号会直接显示在渲染界面 )<ul><li>脚注代号可以随便命名，不过推荐使用 <strong>数字序号</strong></li></ul></li></ul></li><li>在其他区域，输入：<ul><li><strong><code>[^脚注代号]</code></strong> + <strong><kbd>:</kbd></strong> + <strong><kbd>空格</kbd></strong> + 脚注内容 （这个 <strong><kbd>空格</kbd></strong> 不打也没事)</li></ul></li></ul></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">鲁迅原名是什么[^1] ，浙江哪里人[^2]</span><br><span class="line"></span><br><span class="line">&lt;!-- 这里是变量区域 --&gt;</span><br><span class="line">[<span class="symbol">^1</span>]: <span class="link">周树人</span></span><br><span class="line">[<span class="symbol">^2</span>]: <span class="link">绍兴人</span></span><br></pre></td></tr></table></figure><h4 id="示范-23">示范</h4><p>鲁迅原名是什么[^1]，浙江哪里人[^2]</p><p><br><br></p><h2 id="拓展文本格式标记">11. 拓展文本格式标记</h2><ul><li><strong>Markdown</strong> 想实现更多的文本显示效果，只能依赖HTML标记实现</li><li>个人<strong>不是很推荐</strong>在 MD 中使用 HTML，不过一些简单的标记还是可以 <strong>轻度使用</strong> 的</li></ul><p><br></p><h3 id="键盘文本">11.1 键盘文本</h3><ul><li><strong>键盘文本的 格式：</strong><ul><li><strong><code>&lt;kbd&gt;键盘文本&lt;/kbd&gt;</code></strong></li><li><strong><code>&lt;kbd&gt;Ctrl&lt;/kbd&gt; + &lt;kbd&gt;X&lt;/kbd&gt;</code></strong></li></ul></li><li><strong>效果：</strong><ul><li><strong><kbd>键盘文本</kbd></strong></li><li><strong><kbd>Ctrl</kbd></strong> + <strong><kbd>X</kbd></strong> ( 剪切 )</li></ul></li><li><p><strong>说明：</strong></p><ul><li>键盘文本也不一定非得是键盘按键，也可以作为<strong>着重文本</strong>突出显示<ul><li><strong>效果：</strong> <kbd>这也算一种着重文本的方式</kbd></li></ul></li></ul></li></ul><h4 id="加粗键盘文本">11.1.1 加粗键盘文本</h4><ul><li><strong>加粗</strong>键盘文本的格式有<strong>两种</strong>：<ul><li><code>&lt;kbd&gt;**键盘文本**&lt;/kbd&gt;</code></li><li><code>**&lt;kbd&gt;ctrl + x&lt;/kbd&gt;**</code></li></ul></li><li><strong>效果：</strong><ol type="1"><li><kbd><strong>键盘文本</strong></kbd></li><li><strong><kbd>ctrl + x</kbd></strong></li></ol></li></ul><p><br></p><h3 id="放大文本">11.2 放大文本</h3><ul><li><strong>放大文本 的格式：</strong><ul><li><strong><code>这是一段普通文本</code></strong> <code>&lt;big&gt;这是一段放大文本&lt;/big&gt;</code></li></ul></li><li><strong>效果：</strong><ul><li>这是一段普通文本 <big>这是一段放大文本</big></li></ul></li></ul><h4 id="放大粗体文本">11.2.1 放大粗体文本</h4><ul><li>放大<strong>加粗</strong>文本的格式有<strong>两种</strong>：<ol type="1"><li><code>**&lt;big&gt;这是一段放大粗体文本&lt;/big&gt;**</code></li><li><code>&lt;big&gt;**这是一段放大粗体文本**&lt;/big&gt;</code></li></ol></li><li><strong>效果：</strong><ol type="1"><li><strong><big>这是一段放大粗体文本</big></strong></li><li><big><strong>这是一段放大粗体文本</strong></big></li></ol></li></ul><p><br></p><h3 id="缩小文本">11.3 缩小文本</h3><ul><li><strong>缩小文本 的格式：</strong><ul><li><strong><code>这是一段普通文本</code></strong> <strong><code>&lt;small&gt;这是一段缩小文本&lt;/small&gt;</code></strong></li></ul></li><li><strong>效果：</strong><ul><li>这是一段普通文本 <small>这是一段缩小文本</small></li></ul></li></ul><h4 id="缩小斜体文本">11.3.1 缩小斜体文本</h4><ul><li><strong>斜体</strong>缩小文本 的格式有<strong>两种</strong>：<ol type="1"><li><code>&lt;small&gt;*这是一段缩小斜体文本*&lt;/small&gt;</code></li><li><code>*&lt;small&gt;这是一段缩小斜体文本&lt;/small&gt;*</code></li></ol></li><li><strong>效果：</strong><ol type="1"><li><small><em>这是一段缩小斜体文本</em></small></li><li><em><small>这是一段缩小斜体文本</small></em></li></ol></li></ul><p><br></p><h3 id="多彩文本">11.4 多彩文本</h3><ul><li><strong>多彩文本 的格式：</strong><ul><li><strong><code>&lt;font color=orange&gt;这是一段橘色文本&lt;/font&gt;</code></strong></li></ul></li><li><strong>效果：</strong><ul><li><font color=orange>这是一段橘色文本</font><ul><li>color 里的颜色支持 英文单词，<strong><a href="http://c.runoob.com/front-end/55" title="可跳转至菜鸟教程了解">16进制</a>，<a href="https://www.runoob.com/cssref/func-rgb-css.html" title="可跳转至菜鸟教程了解">rgb</a>，<a href="https://www.runoob.com/cssref/func-rgba.html" title="可跳转至菜鸟教程了解">rgba</a></strong></li></ul></li></ul></li></ul><p><br></p><h4 id="多彩粗体文本">11.4.1 多彩粗体文本</h4><ul><li>只需要在上面示例的基础上，加上 <strong>加粗标识符</strong>，有<strong>两种</strong>格式：<ol type="1"><li><strong>格式1：</strong> <code>**&lt;font color=teal&gt;这是一段加粗的水鸭色文本&lt;/font&gt;**</code><ul><li><strong>效果：</strong> <strong><font color=teal>这是一段加粗的水鸭色文本</font></strong></li></ul></li><li><strong>格式2：</strong> <code>&lt;font color=teal&gt;**这是一段加粗的水鸭色文本**&lt;/font&gt;</code><ul><li><strong>效果：</strong> <font color=teal><strong>这是一段加粗的水鸭色文本</strong></font></li></ul></li></ol></li><li>若上述混搭方法的样式<strong>失效</strong> ，可以使用 纯HTML标记<ul><li><strong>格式：</strong> <code>&lt;strong style=&quot;color:teal;&quot;&gt;这是一段加粗的水鸭色文本&lt;/strong&gt;</code> (标记略复杂，不是很推荐)</li><li><strong>效果：</strong> <strong style="color:teal;">这是一段加粗的水鸭色文本</strong></li></ul></li></ul><p><br></p><h4 id="多彩斜体文本">11.4.2 多彩斜体文本</h4><ul><li>跟多彩加粗文本完全一样，只需把首尾的 <code>**</code> 换成 <code>*</code> 即可</li></ul><ol type="1"><li><strong>格式1：</strong> <code>*&lt;font color=teal&gt;This is an italic teal text&lt;/font&gt;*</code><ul><li><strong>效果：</strong> <em><font color=teal>This is an italic teal text</font></em></li></ul></li><li><strong>格式2：</strong> <code>&lt;font color=teal&gt;*This is an italic teal text*&lt;/font&gt;</code><ul><li><strong>效果：</strong> <font color=teal><em>This is an italic teal text</em></font></li></ul></li></ol><p><br></p><h4 id="多彩粗斜体文本">11.4.2 多彩粗斜体文本</h4><ul><li>首尾换成 <code>***</code></li></ul><ol type="1"><li><strong>格式1：</strong> <code>***&lt;font color=teal&gt;This is a bold italic teal text&lt;/font&gt;***</code><ul><li><strong>效果：</strong> <strong><em><font color=teal>This is a bold italic teal text</font></em></strong></li></ul></li><li><strong>格式2：</strong> <code>&lt;font color=teal&gt;***This is a bold italic teal text***&lt;/font&gt;</code><ul><li><strong>效果：</strong> <font color=teal><strong><em>This is a bold italic teal text</em></strong></font></li></ul></li></ol><p><br></p><p>#注意 多彩文本尽量慎用，<strong>Markdown</strong> 的核心就是 <strong>简洁精炼</strong>，注重 <strong>实质内容</strong>，而非花哨的 颜色样式</p><p><br><br></p><h2 id="拓展文本显示效果">12. 拓展文本显示效果</h2><ul><li>拓展显示效果既不是原生 <strong>Markdown语法</strong> 支持的，也非 HTML标记，而是部分编辑器 提供的 <strong>额外标识符</strong>，属于拓展语法，旨在为 <strong>Markdown使用者</strong> 提供更多样式选择</li><li>不同编辑器，支持不一样，这里以 <strong>Typora编辑器</strong> 为例</li></ul><p><br></p><h3 id="文本高亮">12.1 文本高亮</h3><ul><li><strong>文本高亮 的格式：</strong><ul><li><strong><code>==这里是一段高亮文本==</code></strong></li></ul></li><li><strong>效果：</strong><ul><li>==这里是一段高亮文本==</li></ul></li></ul><p><br></p><h3 id="上标">12.2 上标</h3><ul><li>用一对 <strong><kbd>^</kbd></strong> 包裹 (<strong><kbd>Shift</kbd>+ <kbd>6</kbd></strong>)<ul><li><strong>格式：</strong> <strong><code>x^2^</code></strong></li><li><strong>效果：</strong> x<sup>2</sup><br /></li></ul></li><li><strong>Obsidian</strong> 没效果的，可以用后面会讲的 <strong>Latex</strong></li><li>或者，也可以使用 <strong>HTML标记</strong><ul><li><code>&lt;sup&gt;这里是上标内容&lt;/sup&gt;</code></li><li><code>X&lt;sup&gt;2&lt;/sup&gt;</code></li></ul></li><li><strong>效果：</strong><ul><li><strong>X<sup>2</sup></strong></li></ul></li></ul><p><br></p><h3 id="下标">12.3 下标</h3><ul><li>用一对 <strong><kbd>~</kbd></strong> 包裹 (<strong><kbd>Shift</kbd> + <kbd><code>&lt;/kbd&gt;**) - **格式：** **</code>H<sub>2</sub>O`</strong><ul><li><strong>效果：</strong> H<sub>2</sub>O</li></ul></li><li><strong>Obsidian</strong> 没效果的，可以用后面会讲的 <strong>Latex</strong></li><li>或者，也可以使用 <strong>HTML标记</strong><ul><li><code>&lt;sub&gt;这里是下标内容&lt;/sub&gt;</code></li><li><code>H&lt;sub&gt;2&lt;/sub&gt;O</code></li></ul></li><li><strong>效果：</strong><ul><li><strong>H<sub>2</sub>O</strong></li></ul></li></ul><p><br></p><h3 id="emoji-符号">12.4 Emoji 符号</h3><p>用一对 <kbd>:</kbd> 包裹，里面是 <strong>Emoji</strong> 符号的 <strong>语义化文本</strong> ( <strong>Typora编辑器</strong> 中，输入 <code>:</code> 就会带提示器 )</p><ul><li><strong>示例：</strong><ul><li><code>:smile:</code> <code>:sweat:</code> <code>:cat:</code> <code>:woman_cartwheeling:</code></li></ul></li><li><strong>效果：</strong><ul><li>:smile: :sweat: :cat: :woman_cartwheeling:</li></ul></li></ul><p><br></p><ul><li><strong>补充：</strong><ul><li>不支持上述方式的 MD编辑器或笔记软件，直接用 <strong>输入法</strong> 输入也是可以的</li><li><strong>Windows系统</strong> 用户 <strong><kbd>win</kbd> + <kbd>.</kbd></strong> 就可以输入 Emoji 了</li><li><strong>Obsidian</strong> 用户可以安装<strong>第三方插件</strong>来支持 <strong>Emoji</strong> 的输入，推荐两个<ol type="1"><li>==Emoji Shortcodes==</li><li>==Emoji Toolbar==</li></ol></li></ul></li></ul><p><br><br></p><h2 id="转义字符">13. 转义字符</h2><ul><li>在 <strong>Markdown</strong> 中，我们 通过 <strong>标识符</strong> 改变 <strong>文本显示效果</strong></li><li>现在我们希望它不作为标识符，而是 <strong>作为字符本身呈现出来</strong> （不具备改变文本显示效果的功能，只是一个<strong>普通字符</strong>)<ul><li>首先我们可以用前面介绍的 <strong>代码域</strong> ，因为代码模式的显示效果就是源代码<strong>完全一致</strong>的</li><li>还有一种方法，可以利用转义字符，在这些标识符 <strong>前面</strong> 加上 <strong>反斜线</strong> <strong><kbd>\</kbd></strong> ( 反斜线要紧贴在标识符前面，<strong>不能</strong> 有 <strong><kbd>空格</kbd></strong> )<ul><li><strong>原理：</strong><ul><li><strong><code>\</code></strong> 的作用是让标识符 <strong>转义</strong> 变为一个<strong>普通字符</strong>，完成这个效果后，反斜线会<strong>自动隐藏</strong></li><li>隐藏后的反斜线仅在<strong>源代码</strong>界面<strong>可见</strong>，在<strong>渲染</strong>界面<strong>不可见</strong></li><li>反斜线只<strong>争对标识符</strong>起作用，其他字符添加 <strong><code>\</code></strong>，<strong><code>\</code></strong> 不会自动隐藏</li></ul></li><li><strong>补充：</strong><ul><li>如果想给已经被加在标识符前面，会自动隐藏的 <strong><code>\</code></strong> 显示出来，可以在反斜线前面再加一个 <strong><kbd>\</kbd></strong> ，用它<strong>自己来转义自己</strong><ul><li><strong>示例：</strong> <strong><code>这里紧跟在标识符前面的反斜线\\*会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体*</code></strong></li><li><strong>效果： </strong> 这里紧跟在标识符前面的 反斜线\<em>会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体</em></li></ul></li></ul></li></ul></li></ul></li></ul><p><br></p><h3 id="例1-以普通字符显示星号">例1 以普通字符显示星号</h3><ul><li>如何让被一对或多对 <strong><code>*</code></strong> 号 包裹的文本内容，能够正常显示 <strong><code>*</code></strong> ，且文本不改变格式<ul><li><code>\*这段文本被一对星号包裹，但不会倾斜\*</code><ul><li><strong>效果：</strong> *这段文本被1对星号包裹，但不会倾斜*</li></ul></li><li><code>\*\*这段文本被2对星号包裹，但不会加粗\*\*</code><ul><li><strong>效果：</strong> **这段文本被2对星号包裹，但不会加粗**</li></ul></li><li><code>\*\*\*这段文本被3对星号包裹，但它既不倾斜也不加粗\*\*\*</code><ul><li><strong>效果：</strong> ***这段文本被3对星号包裹，但它既不倾斜也不加粗***</li></ul></li></ul></li></ul><p><br></p><h3 id="例2-表格内-单元格中的竖杠">例2 表格内 单元格中的竖杠</h3><ul><li>在表格中，使用 <strong><kbd>|</kbd></strong> 作为单元格的内容，但<strong>不会</strong>被识别为<strong>表格的结构</strong>，不会增加额外的单元格</li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">|表头1|表头2|</span><br><span class="line">|-|-|</span><br><span class="line">|这里的文本被\|分隔|这里的文本也被\|分隔|</span><br></pre></td></tr></table></figure><ul><li><strong>效果：</strong></li></ul><table><thead><tr class="header"><th style="text-align: center;">表头1</th><th style="text-align: center;">表头2</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">这里的文本被|分隔</td><td style="text-align: center;">这里的文本也被|分隔</td></tr></tbody></table><p><br></p><p>#补充 该技巧可用于 <strong>Obsidian</strong> 表格内 双链的文本修饰</p><p><strong>文本修饰：</strong></p><p>在 双链<code>[[ ]]</code>内 以 <code>|</code> 引导的内容 - <strong>格式：</strong> <code>[[链接的内容|文本修饰]]</code> - <strong>说明：</strong> 文本修饰是渲染界面实际显示的文本，便于更好地融入语境</p><p><strong>表格内的格式：</strong></p><p>在 <code>|</code> 前面加上 <code>\</code> - <code>[[表格内的链接内容\|文本修饰]]</code></p><p><strong>示例：</strong></p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">|                  表头1                  |                        表头2                        |</span><br><span class="line">|:---------------------------------------:|:---------------------------------------------------:|</span><br><span class="line">| [[#例2 表格内 单元格中的竖杠\|单元格中的竖杠]] | [[#例3 不会变成代码的反引号\|不会变成代码的反引号]] |</span><br></pre></td></tr></table></figure><p><strong>效果：</strong></p><table><thead><tr class="header"><th style="text-align: center;">表头1</th><th style="text-align: center;">表头2</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">[[#例2 表格内 单元格中的竖杠|单元格中的竖杠]]</td><td style="text-align: center;">[[#例3 不会变成代码的反引号|不会变成代码的反引号]]</td></tr></tbody></table><p><br></p><h3 id="例3-不会变成代码的反引号">例3 不会变成代码的反引号</h3><p>使用 转义符号<code>\</code> 让 反引号<code>`</code> 变成普通字符，不再具有[[#7 1 行内代码|行内代码]]的标识符功能</p><p><strong>格式：</strong></p><p><code>\`这段被反引号包裹的内容不会变成行内代码\`</code></p><p><strong>效果：</strong></p><p>`这段被反引号包裹的内容不会变成行内代码`</p><p><br></p><h3 id="例4-链接中的中括号">例4 链接中的中括号</h3><p>在 <strong>网页链接</strong> 的 <strong>显示文本内容</strong> 中，使用 <strong>中括号</strong> <strong><code>[ ]</code></strong></p><ul><li>在显示文本内容中，在其中一个中括号前面，加上<strong>转义符号</strong> 反斜杠 <strong><kbd>\</kbd></strong><ul><li><strong>格式：</strong> <strong><code>[链接里的 \[中括号\] 能被正常显示](https://www.runoob.com)</code></strong></li><li><strong>效果：</strong> <a href="https://www.runoob.com">链接里的 [中括号] 能被正常显示</a></li></ul></li></ul><p><br></p><h3 id="例5-不是列表的连接符横杠">例5 不是列表的连接符(横杠)</h3><ul><li>引用一段话，一般会在换行之后，加上 <strong><code>- 出处</code></strong></li><li>因为 <strong><kbd>-</kbd></strong> 是标识符，会变成一个无序列表</li></ul><p><strong>如下所示：</strong></p><blockquote><p>The Web, the Tree, and the String. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。 - 史蒂芬·平克</p></blockquote><ul><li><strong>解决方法：</strong><ul><li>在 <strong><kbd>-</kbd></strong> 前面加上 转义符号 <strong><kbd>\</kbd></strong></li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;The Web, the Tree, and the String.</span><br><span class="line">&gt;写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。</span><br><span class="line">&gt;\- 史蒂芬·平克   &lt;!-- 加上转义符号 \ , 不会变成无序列表 --&gt;</span><br></pre></td></tr></table></figure></li><li><strong>效果：</strong></li></ul><blockquote><p>The Web, the Tree, and the String. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。 - 史蒂芬·平克</p></blockquote><p><br></p><h3 id="例6-不是标题的">例6 不是标题的 #</h3><p>让 <code>#</code> 不被识别为标题标识符</p><p><strong>格式：</strong></p><p><code>\# 这里的内容不会被识别为标题</code></p><p><strong>效果：</strong></p><p># 这里的内容不会被识别为标题</p><p><br></p><h3 id="例7-不会注释的">例7 不会注释的 %</h3><p>在 <strong>Obsidian</strong> 中 注释是前后各两个 <code>%</code> 号</p><p>使用 转义符号<code>\</code>，让 <code>%%</code> 作为普通字符显示出来，不具备注释的功能</p><ul><li><strong>格式：</strong> <code>\%\%这里的内容可以被显示喔\%\%</code></li><li><strong>效果：</strong> %%这里的内容可以被显示喔%%</li></ul><p><br></p><h3 id="例8-木有链接的双链">例8 木有链接的双链</h3><p><strong>Obsidian</strong> 的双向链格式是2个方括号 <code>[[ ]]</code> (双方)，使用 转义符号<code>\</code>，让 <code>[</code> <code>]</code> 不再具有 双链功能</p><p><strong>格式：</strong></p><p><code>\[\[这段文本被双方包裹，但不是一个双向链\]\]</code></p><p><strong>效果：</strong></p><p>[[这段文本被双方包裹，但不是一个双向链]]</p><p><br></p><h3 id="例9-页链接里-显示文本内的-中括号">例9 页链接里 显示文本内的 中括号</h3><p>使用转义符号<code>\</code>，让<code>中括号</code>可以作为<code>显示文本</code> 在[[#5 1 网页链接|网页链接]]中显示出来</p><p><strong>格式：</strong></p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">\[这是一个带中括号的网页链接显示文本，点击会跳转至百度\</span>]](https://www.baidu.com/)</span><br></pre></td></tr></table></figure><p><strong>效果：</strong></p><p><a href="https://www.baidu.com/">[这是一个带中括号的网页链接显示文本，点击会跳转至百度]</a></p><p><br></p><h3 id="特殊情况-文本修饰的中括号">特殊情况 文本修饰的中括号</h3><p>文本修饰的 中括号<code>[ ]</code> <strong>不需要</strong>使用 转义符号<code>\</code></p><p><strong>示范：</strong></p><p><code>[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]</code></p><p><strong>效果：</strong></p><p>[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]</p><p><br><br></p><h2 id="空格换行强制删除">14. 空格&amp;换行&amp;强制删除</h2><p><br></p><h3 id="空格">14.1 空格</h3><ul><li>在一些编辑器或者支持MD的笔记软件里，无论你打多少个<strong>空格</strong>，它只会显示单个 <strong>空格</strong> 的距离<ul><li>可以使用 HTML中 <strong>空格</strong> 的 <strong>字符实体</strong> —— <strong><code>&amp;nbsp;</code></strong><br /></li><li>若要添加 <strong>多个</strong> 空格，就输入多个 —— <strong><code>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;</code></strong></li></ul></li><li><strong>格式：</strong><ul><li><strong><code>这里有&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;6个空格分隔</code></strong></li></ul></li><li><strong>效果：</strong><ul><li>这里有      6个空格分隔</li></ul></li></ul><p><br></p><h3 id="换行">14.2 换行</h3><p><strong>场景1：</strong> - 在一些编辑器或者支持MD的笔记软件里，无论你打多少个 <strong>回车</strong>，它只会显示单个 <strong>回车</strong> 的空行间距 - 可以使用之前表格里提到的 <strong><code>&lt;br&gt;</code></strong> 标签，在 <strong>单独一行</strong> 中使用，增加额外的空行间距 - 如果要增加 <strong>多个</strong>，就输入 <strong>多个</strong> —— <strong><code>&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;</code></strong> - #注意 当单独一行使用 <code>&lt;br&gt;</code> 标签的时候，如果前后有标题标识符或者列表标识符，确保 <strong>br元素</strong> 前后两行都是空白行</p><p><strong>格式：</strong></p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里是第一段文本</span><br><span class="line"></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;<span class="name">br</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;<span class="name">br</span>&gt;</span></span>     &lt;!-- 这里插入了5个空行间距 --&gt;</span><br><span class="line"></span><br><span class="line">这里是第二段文本</span><br></pre></td></tr></table></figure><p><strong>效果：</strong></p><p>这里是第一段文本</p><p><br><br><br><br><br></p><p>这里是第二段文本</p><p><br><br></p><p><strong>场景2：</strong> - 在列表中也可以插入换行符</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> 这是一段无序列表</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">br</span>&gt;</span></span>     &lt;!-- 插入一个空行间距，需单独一行，上下不用预留空格 --&gt;</span><br><span class="line">  这是同一段无序列表中，空一行距离显示的内容</span><br><span class="line"><span class="bullet">-</span> 这是第二段无序列表</span><br></pre></td></tr></table></figure><p><strong>效果：</strong> - 这里是第一段无序列表 <br> 这里是同一段无序列表中，空一行距离显示的内容 - 这里是第二段无序列表</p><p><br></p><ul><li><strong>补充：</strong><ul><li>有一些MD编辑器或笔记软件，严格遵循MD的换行规则，你敲一个回车是没法换行的，必须在 <strong>行末</strong> 敲 <strong>2个空格</strong>，再按回车键<ul><li><strong>格式：</strong><ul><li>这里是一段想换行的文本<kbd>空格</kbd> <kbd>空格</kbd> <kbd>Enter</kbd> 这是换行后的文本</li></ul></li></ul></li></ul></li></ul><p><br></p><h3 id="强制删除">14.3 强制删除</h3><ul><li>很多编辑器都有英文标点自动补全功能，自动生成一对，光标落在中间 只想删除前面1个，却会把 <strong>一整对</strong> 都删掉</li><li>在多个列表的嵌套中，也许会遇到一些 <strong>无法被删除</strong> 的 <strong>列表标识符</strong></li><li><strong>解决方法：</strong> 使用 <strong><code>Shift</code></strong> + <strong><code>Backspace</code></strong> 即可强制删除<ul><li><strong><kbd>Bcakspace</kbd></strong>   ( 退格键 )</li></ul></li></ul><p><br><br></p><h2 id="嵌入">15. 嵌入</h2><ul><li>嵌入都是依赖 <strong>HTML标签</strong> 实现的，嵌入的都是<strong>在线</strong>链接格式<ul><li>如果是本地的，<strong>Obsidian</strong> 中音频是有自带的可录制的录音机插件的，其他的 <strong>音频、视频</strong> 直接复制黏贴就可以了，也可以直接拖拽到OB的笔记界面<ul><li>其他的媒体文件在 <strong>Obsidian</strong> 也和图片一样，以<strong>双链</strong>的格式引用在目标笔记中，使用 <strong><kbd>!</kbd></strong> 使它可见</li></ul></li></ul></li></ul><p><br></p><h3 id="嵌入音频">15.1 嵌入音频</h3><ul><li><strong>格式：</strong><ul><li><strong><code>&lt;audio controls=&quot;controls&quot; preload=&quot;none&quot; src=&quot;音频链接地址&quot;&gt;&lt;/audio&gt;</code></strong></li></ul></li><li><strong>示例：</strong></li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">audio</span> <span class="attr">controls</span>=<span class="string">&quot;controls&quot;</span> <span class="attr">preload</span>=<span class="string">&quot;none&quot;</span> <span class="attr">src</span>=<span class="string">&quot;https://www.ldoceonline.com/media/english/exaProns/p008-001803372.mp3?version=1.2.37&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">audio</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>效果：</strong></li></ul><audio controls="controls" preload="none" src="https://www.ldoceonline.com/media/english/exaProns/p008-001803372.mp3?version=1.2.41"></audio><p><br></p><h3 id="嵌入视频">15.2 嵌入视频</h3><ul><li><strong>格式：</strong></li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">video</span> <span class="attr">width</span>=<span class="string">&quot;600&quot;</span> <span class="attr">height</span>=<span class="string">&quot;420&quot;</span> <span class="attr">controls</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">source</span> <span class="attr">src</span>=<span class="string">&quot;movie.mp4&quot;</span> <span class="attr">type</span>=<span class="string">&quot;video/mp4&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">source</span> <span class="attr">src</span>=<span class="string">&quot;movie.ogg&quot;</span> <span class="attr">type</span>=<span class="string">&quot;video/ogg&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">source</span> <span class="attr">src</span>=<span class="string">&quot;movie.webm&quot;</span> <span class="attr">type</span>=<span class="string">&quot;video/webm&quot;</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">video</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>说明：</strong><ul><li>width ( 宽度 ) height ( 高度 ) ，可以自己设置，直接输入数字即可，单位默认是 px(像素) 也可以使用 <strong>百分比</strong> <strong><code>width=100%</code></strong> 代表水平撑满整个窗口 <strong><code>height=50%</code></strong> 代表垂直撑满半个窗口</li><li><strong>Video标签</strong> 支持的视频格式 ：MP4 ogg webm</li></ul></li></ul><p><br></p><h3 id="嵌入页面">15.3 嵌入页面</h3><ul><li><strong>格式：</strong> <strong><code>&lt;iframe width=600 height=400 src=&quot;页面链接地址&quot; scrolling=&quot;auto&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;</code></strong></li></ul><p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">iframe</span> <span class="attr">width</span>=<span class="string">600</span> <span class="attr">height</span>=<span class="string">400</span> <span class="attr">src</span>=<span class="string">&quot;https://www.runoob.com/html/html-tutorial.html&quot;</span> <span class="attr">scrolling</span>=<span class="string">&quot;auto&quot;</span> <span class="attr">border</span>=<span class="string">&quot;0&quot;</span> <span class="attr">frameborder</span>=<span class="string">&quot;no&quot;</span> <span class="attr">framespacing</span>=<span class="string">&quot;0&quot;</span> <span class="attr">allowfullscreen</span>=<span class="string">&quot;true&quot;</span>&gt;</span> <span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><br></pre></td></tr></table></figure></p><ul><li><strong>效果：</strong></li></ul><iframe width="600" height="400" src="https://www.runoob.com/html/html-tutorial.html" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe><p><br></p><ul><li><strong>iframe标签</strong> 除了嵌入页面，也可以嵌入<strong>在线视频</strong>，主流的视频网站都会提供<strong>嵌入代码</strong><ul><li>具体可以看这个 <a href="https://www.wolai.com/wolai/go85vJpt3wDwrid7DfCZcE">iframe视频嵌入教程</a></li><li><strong>B站</strong> 的视频，得在 <strong><code>//</code></strong> 前面补充 <strong><code>http:</code></strong></li><li>不是所有的 编辑器和笔记软件 都支持这个</li></ul></li><li><strong>示例：</strong></li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">iframe</span> <span class="attr">width</span>=<span class="string">600</span> <span class="attr">height</span>=<span class="string">400</span> <span class="attr">src</span>=<span class="string">&quot;http://player.bilibili.com/player.html?aid=20190823&amp;bvid=BV1yW411s7og&amp;cid=32964980&amp;page=1&quot;</span> <span class="attr">scrolling</span>=<span class="string">&quot;no&quot;</span> <span class="attr">border</span>=<span class="string">&quot;0&quot;</span> <span class="attr">frameborder</span>=<span class="string">&quot;no&quot;</span> <span class="attr">framespacing</span>=<span class="string">&quot;0&quot;</span> <span class="attr">allowfullscreen</span>=<span class="string">&quot;true&quot;</span>&gt;</span> <span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>宽高设置和前面的 <strong>video</strong> 一样</li></ul><p><br></p><ul><li><strong>效果：</strong></li></ul><iframe width="600" height="400" src="http://player.bilibili.com/player.html?aid=20190823&amp;bvid=BV1yW411s7og&amp;cid=32964980&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe><p><br><br></p><h2 id="latex-数学公式">16. Latex 数学公式</h2><ul><li>主要用于 <strong>数学公式</strong> 与 <strong>化学公式</strong> 的书写</li></ul><p><br></p><h3 id="行内公式">16.1 行内公式</h3><ul><li><p><strong>格式：</strong></p><ul><li><strong><kbd><span class="math inline">\(&lt;/kbd&gt;** + 行内公式 + **&lt;kbd&gt;\)</span></kbd></strong></li></ul></li></ul><p><br></p><ul><li><strong>示例：</strong><ul><li><strong><code>$x^2 + 2x + 5 + \sqrt x = 0$</code></strong></li><li><strong><code>$\ce&#123;CO2 + C -&gt; 2 CO&#125;$</code></strong></li><li><code>$\ce&#123;CO2 + C -&gt; 2 CO&#125;$</code></li><li><code>$\ce&#123;2Mg + O2 -&gt;[燃烧] 2 MgO&#125;$</code></li></ul></li></ul><p><br></p><ul><li><strong>效果：</strong><ul><li><span class="math inline">\(x^2 + 2x + 5 + \sqrt x = 0\)</span></li><li><span class="math inline">\(e^{i\pi} + 1 = 0\)</span></li><li><span class="math inline">\(\ce{CO2 + C -&gt; 2 CO}\)</span></li><li><span class="math inline">\(\ce{2Mg + O2 -&gt;[燃烧] 2 MgO}\)</span></li></ul></li></ul><p><br></p><h3 id="公式块">16.2 公式块</h3><ul><li><strong>格式：</strong><ul><li><strong><code>$$</code></strong> 公式块 <strong><code>$$</code></strong></li></ul></li></ul><p><br></p><ul><li><strong>示例：</strong></li></ul><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 化学公式</span></span><br><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br><span class="line"><span class="keyword">\ce</span>&#123;Zn<span class="built_in">^</span>2+  &lt;=&gt;[+ 2OH-][+ 2H+]  <span class="built_in">$</span><span class="keyword">\underset</span>&#123;<span class="keyword">\text</span>&#123;amphoteres Hydroxid&#125;&#125;&#123;<span class="keyword">\ce</span>&#123;Zn(OH)2 v&#125;&#125;<span class="built_in">$</span>  &lt;=&gt;[+ 2OH-][+ 2H+]  <span class="built_in">$</span><span class="keyword">\underset</span>&#123;<span class="keyword">\text</span>&#123;Hydroxozikat&#125;&#125;&#123;<span class="keyword">\ce</span>&#123;[Zn(OH)4]<span class="built_in">^</span>2-&#125;&#125;<span class="built_in">$</span>&#125;</span><br><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br></pre></td></tr></table></figure><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 麦克斯韦方程组</span></span><br><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;array&#125;&#123;lll&#125;</span><br><span class="line"><span class="keyword">\nabla</span><span class="keyword">\times</span> E <span class="built_in">&amp;</span>=<span class="built_in">&amp;</span> -<span class="keyword">\;</span><span class="keyword">\frac</span>&#123;<span class="keyword">\partial</span>&#123;B&#125;&#125;&#123;<span class="keyword">\partial</span>&#123;t&#125;&#125;   </span><br><span class="line"><span class="keyword">\ </span><span class="keyword">\nabla</span><span class="keyword">\times</span> H <span class="built_in">&amp;</span>=<span class="built_in">&amp;</span> <span class="keyword">\frac</span>&#123;<span class="keyword">\partial</span>&#123;D&#125;&#125;&#123;<span class="keyword">\partial</span>&#123;t&#125;&#125;+J   </span><br><span class="line"><span class="keyword">\ </span><span class="keyword">\nabla</span><span class="keyword">\cdot</span> D <span class="built_in">&amp;</span>=<span class="built_in">&amp;</span> <span class="keyword">\rho</span></span><br><span class="line"><span class="keyword">\ </span><span class="keyword">\nabla</span><span class="keyword">\cdot</span> B <span class="built_in">&amp;</span>=<span class="built_in">&amp;</span> 0</span><br><span class="line"><span class="keyword">\ </span><span class="keyword">\end</span>&#123;array&#125;</span><br><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br></pre></td></tr></table></figure><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 薛定谔方程</span></span><br><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br><span class="line">i<span class="keyword">\hbar</span><span class="keyword">\frac</span>&#123;<span class="keyword">\partial</span> <span class="keyword">\psi</span>&#125;&#123;<span class="keyword">\partial</span> t&#125; = <span class="keyword">\frac</span>&#123;-<span class="keyword">\hbar</span><span class="built_in">^</span>2&#125;&#123;2m&#125; <span class="keyword">\left</span>(<span class="keyword">\frac</span>&#123;<span class="keyword">\partial</span><span class="built_in">^</span>2&#125;&#123;<span class="keyword">\partial</span> x<span class="built_in">^</span>2&#125; + <span class="keyword">\frac</span>&#123;<span class="keyword">\partial</span><span class="built_in">^</span>2&#125;&#123;<span class="keyword">\partial</span> y<span class="built_in">^</span>2&#125;+<span class="keyword">\frac</span>&#123;<span class="keyword">\partial</span><span class="built_in">^</span>2&#125;&#123;<span class="keyword">\partial</span> z<span class="built_in">^</span>2&#125; <span class="keyword">\right</span>) <span class="keyword">\psi</span> + V <span class="keyword">\psi</span></span><br><span class="line"><span class="built_in">$</span><span class="built_in">$</span></span><br></pre></td></tr></table></figure><p><br></p><ul><li><strong>效果：</strong></li></ul><p><span class="math display">\[% 化学公式\ce{Zn^2+  &lt;=&gt;[+ 2OH-][+ 2H+]  $\underset{\text{amphoteres Hydroxid}}{\ce{Zn(OH)2 v}}$  &lt;=&gt;[+ 2OH-][+ 2H+]  $\underset{\text{Hydroxozikat}}{\ce{[Zn(OH)4]^2-}}$}\]</span></p><p><br></p><p><span class="math display">\[% 麦克斯韦方程组\begin{array}{lll}\nabla\times E &amp;=&amp; -\;\frac{\partial{B}}{\partial{t}}   \ \nabla\times H &amp;=&amp; \frac{\partial{D}}{\partial{t}}+J   \ \nabla\cdot D &amp;=&amp; \rho\ \nabla\cdot B &amp;=&amp; 0\ \end{array}\]</span></p><p><br></p><p><span class="math display">\[i\hbar\frac{\partial \psi}{\partial t} = \frac{-\hbar^2}{2m} \left(\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2} \right) \psi + V \psi\]</span></p><ul><li><strong>补充：</strong><ul><li>需要详细教程的，可戳下方链接</li><li><a href="https://www.wolai.com/wolai/egjDbHiAfGfJmwR972fcEW">Latex详细教程</a></li></ul></li></ul><p><br><br></p><h2 id="mermaid">17. Mermaid</h2><ul><li><p>一些 <strong>MD编辑器</strong> 和 <strong>笔记软件</strong> 支持通过 <a href="https://mermaid-js.github.io/" title="Mermaid官网"><strong>Mermaid</strong></a> 及其所提供的 <a href="https://mermaid-js.github.io/mermaid-live-editor" title="Mermaid在线编译器">编译器</a> 来为用户提供图表的绘制功能</p></li><li>这里只提供一些演示的图表，具体教程可戳下方<ul><li>[[MOC Mermiad 教程 Obsidian版| Mermiad 超级教程 Obsidian版]]</li></ul></li></ul><p><br></p><h3 id="流程图">17.1 流程图</h3><p><br></p><p><strong><big>源码1：</big></strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">```mermaid</span><br><span class="line">graph TB</span><br><span class="line">%% s=start  e=end  f=fork  n=normal</span><br><span class="line"></span><br><span class="line">s([开始])--&gt;f1&#123;&#123;if条件&#125;&#125;;</span><br><span class="line"></span><br><span class="line">%% 分支点2</span><br><span class="line">f1--true--&gt;n1[if语句块]--&gt;e([结束]);</span><br><span class="line">f1--false--&gt;f2&#123;&#123;else if条件&#125;&#125;;</span><br><span class="line"></span><br><span class="line">%% 分支点1</span><br><span class="line">f2--true--&gt;n2[else if语句块]--&gt;e;</span><br><span class="line">f2--false--&gt;n3[else语句块]--&gt;e;</span><br><span class="line">```</span><br></pre></td></tr></table></figure><p><strong><big>渲染1：</big></strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">%% s=start  e=end  f=fork  n=normal</span><br><span class="line"></span><br><span class="line">s([开始])--&gt;f1&#123;&#123;if条件&#125;&#125;;</span><br><span class="line"></span><br><span class="line">%% 分支点1</span><br><span class="line">f1--true--&gt;n1[if语句块]--&gt;e([结束]);</span><br><span class="line">f1--false--&gt;f2&#123;&#123;else if条件&#125;&#125;;</span><br><span class="line"></span><br><span class="line">%% 分支点2 </span><br><span class="line">f2--true--&gt;n2[else if语句块]--&gt;e;</span><br><span class="line">f2--false--&gt;n3[else语句块]--&gt;e;</span><br></pre></td></tr></table></figure><p><br></p><p><strong><big>源码2：</big></strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">```mermaid</span><br><span class="line">graph LR</span><br><span class="line">%% s=start  e=end  f= fork n=normal </span><br><span class="line"></span><br><span class="line">%% 虚线</span><br><span class="line">s[朱百六]-.-&gt;|子|n1[朱四九]-.-&gt;|子|n2[朱五四]-.-&gt;|子|f1_帝((朱八八))</span><br><span class="line"></span><br><span class="line">%% 分支点 朱八八</span><br><span class="line">f1_帝--&gt;|长子|f2[朱标]</span><br><span class="line">f1_帝--&gt;|次子|n3[朱樉]</span><br><span class="line">f1_帝--&gt;|三子|n4[朱棢]</span><br><span class="line">f1_帝--&gt;|四子|n5_帝((朱棣))</span><br><span class="line"></span><br><span class="line">%% 分支点 朱标</span><br><span class="line">f2--&gt;|长子|e1[朱雄英]</span><br><span class="line">f2--&gt;|次子|e2_帝((朱允炆))</span><br><span class="line"></span><br><span class="line">n5_帝--&gt;|长子|e3[朱高炽]</span><br><span class="line">```</span><br></pre></td></tr></table></figure><p><strong><big>渲染2：</big></strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">%% s=start  e=end  f= fork n=normal </span><br><span class="line"></span><br><span class="line">%% 虚线</span><br><span class="line">s[朱百六]-.-&gt;|子|n1[朱四九]-.-&gt;|子|n2[朱五四]-.-&gt;|子|f1_帝((朱八八))</span><br><span class="line"></span><br><span class="line">%% 分支点 朱八八</span><br><span class="line">f1_帝--&gt;|长子|f2[朱标]</span><br><span class="line">f1_帝--&gt;|次子|n3[朱樉]</span><br><span class="line">f1_帝--&gt;|三子|n4[朱棢]</span><br><span class="line">f1_帝--&gt;|四子|n5_帝((朱棣))</span><br><span class="line"></span><br><span class="line">%% 分支点 朱标</span><br><span class="line">f2--&gt;|长子|e1[朱雄英]</span><br><span class="line">f2--&gt;|次子|e2_帝((朱允炆))</span><br><span class="line"></span><br><span class="line">n5_帝--&gt;|长子|e3[朱高炽]</span><br></pre></td></tr></table></figure><p><br></p><h3 id="饼图">17.2 饼图</h3><p><br></p><p><strong><big>源码：</big></strong></p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">```mermaid</span></span><br><span class="line"><span class="code">pie</span></span><br><span class="line"><span class="code">    title 为什么总是宅在家里？</span></span><br><span class="line"><span class="code">    &quot;喜欢宅&quot; : 45</span></span><br><span class="line"><span class="code">    &quot;天气太热&quot; : 70</span></span><br><span class="line"><span class="code">    &quot;穷&quot; : 500</span></span><br><span class="line"><span class="code">&quot;关你屁事&quot; : 95</span></span><br><span class="line"><span class="code">```</span></span><br></pre></td></tr></table></figure><p><strong><big>渲染：</big></strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pie</span><br><span class="line">    title 为什么总是宅在家里？</span><br><span class="line">    &quot;喜欢宅&quot; : 45</span><br><span class="line">    &quot;天气太热&quot; : 70</span><br><span class="line">    &quot;穷&quot; : 500</span><br><span class="line">&quot;关你屁事&quot; : 95</span><br></pre></td></tr></table></figure><p><br></p><h3 id="序列图-时序图">17.3 序列图 (时序图)</h3><p><br></p><p><strong><big>源码：</big></strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">```mermaid</span><br><span class="line">sequenceDiagram</span><br><span class="line">%% 自动编号</span><br><span class="line">autonumber</span><br><span class="line">%% 定义参与者并取别名，aliases：别名</span><br><span class="line">        participant A as Aly</span><br><span class="line">        participant B as Bob</span><br><span class="line">        participant C as CofCai</span><br><span class="line">        %% 便签说明</span><br><span class="line">        Note left of A: 只复习了一部分</span><br><span class="line">        Note right of B: 没复习</span><br><span class="line">        Note over A,B: are contacting</span><br><span class="line">        </span><br><span class="line">        A-&gt;&gt;B: 明天是要考试吗？</span><br><span class="line">        B--&gt;&gt;A: 好像是的！</span><br><span class="line">        </span><br><span class="line">        %% 显示并行发生的动作，parallel：平行</span><br><span class="line">        %% par [action1]</span><br><span class="line">        rect rgb(0, 25, 155)</span><br><span class="line">            par askA</span><br><span class="line">                C --&gt;&gt; A:你复习好了吗？</span><br><span class="line">            and askB</span><br><span class="line">                C --&gt;&gt; B:你复习好了吗？</span><br><span class="line">            and self</span><br><span class="line">                C -&gt;&gt;C:我还没准备复习......</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        %% 背景高亮，提供一个有颜色的背景矩形</span><br><span class="line">        rect rgb(25, 55, 0)</span><br><span class="line">            loop 自问/Every min</span><br><span class="line">            %% &lt;br/&gt;可以换行</span><br><span class="line">            C -&gt;&gt; C:我什么时候&lt;br/&gt;开始复习呢？</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        %% 可选择路径</span><br><span class="line">        rect rgb(153, 83, 60)</span><br><span class="line">            alt is good</span><br><span class="line">                A -&gt;&gt; C:复习了一点</span><br><span class="line">            else is common</span><br><span class="line">                B -&gt;&gt; C:我也是</span><br><span class="line">            end</span><br><span class="line">            %% 没有else时可以提供默认的opt</span><br><span class="line">            opt Extra response</span><br><span class="line">                C -&gt;&gt; C:你们怎么不回答我</span><br><span class="line">            end</span><br><span class="line">        endsequenceDiagram</span><br><span class="line">%% 自动编号</span><br><span class="line">autonumber</span><br><span class="line">%% 定义参与者并取别名，aliases：别名</span><br><span class="line">        participant A as Aly</span><br><span class="line">        participant B as Bob</span><br><span class="line">        participant C as CofCai</span><br><span class="line">        %% 便签说明</span><br><span class="line">        Note left of A: 只复习了一部分</span><br><span class="line">        Note right of B: 没复习</span><br><span class="line">        Note over A,B: are contacting</span><br><span class="line">        </span><br><span class="line">        A-&gt;&gt;B: 明天是要考试吗？</span><br><span class="line">        B--&gt;&gt;A: 好像是的！</span><br><span class="line">        </span><br><span class="line">        %% 显示并行发生的动作，parallel：平行</span><br><span class="line">        %% par [action1]</span><br><span class="line">        rect rgb(0, 25, 155)</span><br><span class="line">            par askA</span><br><span class="line">                C --&gt;&gt; A:你复习好了吗？</span><br><span class="line">            and askB</span><br><span class="line">                C --&gt;&gt; B:你复习好了吗？</span><br><span class="line">            and self</span><br><span class="line">                C -&gt;&gt;C:我还没准备复习......</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        %% 背景高亮，提供一个有颜色的背景矩形</span><br><span class="line">        rect rgb(25, 55, 0)</span><br><span class="line">            loop 自问/Every min</span><br><span class="line">            %% &lt;br/&gt;可以换行</span><br><span class="line">            C -&gt;&gt; C:我什么时候&lt;br/&gt;开始复习呢？</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        %% 可选择路径</span><br><span class="line">        rect rgb(153, 83, 60)</span><br><span class="line">            alt is good</span><br><span class="line">                A -&gt;&gt; C:复习了一点</span><br><span class="line">            else is common</span><br><span class="line">                B -&gt;&gt; C:我也是</span><br><span class="line">            end</span><br><span class="line">            %% 没有else时可以提供默认的opt</span><br><span class="line">            opt Extra response</span><br><span class="line">                C -&gt;&gt; C:你们怎么不回答我</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">```</span><br></pre></td></tr></table></figure><p><strong><big>渲染：</big></strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">%% 自动编号</span><br><span class="line">autonumber</span><br><span class="line">%% 定义参与者并取别名，aliases：别名</span><br><span class="line">        participant A as Aly</span><br><span class="line">        participant B as Bob</span><br><span class="line">        participant C as CofCai</span><br><span class="line">        %% 便签说明</span><br><span class="line">        Note left of A: 只复习了一部分</span><br><span class="line">        Note right of B: 没复习</span><br><span class="line">        Note over A,B: are contacting</span><br><span class="line">        </span><br><span class="line">        A-&gt;&gt;B: 明天是要考试吗？</span><br><span class="line">        B--&gt;&gt;A: 好像是的！</span><br><span class="line">        </span><br><span class="line">        %% 显示并行发生的动作，parallel：平行</span><br><span class="line">        %% par [action1]</span><br><span class="line">        rect rgb(0, 25, 155)</span><br><span class="line">            par askA</span><br><span class="line">                C --&gt;&gt; A:你复习好了吗？</span><br><span class="line">            and askB</span><br><span class="line">                C --&gt;&gt; B:你复习好了吗？</span><br><span class="line">            and self</span><br><span class="line">                C -&gt;&gt;C:我还没准备复习......</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        %% 背景高亮，提供一个有颜色的背景矩形</span><br><span class="line">        rect rgb(25, 55, 0)</span><br><span class="line">            loop 自问/Every min</span><br><span class="line">            %% &lt;br/&gt;可以换行</span><br><span class="line">            C -&gt;&gt; C:我什么时候&lt;br/&gt;开始复习呢？</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">        %% 可选择路径</span><br><span class="line">        rect rgb(153, 83, 60)</span><br><span class="line">            alt is good</span><br><span class="line">                A -&gt;&gt; C:复习了一点</span><br><span class="line">            else is common</span><br><span class="line">                B -&gt;&gt; C:我也是</span><br><span class="line">            end</span><br><span class="line">            %% 没有else时可以提供默认的opt</span><br><span class="line">            opt Extra response</span><br><span class="line">                C -&gt;&gt; C:你们怎么不回答我</span><br><span class="line">            end</span><br><span class="line">        end</span><br></pre></td></tr></table></figure><p><br></p><h3 id="甘特图">17.4 甘特图</h3><p><br></p><p><strong><big>源码：</big></strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">```mermaid</span><br><span class="line">gantt</span><br><span class="line">    title A Gantt Diagram</span><br><span class="line">    dateFormat  YYYY-MM-DD</span><br><span class="line">    section Section</span><br><span class="line">    A task           :a1, 2014-01-01, 30d</span><br><span class="line">    Another task     :after a1  , 20d</span><br><span class="line">    section Another</span><br><span class="line">    Task in sec      :2014-01-12  , 12d</span><br><span class="line">    another task      : 24d</span><br><span class="line">```</span><br></pre></td></tr></table></figure><p><strong><big>渲染：</big></strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">gantt</span><br><span class="line">    title A Gantt Diagram</span><br><span class="line">    dateFormat  YYYY-MM-DD</span><br><span class="line">    section Section</span><br><span class="line">    A task           :a1, 2014-01-01, 30d</span><br><span class="line">    Another task     :after a1  , 20d</span><br><span class="line">    section Another</span><br><span class="line">    Task in sec      :2014-01-12  , 12d</span><br><span class="line">    another task      : 24d</span><br></pre></td></tr></table></figure><h3 id="类图">17.5 类图</h3><p><br></p><p><strong><big>源码：</big></strong></p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">```mermaid</span></span><br><span class="line"><span class="code">classDiagram</span></span><br><span class="line"><span class="code">    Animal &lt;|-- Duck</span></span><br><span class="line"><span class="code">    Animal &lt;|-- Fish</span></span><br><span class="line"><span class="code">    Animal &lt;|-- Zebra</span></span><br><span class="line"><span class="code">    Animal : +int age</span></span><br><span class="line"><span class="code">    Animal : +String gender</span></span><br><span class="line"><span class="code">    Animal: +isMammal()</span></span><br><span class="line"><span class="code">    Animal: +mate()</span></span><br><span class="line"><span class="code">    class Duck&#123;</span></span><br><span class="line"><span class="code">      +String beakColor</span></span><br><span class="line"><span class="code">      +swim()</span></span><br><span class="line"><span class="code">      +quack()</span></span><br><span class="line"><span class="code">    &#125;</span></span><br><span class="line"><span class="code">    class Fish&#123;</span></span><br><span class="line"><span class="code">      -int sizeInFeet</span></span><br><span class="line"><span class="code">      -canEat()</span></span><br><span class="line"><span class="code">    &#125;</span></span><br><span class="line"><span class="code">    class Zebra&#123;</span></span><br><span class="line"><span class="code">      +bool is_wild</span></span><br><span class="line"><span class="code">      +run()</span></span><br><span class="line"><span class="code">    &#125;</span></span><br><span class="line"><span class="code">```</span></span><br></pre></td></tr></table></figure><p><strong><big>渲染：</big></strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">classDiagram</span><br><span class="line">    Animal &lt;|-- Duck</span><br><span class="line">    Animal &lt;|-- Fish</span><br><span class="line">    Animal &lt;|-- Zebra</span><br><span class="line">    Animal : +int age</span><br><span class="line">    Animal : +String gender</span><br><span class="line">    Animal: +isMammal()</span><br><span class="line">    Animal: +mate()</span><br><span class="line">    class Duck&#123;</span><br><span class="line">      +String beakColor</span><br><span class="line">      +swim()</span><br><span class="line">      +quack()</span><br><span class="line">    &#125;</span><br><span class="line">    class Fish&#123;</span><br><span class="line">      -int sizeInFeet</span><br><span class="line">      -canEat()</span><br><span class="line">    &#125;</span><br><span class="line">    class Zebra&#123;</span><br><span class="line">      +bool is_wild</span><br><span class="line">      +run()</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p><br><br></p><h2 id="标签-tag">18. 标签 (Tag)</h2><ul><li>标签是 <strong>Obsidian</strong> 特有的一个功能，标签可以通过点击唤起快速搜索 (搜索包含该标签的所有笔记)</li></ul><p><strong>格式：</strong> - <strong>#</strong> + <strong>标签名</strong> - <code>#标签名</code></p><h3 id="关于空格">关于空格</h3><ul><li>在一段正文文本的后面添加 Tag， <kbd><strong>#</strong></kbd> 的<strong>前面</strong> 需要有个空格<ul><li><kbd><strong>空格</strong></kbd> + <strong>#</strong> + 标签名</li></ul></li></ul><p><br></p><ul><li># 与 标签名 <strong>之间</strong>，<strong>不能</strong>有空格，否则就变成 一级标题 了</li></ul><p><br></p><ul><li>标签名的<strong>内部</strong>，<strong>不允许</strong>使用空格，若想区分标签中的词语，可使用以下<strong>三种</strong>方法：<ol type="1"><li>驼峰式大小写： <code>#BlueTopaz</code></li><li>下划线： <code>#blue_topaz</code></li><li>连字符： <code>#blue-topaz</code></li></ol></li></ul><p><br></p><h3 id="关于数字">关于数字</h3><ul><li>标签内允许使用数字，但不能完全由数字组成<ul><li><code>#1984</code> ❌</li><li><code>#1984Date</code> ⭕</li><li><code>#da_1984_te</code> ⭕</li><li><code>#date-1984</code> ⭕</li></ul></li></ul><p><br></p><h3 id="标签的嵌套">标签的嵌套</h3><p>在标签名内，使用 <code>/</code> 斜杠 可以实现标签的嵌套</p><p><strong>格式：</strong> - <code>#主标签/子标签1</code> - <code>#主标签/子标签2</code> - <code>#主标签/子标签3</code></p><p>嵌套标签可以像普通标签一样通过点击来唤起搜索，嵌套标签允许你选择搜索的层次。<strong>例如：</strong> - 搜索 <code>#主标签</code> ，即可找到包含任意一个子标签的所有笔记 - 返回的结果会是上述的三个例子 - 当你在一个主分类下设置了多个子分类，想找到这个主分类包含的所有内容时，该功能会很实用</p><p><br></p><h3 id="能被使用的符号">能被使用的符号</h3><p>综上所述，标签内能被使用的符号共有三种</p><ol type="1"><li><code>_</code> 下划线</li><li><code>-</code> 连字符</li><li><code>/</code> 斜杠</li></ol><p><br></p><h3 id="如何让-不被识别">如何让 # 不被识别</h3><p>可以使用前面提到的转义符号 <code>\</code> 反斜杠，与上述的 转义标题 类似</p><p><strong>格式：</strong></p><p><code>\#这里的内容不会被识别为标签</code></p><p><strong>效果：</strong></p><p>#这里的内容不会被识别为标签</p><p><br></p><h2 id="避免标识符的滥用">19. 避免标识符的滥用</h2><p>即使在 <strong>Markdown</strong> 中，也要尽量<strong>避免</strong>标识符的滥用</p><p>比如我的这篇教程，就存在一定程度的滥用 - 其实是因为我这篇是教学性质的，不太一样，有些不能避免 - <strong>(好吧，我就是在甩锅)</strong></p><p>标识符的本质是突出显示，代表<strong>重点</strong> - 一篇笔记里的某段文本，使用<strong>各式各样的</strong>的标识符，会造成<strong>重点不清晰</strong></p><p>有<strong>三种</strong>标识，<strong>慎用</strong>！<br />1. 词中对<strong>单个汉字</strong>的标识 1. 卧==虎==藏==龙== 2. 短语中对<strong>单个英语单词</strong>的标识 1. get a ==bang== out of 3. 标识符的<strong>多层嵌套</strong> 1. <strong>我感觉快要==原地<code>起飞</code>==了</strong></p><p><strong>原因：</strong> - 词义的割裂 - 视觉的混乱 - 不利于搜索 - <code>卧==虎==藏==龙==</code> - 搜 <code>卧虎</code> -- 搜不到 - 搜 <code>藏龙</code> -- 搜不到</p>]]></content>
      
      
      
        <tags>
            
            <tag> 学习 </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.3.谓词演算的等价式与蕴涵式</title>
      <link href="/2023/01/13/cmath_8/"/>
      <url>/2023/01/13/cmath_8/</url>
      
        <content type="html"><![CDATA[<h1 id="谓词演算的等价式与蕴涵式">3.3 谓词演算的等价式与蕴涵式</h1><h5 id="定义3.7谓词公式在共同论域等价">【定义3.7】谓词公式在共同论域等价</h5><pre><code>给定任何两个谓词公式WffA和WffB，设它们有共同的论域E，若对A和B的任一组个体变元进行赋值，所得命题的真值相同，则称谓词公式A和B在E上是等价的，并记作A&lt;=&gt;B</code></pre><h5 id="定义3.8谓词公式有效无效可满足">【定义3.8】谓词公式有效、无效、可满足</h5><pre><code>给定任意谓词公式WffA，其论域为E，対于A的所有赋值WffA都为真，则称WffA在E上是有效的（或永真的）。如果在所有赋值下WffA都为假，则称WffA为不可满足的。如果至少在一种赋值下为真，则称该WffA为可满足的。</code></pre><h5 id="定理3.1量词与否定联结词之间有如下关系">【定理3.1】量词与否定联结词之间有如下关系</h5><ul><li>¬∀xQ(x) &lt;=&gt; ∃x¬Q(x)</li><li>¬∃xQ(x) &lt;=&gt; ∀x¬Q(x)</li></ul><h5 id="表3.1-谓词等值式与蕴涵式">表3.1 谓词等值式与蕴涵式</h5><p>¬、∧、∨、→、↔</p><table><thead><tr class="header"><th style="text-align: center;">E<sub>23</sub></th><th style="text-align: center;">∃x(A(x) ∨ B(x)) &lt;=&gt; ∃xA(x) ∨ ∃xB(x)</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">E<sub>24</sub></td><td style="text-align: center;">∀x(A(x) ∧ B(x)) &lt;=&gt; ∀xA(x) ∧ ∀xB(x)</td></tr><tr class="even"><td style="text-align: center;">E<sub>25</sub></td><td style="text-align: center;">¬∃xA(x) &lt;=&gt; ∀x¬A(x)</td></tr><tr class="odd"><td style="text-align: center;">E<sub>26</sub></td><td style="text-align: center;">¬∀xA(x) &lt;=&gt; ∃x¬A(x)</td></tr><tr class="even"><td style="text-align: center;">E<sub>27</sub></td><td style="text-align: center;">∀x(A∨ B(x)) &lt;=&gt; A∨∀xB(x)</td></tr><tr class="odd"><td style="text-align: center;">E<sub>28</sub></td><td style="text-align: center;">∃x(A∧ B(x)) &lt;=&gt; A∧∃xB(x)</td></tr><tr class="even"><td style="text-align: center;">E<sub>29</sub></td><td style="text-align: center;">∃x(A(x) → B(x)) &lt;=&gt; ∀xA(x)→∃xB(x)</td></tr><tr class="odd"><td style="text-align: center;">E<sub>30</sub></td><td style="text-align: center;">∀xA(x) →B &lt;=&gt; ∃x(A(x) →B)</td></tr><tr class="even"><td style="text-align: center;">E<sub>31</sub></td><td style="text-align: center;">∃xA(x) →B &lt;=&gt; ∀x(A(x) →B)</td></tr><tr class="odd"><td style="text-align: center;">E<sub>32</sub></td><td style="text-align: center;">A→∀xB(x) &lt;=&gt; ∀x(A →B(x))</td></tr><tr class="even"><td style="text-align: center;">E<sub>33</sub></td><td style="text-align: center;">A→∃xB(x) &lt;=&gt; ∃x(A →B(x))</td></tr><tr class="odd"><td style="text-align: center;">I<sub>17</sub></td><td style="text-align: center;">∀xA(x) ∨ ∀xB(x) =&gt; ∀x(A(x) ∨ B(x))</td></tr><tr class="even"><td style="text-align: center;">I<sub>18</sub></td><td style="text-align: center;">∃x(A(x) ∧ B(x)) =&gt; ∃xA(x) ∧ ∃xB(x)</td></tr><tr class="odd"><td style="text-align: center;">I<sub>19</sub></td><td style="text-align: center;">∃xA(x)→∀xB(x) =&gt; ∀x(A(x) → B(x))</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 离散数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离散数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.2.量词与合式公式</title>
      <link href="/2023/01/13/cmath_7/"/>
      <url>/2023/01/13/cmath_7/</url>
      
        <content type="html"><![CDATA[<h1 id="量词与合式公式"><strong>3.2 量词与合式公式</strong></h1><blockquote><p>​ <strong>命题函数中表示数量的词称为量词，可以使用量词来表示个体常项与变项之间的数量关系，即对命题函数进行量化。量词分为两种，全称量词与存在量词。</strong></p></blockquote><h5 id="定义3.2全程量词"><strong>【定义3.2】全程量词</strong></h5><pre><code>P(x)的全称量化是命题 “P(x)对x在其论域的所有值为真” 。符号∀xP(x)表示P(x)的全称量化，其中 ∀ 称为全称量词。</code></pre><p>​ ∀xP(x)也可表示为“ 对所有x，P(x)”。日常语言中的“一切”、“任意”、“所有”、“凡是” 都可对应全称量词。</p><h5 id="定义3.3存在量词"><strong>【定义3.3】</strong>存在量词</h5><pre><code>P(x)的存在量化是命题 “论域中存在一个元素x使P(x)为真”。符号∃xP(x)表示P(x)的存在量化，其中 ∃ 称为存在量词。</code></pre><p>​ ∃xP(x)也可表示为“ 有一个x使得P(x) ” 或是 “ 至少有一个x使得P(x) ”。日常语言中的 “ 存在着 ”、“ 有一个 ”、“ 至少有一个 ” 都可对应存在量词。</p><ul><li><p><strong>特性谓词</strong></p><p>​ 在全程量词中，特性谓词是条件式的前件，在存在量词中特性谓词后跟一个合取项。</p><p>​ 一般的，在全总个体域中才产生特性谓词，如果事先规定了个体域，则可免去特性谓词。一般约定：如果事先没有明确提出个体域，则认为个体域是全总个体域。</p></li></ul><h5 id="定义3.4复合谓词公式"><strong>【定义3.4】复合谓词公式</strong></h5><pre><code>有一个或多个原子谓词以及逻辑联结词组合而成的表达式称为复合谓词公式。</code></pre><p>​ 由一个谓词及若干个体变量组成的表达式称为原子谓词公式。原子谓词公式可以组成复合谓词公式。</p><p>​ 复合谓词公式中使用的逻辑联结词包括：¬、∧、∨、→、↔，意义与命题演算中的解释相似。</p><h5 id="定义3.5-谓词演算的合式公式递归的定义如下"><strong>【定义3.5】 谓词演算的合式公式,递归的定义如下：</strong></h5><ul><li><strong>原子谓词公式是合式公式。</strong></li><li><strong>若A是合式公式，则（¬A）也是合式公式。</strong></li><li><strong>若A和B都是合适公式，则（A∧B）、（A∨B）、（A→B）、（A↔B）也是合式公式。</strong></li><li><strong>若A是合式公式，x是A中出现的任何个体变元，则 (∀x)A 和 (∃x)A 也是合式公式。</strong></li><li><strong>只有经过有限次应用上述所得到的符号串才是合式公式。</strong></li></ul><h5 id="定义3.6指导变元辖域约束出现自由出现">【定义3.6】指导变元、辖域、约束出现、自由出现</h5><pre><code>    给定谓词合式公式A，其中一部分公式形式为∀xB(x)或∃xB(x)，称量词、 后面的x为指导变元，也称为作用变元。称B(x) 为相应量词的辖域（或作用域）。在辖域中，x的一切出现称为约束出现。在B(x) 中除去约束出现的其他变元的出现称为自由出现。</code></pre><ul><li><p>例子</p><p>∀x(P(x,y)→R(x,z)）</p><p>​ 指导变元为x，∀x的辖域是(P(x,y)→R(x,z))，x为约束出现，y和z是自由出现。</p></li><li><p>例子2</p><p>∀x(F(x)→G(x))→∃y(H(x)∧L(x,y,z))</p><p>​ 该公式含有两个量词，前件中的量词∀的指导变元为x，其辖域为(F(x)→G(x))，其中x是约束出现，y是自由出现。后件中的量词∃的指导变元为y，其辖域为(H(x)∧L(x,y,z))， 其中y是约束出现，x和z为自由出现。</p></li></ul><h5 id="改变命题公式写法规则">改变命题公式写法规则</h5><ul><li><p>约束变元改名规则</p><p>​ 将量词辖域中，量词的指导变元及其辖域中该变元的所有约束出现均改为本辖域中未曾出现过的个体变元，其余不变。</p></li><li><p>自由变元带入规则</p><p>​ 把公式中的某一自由变元，用该公式中没有出现的个体变元符号替代，且要替换该自由变元在公式中的所有出现处。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 离散数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离散数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.1.谓词的概念与表示</title>
      <link href="/2023/01/13/cmath_6/"/>
      <url>/2023/01/13/cmath_6/</url>
      
        <content type="html"><![CDATA[<h1 id="谓词的概念与表示">3.1 谓词的概念与表示</h1><blockquote><p>命题中，个体词（主语）是指所研究对象中可以独立存在的具体的或抽象的客体，既可以使用变量，也可以带入特定的值。</p></blockquote><ul><li><p>个体常项</p><p>​ 表示具体的或特定的客体时称为个体常项，多用小写英文字母a，b，c，... 等表示</p></li><li><p>个体变项</p><p>​ 表示抽象或泛指的个体时称为个体变项,也称个体变元或个体变量，多用小写英文字母x，y，z，... 表示</p></li><li><p>个体域（论域）</p><p>​ 个体变项的取值范围为个体域，或称论域。</p></li></ul><blockquote><p>谓词用来指明个体的性质或个体之间的关系等， 常用大写英文字母P，Q，R，... 来表示。</p></blockquote><ul><li><p>谓词常项</p><p>​ 表示具体性质或关系的谓词称为谓词常项。</p></li><li><p>谓词变项</p><p>​ 表示抽象的或泛指的性质或关系的谓词称为谓词变项。</p></li></ul><h5 id="定义3.1-由一个谓词一些个体变量组成的表达式称为谓词变项或命题函数">【定义3.1】 由一个谓词、一些个体变量组成的表达式称为谓词变项或命题函数。</h5><p>例： 小张是名大学生</p><p>解： 设小张表示为a， A(x)：x是名大学生</p><p>​ 题目所代表的命题为A(a)</p>]]></content>
      
      
      <categories>
          
          <category> 离散数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离散数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.3.自然推理系统</title>
      <link href="/2023/01/13/cmath_5/"/>
      <url>/2023/01/13/cmath_5/</url>
      
        <content type="html"><![CDATA[<h1 id="自然推理系统">2.3 自然推理系统</h1><h3 id="推理">2.3.1 推理</h3><blockquote><p>数理逻辑的主要任务是用逻辑的方法研究数学中的推理。</p></blockquote><blockquote><p>所谓推理是指从前提出发，应用推理规则推出结论的思维过程</p></blockquote><blockquote><p>任何一个推理都由前提和结论两部分组成。前提就是推理所根据的已知命题，结论则是从前提出发通过推理而得到的新命题。</p></blockquote><blockquote><p>数理逻辑从形式结构上来研究推理的有效性</p></blockquote><h5 id="定义-2.8">【定义 2.8】</h5><blockquote><p>设H<sub>1</sub>，H<sub>2</sub>，...，H<sub>n</sub>和C都是命题公式，若H<sub>1</sub>∧H<sub>2</sub>∧...∧H<sub>n</sub>=&gt;C，则称C为一组前提H<sub>1</sub>，H<sub>2</sub>，...，H<sub>n</sub>的有效结论或逻辑结论。也称由前提H<sub>1</sub>，H<sub>2</sub>，...，H<sub>n</sub>推出结论C的推理是有效的或正确的。</p></blockquote><blockquote><p>H<sub>1</sub>∧H<sub>2</sub>∧...∧H<sub>n</sub>=&gt;C，亦可记为H<sub>1</sub>∧H<sub>2</sub>∧...∧H<sub>n</sub>├ C</p></blockquote><h5 id="证明">2.3.1.1 证明</h5><blockquote><p>要证明C为一组前提H<sub>1</sub>，H<sub>2</sub>，...，H<sub>n</sub>的有效结论，只需证明H<sub>1</sub>∧H<sub>2</sub>∧...∧H<sub>n</sub>→C为重言式。</p></blockquote><p>​方法：</p><blockquote><ol type="1"><li>真值表法</li><li>等值演算法</li><li>主析（合）取范式</li><li>推理法</li></ol></blockquote><h5 id="推理法">2.3.1.2 推理法</h5><p>根据已知的等值公式和蕴涵式，利用推理规则证明。</p><ul><li><p>直接推理</p><ul><li><p>推理规则</p><ol type="1"><li><p>p规则（前提引入规则）</p><p>​ 前提在推导过程中的任何时候都可以引入使用。</p></li><li><p>T规则（结论引入规则）</p><p>​ 推理中，如果一个或多个公式，蕴含了公式S，则公式S可以引入到以后的推理之中。</p></li><li><p>转换规则</p><p>​ 在推导过程的任何步骤上，命题公式中的子公式都可以用与之等价的公式置换，此规则也可以记为T规则。</p></li></ol></li><li><p>常用的推理定律</p><table><thead><tr class="header"><th style="text-align: center;">常用的推理定律</th><th style="text-align: center;"></th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">附加律</td><td style="text-align: center;">A├(A∨B)</td></tr><tr class="even"><td style="text-align: center;">化简律</td><td style="text-align: center;">(A∧B)├A 或 (A∧B)├B</td></tr><tr class="odd"><td style="text-align: center;">假言推理*</td><td style="text-align: center;">((A→B)∧A)├B</td></tr><tr class="even"><td style="text-align: center;">拒取式*</td><td style="text-align: center;">((A→B)∧¬B)├¬A</td></tr><tr class="odd"><td style="text-align: center;">条件（假言）三段论*</td><td style="text-align: center;">((A→B)∧(B→C))├(A→C)</td></tr><tr class="even"><td style="text-align: center;">析取三段论</td><td style="text-align: center;">((A∨B)∧¬B)├A 或 ((A∨B)∧¬A)├B</td></tr><tr class="odd"><td style="text-align: center;">合取引入规则</td><td style="text-align: center;">A，B├(A∨B)</td></tr></tbody></table></li></ul></li><li><p>间接推理</p><ul><li><p>归谬法（反证法）</p><p>​ 结论的否定作为新的规则引入， 寻找矛盾</p></li><li><p>CP规则</p><blockquote><p>将结论中的R作为条件引入</p></blockquote><p>若H<sub>1</sub>，H<sub>2</sub>，... ，H<sub>n</sub>，R├ C，则H<sub>1</sub>，H<sub>2</sub>，... ，H<sub>n</sub>├ R → C</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 离散数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离散数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.2.主范式</title>
      <link href="/2023/01/13/cmath_4/"/>
      <url>/2023/01/13/cmath_4/</url>
      
        <content type="html"><![CDATA[<h1 id="主范式">2.2 主范式</h1><h3 id="主析取范式">2.2.1 主析取范式</h3><h5 id="定义-2.6主析取范式">【定义 2.6】主析取范式</h5><blockquote><p>对于给定的命题公式，如果有一个它的等价公式，仅由<strong><u>小项的析取组成</u></strong>，称该公式为原公式的主析取范式。</p><p>可以证明：任何命题公式都存在着与之等价的主析取范式。</p></blockquote><h5 id="定理-2.1">【定理 2.1】</h5><blockquote><p>在真值表中，一个公式的成真指派所对应的小项的析取，即构成该公式的主析取范式。</p></blockquote><h5 id="构成命题公式的主析取范式的两种方法">2.2.1.1 构成命题公式的主析取范式的两种方法</h5><ul><li>真值表法<ol type="1"><li>构造命题公式的真值表</li><li>找出公式的成真指派对应的小项</li><li>这些小项的析取就是此公式的主析取范式</li></ol></li><li>等价演算法<ol type="1"><li>划归为析取范式</li><li>除去析取范式中所有永假的简单合取式</li><li>在简单合取式中，将重复出现的合取项和相同变元合并</li><li>在简单合取式中补入没有出现的命题变元，即添加∧(p∨¬p)，<strong>再用分配律展开，最后合并</strong>相同的小项</li></ol></li></ul><h5 id="注意">2.2.1.2 注意</h5><blockquote><p>矛盾式无成真指派，因而主析取范式不含任何小项，将矛盾式的主析取范式记作0。</p><p>重言式无成假指派，因而主析取范式含2<sup>n</sup>(n为公式中命题变元的个数)个小项。</p><p>可满足式的主析取范式中小项的个数一定小于等于2<sup>n</sup>(n为公式中命题变元的个数)个小项</p></blockquote><h3 id="主合取范式">2.2.2 主合取范式</h3><h5 id="定义-2.7主合取范式">【定义 2.7】主合取范式</h5><blockquote><p>对于给定的命题公式，如果有一个它的等价公式，仅由大项的合取组成，称该公式为原公式的主合取范式。</p><p>可以证明：任何命题公式都存在着与之等价的主合取范式。</p></blockquote><h5 id="定理-2.2">【定理 2.2】</h5><blockquote><p>在真值表中，一个公式的真值为F的指派所对应的大项的合取，即为该公式的主合取范式。</p></blockquote><h5 id="构成命题公式的主合取范式的两种方法">2.2.2.1 构成命题公式的主合取范式的两种方法</h5><ul><li>真值表<ol type="1"><li>构造命题公式的真值表</li><li>找出公式的成假指派对应的大项</li><li>这些大项的合取就是此公式的主合取范式</li></ol></li><li>等价演算法<ol type="1"><li>划归为合取范式</li><li>除去合取范式中所有永真的简单析取式</li><li>在简单析取式中，将重复出现的析取项和相同变元合并</li><li>在简单析取式中补入没有出现的命题变元，即添加∨(p∧¬p)，<strong>再用分配律展开，最后合并</strong>相同的大项</li></ol></li></ul><h3 id="主析取范式和主合取范式的关系">2.2.3 主析取范式和主合取范式的关系</h3><p>​ 同一公式的主析取范式中m的下标和主合取范式中的M的下标是互补的。因此，知道了主析（合）取范式就可以写出主合（析）取范式</p><ul><li><p>栗子</p><p>公式(p→q)→r的</p><p>主析取范式为：m<sub>7</sub>∨m<sub>5</sub>∨m<sub>4</sub>∨m<sub>3</sub>∨m<sub>1</sub></p><p>主合取范式为：M<sub>0</sub>∧M<sub>2</sub>∧M<sub>6</sub></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 离散数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离散数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.1.范式</title>
      <link href="/2023/01/13/cmath_3/"/>
      <url>/2023/01/13/cmath_3/</url>
      
        <content type="html"><![CDATA[<h1 id="范式">2.1 范式</h1><h3 id="范式的概念">2.1.1 范式的概念</h3><h5 id="定义-2.1简单析合取式">【定义 2.1】简单析（合）取式</h5><blockquote><p>由一些命题变元或其否定构成的析（合）取式称为简单析（合）取式。约定单个变元或其否定是简单析（合）取式。</p></blockquote><blockquote><p>命题变元及其否定统称为文字。仅由有限个文字构成的析取式称作简单析取式。简单析取式中不包含除析取联结词之外的其他联结词。</p><p>仅有限个文字构成的合取式称作简单合取式。 简单合取式不包含除合取联结词之外的其他联结词。</p></blockquote><h5 id="定理-2.1简单析取式和简单合取式有一下性质">【定理 2.1】简单析取式和简单合取式有一下性质。</h5><blockquote><p>一个简单析取式是重言式，当且仅当它同时含某个命题变元及它的否定式。</p><p>一个简单合取式是矛盾式，当且仅当它同时含某个命题变元及它的否定式。</p></blockquote><h5 id="定义2.2合取范式">【定义2.2】合取范式</h5><blockquote><p>由简单析取式的合取构成的公式叫做合取范式。约定单个简单析取式是合取范式。</p></blockquote><blockquote><p>一个命题公式称为合取范式，当且仅当它具有形式：A<sub>1</sub>∧A<sub>2</sub>∧...∧A<sub>n</sub>（n&gt;=1），其中A<sub>1</sub>，A<sub>2</sub>，A<sub>n</sub>都是简单析取式。</p></blockquote><h5 id="定义-2.3析取范式">【定义 2.3】析取范式</h5><blockquote><p>由简单合取式的吸取构成的公式叫做析取范式。约定单个简单合取式是吸取范式。</p></blockquote><blockquote><p>一个命题公式称为析取范式，当且仅当它具有形式：A<sub>1</sub>∨A<sub>2</sub>∨...∨A<sub>n</sub>（n&gt;=1），其中A<sub>1</sub>，A<sub>2</sub>，A<sub>n</sub>都是简单合取式。</p></blockquote><ul><li><p>有些命题公式既可以看作析取范式也可以看成是合取范式。</p><p>例如： ¬P∧Q∧R 既可以看成是由一个简单合取式构成的析取范式，也可以看成是由三个简单析取式构成的合取范式。</p></li></ul><h5 id="等值合析取范式求解步骤">等值合（析）取范式求解步骤：</h5><blockquote><p>①等值等价式与蕴涵等值式： 消去公式中的联结词↔和→</p><p>②双重否定律与德摩根律：使否定联结词¬出现在命题变元的前面，而不是括号的前面</p><p>③分配律</p></blockquote><h3 id="小项与大项">2.1.2 小项与大项*</h3><h5 id="定义-2.4小项">【定义 2.4】小项</h5><blockquote><p>在简单合取式中，<strong>每个变元及其否定不同时存在</strong>，但<strong>两者之间必须出现且仅出现一次</strong>，这样的简单合取式叫做布尔合取也叫小项或极小项</p></blockquote><blockquote><p>n个命题变元的简单合取式，称作布尔合取或极小项，简称为小项，其中每个命题变元与它的否定不能同时存在，但该命题变元必须出现且仅出现一次，或以变元的形式，或以变元的否定形式。</p></blockquote><ul><li><p>两个命题变元的小项共有4（=2<sup>2</sup>）个，n个命题变元共有2<sup>n</sup>个小项。</p></li><li><p>编码说明</p><p>​ 以字母m加上由编码构成的下标来表示每个小项，其中下标是一个n位的二进制数。若出现命题变元P<sub>i</sub>，则对应该小项编码的第i位是1（<strong>肯定=&gt;1</strong>）；若出现的是命题变元P<sub>i</sub>的否定，则对应的第i位是0（<strong>否定=&gt;0</strong>）。</p><p>​ 例如：¬P∧Q∧R 的编码为011，表示为m<sub>011</sub>，十进制表示为m<sub>3</sub></p></li><li><p>性质</p><ol type="1"><li>每个小项当其真值指派与编码相同时，其真值为T，在其余情况下均为F</li><li>任意两个不同小项的合取式永假</li><li>全体小项的析取式永真</li></ol></li></ul><h5 id="定义2.5大项">【定义2.5】大项</h5><blockquote><p>在简单析取式中，<strong>每个变元及其否定不同时存在</strong>，但<strong>两者之间必须出现且仅出现一次</strong>，这样的简单析取式叫做布尔析取也叫大项或极大项。</p></blockquote><blockquote><p>n个命题变元的简单析取式，称作布尔析取或极大项，简称为大项，其中每个命题变元与它的否定不能同时存在，但该命题变元必须出现且仅出现一次，或以变元的形式，或以变元的否定形式。</p></blockquote><ul><li><p>两个命题变元的大项共有4（=2<sup>2</sup>）个，n个命题变元共有2<sup>n</sup>个大项。</p></li><li><p>编码说明</p><p>​ 以字母M加上由编码构成的下边来表示每个大项，其中下标是一个n位的二进制数。若出现命题变元P<sub>i</sub>，则对应该大项编码的第i位是0（<strong>肯定=&gt;0</strong>）；若出现的是命题变元P<sub>i</sub>的否定，则对应的第i位是1（<strong>否定=&gt;1</strong>）。</p></li><li><p>性质</p><ol type="1"><li>每个大项当其真值指派与编码相同时，其真值为F，在其余情况下均为T</li><li>任意两个不同大项的析取式永真</li><li>全体大项的合取式永假</li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> 离散数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离散数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.2.命题公示的等值演算</title>
      <link href="/2023/01/13/cmath_2/"/>
      <url>/2023/01/13/cmath_2/</url>
      
        <content type="html"><![CDATA[<h1 id="命题公式真值表">1.2.1 命题公式/真值表</h1><h3 id="命题公式概念真值表">1.2.1.1 命题公式概念/真值表</h3><blockquote><p>按以下规则构成的符号串称为命题演算的合式公式，也称为命题公式，简称公式。 1. 单个命题变元和常元是合式公式，并称为<strong><u>原子命题公式</u></strong>。</p><ol start="2" type="1"><li><p>如果A是合式公式，则（¬A）是合式公式。</p></li><li><p>若A、B是合式公式，则（A∧B），（A∨B），（A→B），（A↔B）是合式公式。</p></li><li><p>有限次地应用1~3形成的符号串是合式公式。</p></li></ol><p>合式公式也称为命题公式或命题形式，简称为公式。</p></blockquote><h3 id="联结词优先次序">1.2.1.2 联结词优先次序</h3><blockquote><p><strong><u>¬，∧，∨，→，↔</u></strong></p></blockquote><h3 id="子公式公式分量">1.2.1.3 子公式（公式分量）</h3><blockquote><p>设A<sub>i</sub>是公式A的一部分，且A<sub>i</sub>是一个合式公式，称A<sub>i</sub>是A的子公式，或公式分量</p></blockquote><h3 id="成真成假指派">1.2.1.4 成真（成假）指派</h3><blockquote><p>设A为一命题公式，P<sub>1</sub>，P<sub>2</sub>，...，P<sub>n</sub>为出现在A中的所有命题变元，对P<sub>1</sub>，P<sub>2</sub>，...，P<sub>n</sub>各指定一个真值称为对A的一种指派或赋值。若指定的一种指派使A的值为真，则称这组值为A的成真指派；若指定的一种指派使A的值为假，则称这组值为A的成假指派。</p><p><strong>含n个命题变元的命题公式，共有2<sup>n</sup>组指派。</strong></p></blockquote><h3 id="真值表">1.2.1.5 真值表</h3><blockquote><p>将命题公式A在所有指派下的取值情况列成表，称为A的真值表。</p></blockquote><h1 id="命题公式的等值演算">1.2.2 命题公式的等值演算</h1><h3 id="等值等价概念">1.2.2.1 等值（等价）概念</h3><blockquote><p>设A和B是两个命题公式，设P<sub>1</sub>，P<sub>2</sub>，...，P<sub>n</sub>为所有出现于A和B中的原子变元，若给定P<sub>1</sub>，P<sub>2</sub>，...，P<sub>n</sub><u><strong>任一组真值指派，A和B的真值都相同</strong></u>，则称A和B是等值的或等价的，记作A⇔B</p></blockquote><h3 id="永真假式和可满足式">1.2.2.2永真（假）式和可满足式</h3><blockquote><p>设A为一命题公式，若A在它的各种指派情况下，其取值均为真（假），则成公式A为永真式（永假式）或重言式（矛盾式）。</p><p>设A为一命题公式，若A在它的各种指派情况下至少存在一组成真指派，则称A是可满足式。若可满足式A至少存在一个成假赋值，则称A为非重言式的可满足式。</p></blockquote><ul><li>合式公式P是可满足式，等价于P至少存在一个成真赋值；</li><li>重言式一定是可满足式，但可满足式不一定是重言式；</li><li>若两个命题公式P和Q等价，则P→Q是重言式。</li></ul><h3 id="证明两个命题公式等价的方法">1.2.2.3 证明两个命题公式等价的方法：</h3><h4 id="真值表法">1.2.2.3.1 真值表法</h4><blockquote><p>【证明题】根据等价的定义，用真值表证明 p→q ⇔ ¬p∨q</p><table><thead><tr class="header"><th style="text-align: center;">p</th><th style="text-align: center;">q</th><th style="text-align: center;">p→q</th><th style="text-align: center;">¬p∨q</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">T</td><td style="text-align: center;">T</td><td style="text-align: center;">T</td><td style="text-align: center;">T</td></tr><tr class="even"><td style="text-align: center;">T</td><td style="text-align: center;">F</td><td style="text-align: center;">F</td><td style="text-align: center;">F</td></tr><tr class="odd"><td style="text-align: center;">F</td><td style="text-align: center;">T</td><td style="text-align: center;">T</td><td style="text-align: center;">T</td></tr><tr class="even"><td style="text-align: center;">F</td><td style="text-align: center;">F</td><td style="text-align: center;">T</td><td style="text-align: center;">T</td></tr></tbody></table></blockquote><h4 id="等值演算法">1.2.2.3.2 等值演算法</h4><blockquote><p>先用真值表证明一组基本等价式，以它们为基础进行公式之间的演算。基本等价式常叫命题定律。已知成立定律： |已知成立定律|| |:----:|:----:| | 1. 双重否定律 | A⇔ ¬¬A | | 2. 交换律 | A∨B⇔B∨A<br/>A∧B⇔B∧A | | 3. 结合律 | (A∨B)∨C⇔A∨(B∨C)<br/>(A∧B)∧C⇔A∧(B∧C) | | 4. 分配律<font color=#ff0000><em></font> | A∧(B∨C)⇔(A∧B)∨(A∧C)<br/>A∨(B∧C)⇔(A∨B)∧(A∨C) | | 5. 德摩根律<font color=#ff0000></em></font> | ¬(A∨B)⇔¬A∧¬B<br/>¬(A∧B)⇔¬A∨¬B | | 6. 幂等律| A∧A ⇔ A，A∨A ⇔ A | |7. 吸收律 | A∨(A∧B) ⇔ A <br/>A∧(A∨B) ⇔ A | | 8. 零律 | A∨1⇔1 <br/>A∧0⇔0 | | 9. 同一律 | A∨0⇔A <br/>A∧1⇔A | | 10. 排中律 | A∨¬A⇔1 | | 11. 否定律（矛盾律） | A∧¬A⇔0 | | 12. 蕴涵等值式（条件等价式）<font color=#ff0000>*</font> | A→B⇔¬A∨B | | 13. 双条件等价式(充要)（等价等值式） | A↔B⇔(A→B)∧(B→A) | | 14. 假言易位式(逆否) | A→B⇔¬B→¬A | | 15. 双条件否定等价式（等价否定等值式） | A↔B⇔¬ A↔¬B | | 16. 归谬论 | (A→B)∧(A→¬B)⇔¬A |</p></blockquote><h3 id="等值演算与蕴涵式">1.2.2.4 等值演算与蕴涵式</h3><ul><li>【定理1】等价置换</li></ul><blockquote><p>设Φ(A)是含有子公式A的命题公式，使用子公式B置换Φ(A)中A的所有出现，得到公式Φ(B)，若A⇔B，则Φ(A)⇔Φ(B)。满足此定理的置换叫做等价置换。</p></blockquote><ul><li>【定理2】</li></ul><blockquote><p>设A、B为两个命题公式，A⇔B，当且仅当A↔B为一个重言式。</p></blockquote><ul><li>蕴涵式（永真条件式）</li></ul><blockquote><p>当且仅当P↔Q是一个重言式时，我们称“P蕴涵Q”，并记作P=&gt;Q</p></blockquote><h3 id="联结词完备集">1.2.2.5 联结词完备集</h3><blockquote><p>设S是一个联结词集合，如果任何n（n&gt;=1）元真值函数都可以有仅含S中的联结词构成的公式表示，则称S是联结词完备集。</p></blockquote><ul><li><p>最小完备集</p><blockquote><p>{¬, ∨}、{¬, ∧}</p></blockquote></li><li><p>与非门、或非门</p><blockquote><p>设P、Q为两个命题，P与Q的否定式是一个复合命题，称作P与Q的与非式，记作P↑Q，即P↑Q⇔¬（P∧Q）。符号↑称为与非联结词。</p><p>设P、Q为两个命题，P或Q的否定式是一个复合命题，称作P与Q的或非式，记作P↓Q，即P↓Q⇔¬（P∨Q）。符号↓称为或非联结词。</p></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> 离散数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离散数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.1.基本概念</title>
      <link href="/2023/01/13/cmath_1/"/>
      <url>/2023/01/13/cmath_1/</url>
      
        <content type="html"><![CDATA[<h1 id="命题">1.1.1命题</h1><h3 id="命题的基本概念">1.1.1.1 命题的基本概念</h3><blockquote><p>能<strong>判断真假且真值唯一</strong>的<strong>陈述句</strong>称为<strong>命题</strong>。</p><p>一般用<strong>小写英文字母</strong>或<strong>小写英文字母</strong>带下标表示。</p></blockquote><blockquote><p>一个命题表达的判断结果称为命题的<strong>真值</strong>。</p><p>真值为真为真命题，真值为假为假命题。</p></blockquote><h3 id="例子">1.1.1.2 例子</h3><blockquote><p>上海是个小村庄。 假命题 F 0 北京是中国的首都。 真命题 T 1</p></blockquote><h1 id="命题联结词">1.1.2 命题联结词</h1><h3 id="概念">1.1.2.1 概念</h3><blockquote><p>不可再分解的命题被称为<strong>原子命题</strong>。 可再分解的命题称为<strong>复合命题</strong>。</p></blockquote><h3 id="否定联结词">1.1.2.2 否定联结词</h3><blockquote><p>设p为命题, 则p的否定是一个复合命题, 记作: ¬p，读作“非p” 或 “p的否定”。</p><p>定义为: 若p为T，则¬p为F；若p为F，则¬p为T。</p><p>真值表如下：</p><table><thead><tr class="header"><th style="text-align: center;">p</th><th style="text-align: center;">¬p</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">T</td><td style="text-align: center;">F</td></tr><tr class="even"><td style="text-align: center;">F</td><td style="text-align: center;">T</td></tr></tbody></table></blockquote><h3 id="合取联结词">1.1.2.3 合取联结词</h3><blockquote><p>设p和q均为命题，则p和q的合取是一个复合命题，记作p∧q。读作“p与q” 或 “p合取q”。</p><p>定义为：当且仅当p和q均为T时，p∧q 才为T。</p><p>真值表如下：</p><table><thead><tr class="header"><th style="text-align: center;">p</th><th style="text-align: center;">q</th><th style="text-align: center;">p∧q</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">T</td><td style="text-align: center;">T</td><td style="text-align: center;">T</td></tr><tr class="even"><td style="text-align: center;">T</td><td style="text-align: center;">F</td><td style="text-align: center;">F</td></tr><tr class="odd"><td style="text-align: center;">F</td><td style="text-align: center;">T</td><td style="text-align: center;">F</td></tr><tr class="even"><td style="text-align: center;">F</td><td style="text-align: center;">F</td><td style="text-align: center;">F</td></tr></tbody></table></blockquote><h3 id="析取联结词">1.1.2.4 析取联结词</h3><blockquote><p>设p和q均为命题，则p和q的析取是一个复合命题，记作p∨q，读作“p或q” 或者 “p析取q”。 定义为：当且仅当p和q均为F时，p∨q才为F。 真值表如下: |p|q|p∨q| |:----:|:----:|:----:| |T|T|T| |T|F|T| |F|T|T| |F|F|F| * #### 或的两种含义 同或：表示相容，描述的两个命题<strong>并不排斥</strong>，可以同时发生。 异或：表示相斥，描述的两个命题<strong>不可以同时发生</strong>。</p></blockquote><h3 id="条件联结词">1.1.2.5 条件联结词</h3><blockquote><p>设p和q均为命题，则条件命题是个复合命题，记作：p→q，读作 “如果p，那么q” 或者 “若p，则q”。 定义为：当且仅当p为T，q为F时，p→q才为F。p称为条件命题p→q的前件，q称为条件命题p→q的后件。 前件为F时, 条件命题必为T。 真值表如下： |p|q|p→q| |:----:|:----:|:----:| |T|T|T| |T|F|F| |F|T|T| |F|F|T|</p></blockquote><h3 id="双条件联结词">1.1.2.6 双条件联结词</h3><blockquote><p>设p和q均为命题，其复合命题p↔q称为双条件命题，读作：“p双条件q” 或者 “p当且仅当q”。 定义为：当且仅当p和q的真值相同时，p↔q为T。 真值表如下： |p|q|p↔q| |:----:|:----:|:----:| |T|T|T| |T|F|F| |F|T|F| |F|F|T|</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 离散数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离散数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch学习（一）</title>
      <link href="/2023/01/13/Pytorch_1/"/>
      <url>/2023/01/13/Pytorch_1/</url>
      
        <content type="html"><![CDATA[<h1 id="numpy实现">numpy实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = np.square(y_pred - y).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="built_in">print</span>(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loss = (y_pred - y) ** 2</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    <span class="comment"># </span></span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><h1 id="tensor-实现">tensor 实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.<span class="built_in">float</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># device = torch.device(&quot;cuda:0&quot;) # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.mm(w1)</span><br><span class="line">    h_relu = h.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="built_in">print</span>(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</span><br><span class="line">    grad_h = grad_h_relu.clone()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><h1 id="自动求导">自动求导</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">dtype = torch.<span class="built_in">float</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># device = torch.device(&quot;cuda:0&quot;) # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N 是 batch size; D_in 是 input dimension;</span></span><br><span class="line"><span class="comment"># H 是 hidden dimension; D_out 是 output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建随机的Tensor来保存输入和输出</span></span><br><span class="line"><span class="comment"># 设定requires_grad=False表示在反向传播的时候我们不需要计算gradient</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建随机的Tensor和权重。</span></span><br><span class="line"><span class="comment"># 设置requires_grad=True表示我们希望反向传播的时候计算Tensor的gradient</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># 前向传播:通过Tensor预测y；这个和普通的神经网络的前向传播没有任何不同，</span></span><br><span class="line">    <span class="comment"># 但是我们不需要保存网络的中间运算结果，因为我们不需要手动计算反向传播。</span></span><br><span class="line">    y_pred = x.mm(w1).clamp(<span class="built_in">min</span>=<span class="number">0</span>).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过前向传播计算loss</span></span><br><span class="line">    <span class="comment"># loss是一个形状为(1，)的Tensor</span></span><br><span class="line">    <span class="comment"># loss.item()可以给我们返回一个loss的scalar</span></span><br><span class="line">    loss = (y_pred - y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="built_in">print</span>(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># PyTorch给我们提供了autograd的方法做反向传播。如果一个Tensor的requires_grad=True，</span></span><br><span class="line">    <span class="comment"># backward会自动计算loss相对于每个Tensor的gradient。在backward之后，</span></span><br><span class="line">    <span class="comment"># w1.grad和w2.grad会包含两个loss相对于两个Tensor的gradient信息。</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 我们可以手动做gradient descent(后面我们会介绍自动的方法)。</span></span><br><span class="line">    <span class="comment"># 用torch.no_grad()包含以下statements，因为w1和w2都是requires_grad=True，</span></span><br><span class="line">    <span class="comment"># 但是在更新weights之后我们并不需要再做autograd。</span></span><br><span class="line">    <span class="comment"># 另一种方法是在weight.data和weight.grad.data上做操作，这样就不会对grad产生影响。</span></span><br><span class="line">    <span class="comment"># tensor.data会我们一个tensor，这个tensor和原来的tensor指向相同的内存空间，</span></span><br><span class="line">    <span class="comment"># 但是不会记录计算图的历史。</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure><h1 id="定义自动求导函数">定义自动求导函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyReLU</span>(torch.autograd.Function):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">  torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">  which operate on Tensors.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">  @staticmethod</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    In the forward pass we receive a context object and a Tensor containing the</span></span><br><span class="line"><span class="string">    input; we must return a Tensor containing the output, and we can use the</span></span><br><span class="line"><span class="string">    context object to cache objects for use in the backward pass.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ctx.save_for_backward(x)</span><br><span class="line">    <span class="keyword">return</span> x.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">  @staticmethod</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    In the backward pass we receive the context object and a Tensor containing</span></span><br><span class="line"><span class="string">    the gradient of the loss with respect to the output produced during the</span></span><br><span class="line"><span class="string">    forward pass. We can retrieve cached data from the context object, and must</span></span><br><span class="line"><span class="string">    compute and return the gradient of the loss with respect to the input to the</span></span><br><span class="line"><span class="string">    forward function.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x, = ctx.saved_tensors</span><br><span class="line">    grad_x = grad_output.clone()</span><br><span class="line">    grad_x[x &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> grad_x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dtype = torch.<span class="built_in">float</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># device = torch.device(&quot;cuda:0&quot;) # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N 是 batch size; D_in 是 input dimension;</span></span><br><span class="line"><span class="comment"># H 是 hidden dimension; D_out 是 output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建随机的Tensor来保存输入和输出</span></span><br><span class="line"><span class="comment"># 设定requires_grad=False表示在反向传播的时候我们不需要计算gradient</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建随机的Tensor和权重。</span></span><br><span class="line"><span class="comment"># 设置requires_grad=True表示我们希望反向传播的时候计算Tensor的gradient</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># 前向传播:通过Tensor预测y；</span></span><br><span class="line">    <span class="comment"># 使用自定义的ReLU</span></span><br><span class="line">    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 通过前向传播计算loss</span></span><br><span class="line">    loss = (y_pred - y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="built_in">print</span>(t, loss.item())</span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        w1 -= learning_rate * w1.grad</span><br><span class="line">        w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">        w1.grad.zero_()</span><br><span class="line">        w2.grad.zero_()</span><br></pre></td></tr></table></figure><h1 id="pytorch-中的nn">pytorch 中的nn</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></span><br><span class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></span><br><span class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></span><br><span class="line"><span class="comment"># linear function, and holds internal Tensors for its weight and bias.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></span><br><span class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></span><br><span class="line">loss_fn = torch.nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></span><br><span class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></span><br><span class="line">    <span class="comment"># doing so you pass a Tensor of input data to the Module and it produces</span></span><br><span class="line">    <span class="comment"># a Tensor of output data.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss. We pass Tensors containing the predicted and true</span></span><br><span class="line">    <span class="comment"># values of y, and the loss function returns a Tensor containing the</span></span><br><span class="line">    <span class="comment"># loss.</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    <span class="built_in">print</span>(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></span><br><span class="line">    model.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></span><br><span class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></span><br><span class="line">    <span class="comment"># in Tensors with requires_grad=True, so this call will compute gradients for</span></span><br><span class="line">    <span class="comment"># all learnable parameters in the model.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the weights using gradient descent. Each parameter is a Tensor, so</span></span><br><span class="line">    <span class="comment"># we can access its gradients like we did before.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">            param -= learning_rate * param.grad</span><br></pre></td></tr></table></figure><h1 id="pytorch-optim">PyTorch: optim</h1><p>这一次我们不再手动更新模型的weights,而是使用optim这个包来帮助我们更新参数。 optim这个package提供了各种不同的模型优化方法，包括SGD+momentum, RMSProp, Adam等等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold inputs and outputs</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the nn package to define our model and loss function.</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(D_in, H),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(H, D_out),</span><br><span class="line">)</span><br><span class="line">loss_fn = torch.nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = loss_fn(y_pred, y)</span><br><span class="line">    <span class="built_in">print</span>(t, loss.item())</span><br><span class="line">    <span class="comment"># 梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向过程：计算梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h1 id="pytorch-自定义-nn-modules">PyTorch: 自定义 nn Modules</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerNet</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in, H, D_out</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        In the constructor we instantiate two nn.Linear modules and assign them as</span></span><br><span class="line"><span class="string">        member variables.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(TwoLayerNet, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(D_in, H)</span><br><span class="line">        self.linear2 = torch.nn.Linear(H, D_out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        In the forward function we accept a Tensor of input data and we must return</span></span><br><span class="line"><span class="string">        a Tensor of output data. We can use Modules defined in the constructor as</span></span><br><span class="line"><span class="string">        well as arbitrary operators on Tensors.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        h_relu = self.linear1(x).clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">        y_pred = self.linear2(h_relu)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line">x = torch.randn(N, D_in)</span><br><span class="line">y = torch.randn(N, D_out)</span><br><span class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></span><br><span class="line">model = TwoLayerNet(D_in, H, D_out)</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    <span class="built_in">print</span>(t, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习 </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
